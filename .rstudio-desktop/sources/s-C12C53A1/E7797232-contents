
library(ggplot2)
library(plyr)
library(dplyr)
library(MASS)
library(Hmisc)
library(lme4)
library(reshape)
library(data.table)
library(boot)
library(stringr)
library(arsenal)

rm(list=ls(all=TRUE))

setwd("~/Collaborations/DavidLeitman/DataCollection_Ila_Spring2018")

Coding_data_22 <-read.table('./Recordings/IndividualTrials/DescriptionsDefiniteness_Data.txt', header=TRUE)

TrialDesignVideoCode_dat_22 <-read.table("TrialDesignVideoCode_ALL_22pairs.txt", header=TRUE)

ParticipantsDemographBooklet <- read.table("ParticipantsSESPersonalityADD_IlaSpring2018.txt", header=TRUE)

## BEGIN: removing the 2 pairs that we excluded, D13 and D15
nrow(TrialDesignVideoCode_dat_22)
TrialDesignVideoCode_dat_21 <- TrialDesignVideoCode_dat_22[TrialDesignVideoCode_dat_22$Dyad!="D13", ]
TrialDesignVideoCode_dat <- TrialDesignVideoCode_dat_21[TrialDesignVideoCode_dat_21$Dyad!="D15", ]
TrialDesignVideoCode_dat$Dyad <- factor(TrialDesignVideoCode_dat$Dyad)

Coding_data_21 <- Coding_data_22[substr(Coding_data_22$Filename, 1, 3)!="D13", ]
Coding_data <- Coding_data_21[substr(Coding_data_21$Filename, 1, 3)!="D15", ]
## END: removing the 2 pairs that we excluded, D13 and D15

nrow(TrialDesignVideoCode_dat)
nrow(TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceCode=="MISSED", ])

TrialDesignVideoCode_dat$ChoiceEval <- ifelse(TrialDesignVideoCode_dat$ChoiceCode=="MISSED", "missed", ifelse(substr(TrialDesignVideoCode_dat$ChoiceCode, 1, 4)==substr(TrialDesignVideoCode_dat$PictureCode, 1, 4), "correct", "incorrect"))

TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceEval=="missed", ]

## BEGIN: inspection of errors ##

nrow(TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceEval=="incorrect", ])
# write.table((TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceEval=="incorrect", ]), "ErrorsToCheck.txt")

## number of errors for each of the 8 dyads with errors
ddply(TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceEval=="incorrect", ], .(Dyad), nrow)
#D05  1
#D06  6
#D10  7
#D11  6
#D16  5
#D17  1
#D18  3
#D20  4

## Only 8 out of the 20 dyads made any errors. Can a dyad's tendency to make errors correlated with their reuse strategy?

ddply(TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceEval=="incorrect", ], .(Dyad, Mention), nrow)
ddply(TrialDesignVideoCode_dat[TrialDesignVideoCode_dat$ChoiceEval=="incorrect", ], .(Mention), nrow)
#Mention V1
#1       1 23
#2       2 10     ==> so very few errors on mention 2, not a factor that can be modeled in the regression on Reuse

## END: inspection of errors before demographics added ##


nrow(Coding_data)
nrow(TrialDesignVideoCode_dat)

Coding_data$Dyad <- substring(Coding_data$Filename, 1, 3)

Coding_data$TrialNbDesign <- substr(Coding_data$Filename, 9, 12)

## BEGIN: Dealing with the fact that the trial numbers in TrialDesignVideoCode_dat are 1-1, as opposed to 1-01
TrialDesignVideoCode_dat <- TrialDesignVideoCode_dat %>% mutate(TrialNb= ifelse(nchar(as.character(TrialDesignVideoCode_dat$TrialNb))==3, paste(substr(TrialDesignVideoCode_dat$TrialNb, 1, 2), substr(TrialDesignVideoCode_dat$TrialNb, 3, 3), sep="0"), as.character(TrialNb)))
TrialDesignVideoCode_dat$TrialNb <- as.character(TrialDesignVideoCode_dat$TrialNb)
## END: Dealing with the fact that the trial numbers in TrialDesignVideoCode_dat are 1-1, as opposed to 1-01

# Here, I add the lag information between mention 1 and mention 2
TrialDesignVideoCode_datC<- ddply(TrialDesignVideoCode_dat, .(Dyad, PictureCode), function(x){
    for(i in 1:nrow(x)){
      y <- x[x$Mention==1, ]
      z <- x[x$Mention==2, ]
      for(j in 1:nrow(y)){
        mention1position <- substr(y[j, c("TrialNb")], 3, 5)
      }
      for(k in 1:nrow(z)){
        mention2position <- substr(z[k, c("TrialNb")], 3, 5)
      }
      if(x[i, c("Mention")]==1){
        x[i, c("Lag")] <- 0
        }
      else{
        x[i, c("Lag")]<- as.numeric(mention2position)-(as.numeric(mention1position)+1)
      }
    }
    return(x)  
    })


Data <- merge(Coding_data, TrialDesignVideoCode_datC, by.x=c("Dyad", "TrialNbDesign"), 
           by.y=c("Dyad", "TrialNb"), all.x=TRUE, all.y=FALSE)



## Preparing demographics data on participants to merge with trial accuracy data

## Below, I assign a SES level based on Education. First, I average
## the educ levels of both parents; when the info for one of them is 
## missing, I keep only the one present. If both are missing, it's NaN

ParticipantsDemographBooklet$EducMotherCode <- as.numeric(ParticipantsDemographBooklet$EducMotherCode)
ParticipantsDemographBooklet$EducFatherCode <- as.numeric(ParticipantsDemographBooklet$EducFatherCode)

## ParticipantsDemograph[, c("ID", "EducMotherCode", "EducFatherCode")]

## I average the 2 codes for each parent when I have them; when I have a code for only one, it keeps this code only.

ParticipantsDemographBooklet$EducParents <- rowMeans(subset(ParticipantsDemographBooklet, select = 
                                                       c(EducMotherCode, EducFatherCode)), na.rm = TRUE)

## Then I apply the following rule: If the participant is younger than 30
## use the parent's education level as proxy for SES; if older than 30
## use the particiant's own level of education as proxy for SES. This 
## leaves 4 participants with no SES level (one declined answering any
## of these questions anyway...

ParticipantsDemographBooklet$Age <- as.numeric(as.character(ParticipantsDemographBooklet$Age))

ParticipantsDemographBooklet$SESEduc <- ifelse(ParticipantsDemographBooklet$Age<30, ParticipantsDemographBooklet$EducParents, 
                                        ifelse(ParticipantsDemographBooklet$Age>=30, ParticipantsDemographBooklet$EducSelfCode, "ERROR"))

nrow(ParticipantsDemographBooklet[ParticipantsDemographBooklet$SESEduc=="ERROR" | ParticipantsDemographBooklet$SESEduc=="NaN", ])

## 3 participants (out of 44) do not have a SES level established following the scheme described above; one is one of the people of 
## Dyad 13, which got excluded. The other ones are D10_A (who didn't specify educ level for themselves or their parents), and D19_A, 
## who is 19 but didn't specify their parent's education. 

ParticipantsDemographBooklet[substr(ParticipantsDemographBooklet$ID, 1, 3)=="D13", ]
# rows 25 & 26
ParticipantsDemographBooklet[substr(ParticipantsDemographBooklet$ID, 1, 3)=="D15", ]
# rows 29 & 30

## BEGIN: WHAT IS THE DISTRIBUTION OF AGES AND SESEduc IN THE SAMPLE?
ParticipantsDemograph_20 <- ParticipantsDemographBooklet[c(1:24, 27:28, 31:44), ]

max(ParticipantsDemograph_20$Age) #60
min(ParticipantsDemograph_20$Age) #19
median(ParticipantsDemograph_20$Age) #31

table(ParticipantsDemograph_20$SESEduc, useNA="always")
#  1    2  2.5    3  3.5    4  4.5    5  5.5    6  NaN <NA> 
#  5   12    1    4    2    1    1    8    1    3    1    1

median(ParticipantsDemograph_20$SESEduc, na.rm=TRUE)
# 3
## END: WHAT IS THE DISTRIBUTION OF AGES AND SESEduc IN THE SAMPLE?

##########################################################
#### BEGIN: Preparing data on descriptions ###############
##########################################################

Data <- merge(Coding_data, TrialDesignVideoCode_datC, by.x=c("Dyad", "TrialNbDesign"), 
              by.y=c("Dyad", "TrialNb"), all.x=TRUE, all.y=FALSE)


table(Data$ChoiceEval, useNA="always")


## The missed trials are no longer represented because there were no descriptions produced on those trials!!

##write.table(Data, "ToFindNA.txt")

Data$Role <- ifelse(substr(Data$ContributingSpeaker, 8, 8)==Data$DirectorID, "director", 
                    ifelse(substr(Data$ContributingSpeaker, 8, 8)==Data$MatcherID, "matcher", "ERROR"))
nrow(Data[Data$Role=="ERROR", ])

#BEGIN: Checking that Definiteness and Description are segments that align.
Data$DescripDefCheck <- ifelse(Data$Onset_sec==Data$Onset_sec_Def, "correct", "ERROR")
nrow(Data[Data$DescripDefCheck=="ERROR", ])
# if no issue, delete the column
Data$DescripDefCheck <- NULL
#END: Checking that Definiteness and Description are segments that align.

# Adding an identifier to each speaker

Data$DyadSpeaker <- paste(Data$Dyad, substr(Data$ContributingSpeaker, 8, 8), sep="-")


# Distribution of descriptions across roles and mentions

#BEGIN: checking that there is at least 1 description per trial for director; if not (and those trials are not missing), 
#       the description may have been coded on the wrong tier.

xtabs(~ Dyad + TrialNbDesign, data=Data[Data$Role=="director" & substr(Data$TrialNbDesign, 1, 1)=="1", ])
xtabs(~ Dyad + TrialNbDesign, data=Data[Data$Role=="director" & substr(Data$TrialNbDesign, 1, 1)=="2", ])

#END: checking that there is at least 1 description per trial for director. 


xtabs(~Role + Mention, Data)

#          Mention
#Role         1   2
#  director 806 756
#  matcher   67  33
#          92%  96%

xtabs(~ Dyad + Role + Mention, Data)

prop.table((xtabs(~ Dyad + Role, Data[Data$Mention=="1", ])), 1)
prop.table((xtabs(~ Dyad + Role, Data[Data$Mention=="2", ])), 1)

# how many different descriptions (on average) did each dyad (aggregating between directors and matchers) use on each mention?

ddply((ddply(Data, .(Dyad, Mention, PictureCode), nrow)), .(Dyad, Mention), summarise, MeanDescripNb = mean(V1))

# if I average across dyads:
ddply(ddply((ddply(Data, .(Dyad, Mention, PictureCode), nrow)), .(Dyad, Mention), summarise, MeanDescripNb = mean(V1)), .(Mention), summarise, mean = mean(MeanDescripNb))

##Mention     mean
##1       1 1.37
##2       2 1.23

## Distribution of the number of descriptions / expressions used by each dyad on mention 1

table((ddply(Data[Data$Mention=="1", ], .(Dyad, Mention, PictureCode), nrow))$V1, useNA="always")

#    1    2    3    4    5    6 <NA> 
#  489   89   37   12    7    2    0 

# 489 / 636 (total)= 77% ==> on 77% of the trials, directors offered one description and the trial ended there

## Distribution of the number of descriptions / expressions used by each dyad on mention 2

table((ddply(Data[Data$Mention=="2", ], .(Dyad, Mention, PictureCode), nrow))$V1, useNA="always")

#    1    2    3    4    5    6 <NA> 
#  535   71   26    7    1    0  

# 535 / 640 (total)= 83% ==> on 83% of the trials, directors offered one description and the trial ended there


## INSPECTING DESCRIPTIONS TO GET A SENSE OF HOW THEY DIFFER ACROSS MENTIONS

## BEGIN: Saving the data in a text file for visual inspection that can help spot human errors (i.e., when the same content words, but in different orders,
## were assigned as label for description of the same picture by the same dyad). This also allows to check for cases where two labels differ in number
## only (e.g., red_heel vs. red_heels) or for typos.
ToCheckDescriptions <-ddply((ddply(Data, .(Dyad, PictureCode, PictureName, Description), nrow)), .(PictureCode, PictureName, Dyad), summarise, Descriptions=toString(Description))

# This dataframe can be inspected by clicking on it on the top right window, in the "Environment"
# tab. It will be displayed in a new tab on the top left window, as it would be in Excel.

## >>>>>>SUBIN<<<<<
## Please check that the description associated with each PictureName is a plausible
## description. If the description seems to apply to another card, please right now the
## Dyad number. There's probably a coding mistake that needs to get fixed.

# write.table(ToCheckDescriptions, "ToCheckDescriptions.txt")
## END

###############################################################################
#### NEW NEW NEW NEW NEW ######################################################
## PREPARING THE DATA FOR ANALYSIS OF DESCRIPTION CONTENTS (SUBIN, AUG 2020) ##
###############################################################################

## STEP 1: Extracting the head noun as the last string (separated by _) of the description
## errors are identified and corrected by hand

## The next line extracts the last 'word' of the field 'Description' and lists it in a new
## column called 'HeadNoun'
Data$HeadNoun <- word(Data$Description, -1, sep=fixed("_"))

Data[c(1:5), c('Description', 'HeadNoun')]


# Now creating a dataframe that will allow me to spot the problems with this 
# 'last-word-is-head-noun' method.
ToCheckHeadNouns <-ddply((ddply(Data, .(Dyad, PictureName, HeadNoun), nrow)), .(PictureName, Dyad), summarise, HeadNouns=toString(HeadNoun))

## How to fix issues that the file ToCheckHeadNouns reveal:
## For instance, D10, for the picturename 'baseballhat', has 'pocketknife' as head noun. That
## cannot be correct.

## First, list the Descriptions:
Data[Data$Dyad=="D10" & Data$PictureName=="baseballhat", c("Description")]
#"hat_same_color_as_pocketknife" "red_cap"                       "baseball_cap"

## The one to manually change is 'hat_same_color_as_pocketknife', with head noun as 'hat'.
## Next, change the head noun from 'pocketknife' to 'hat'
Data[Data$Dyad=="D10" & Data$PictureName=="baseballhat" & Data$Description=="hat_same_color_as_pocketknife", c("HeadNoun")] <- "hat"

## !!! SUBIN: DO THE SAME FOR EACH OF THE PROBLEMS YOU FIND.

###################### BLOUSE ######################
Data[Data$Dyad=="D02" & Data$PictureName=="blouse", c("Description")]
# "green_female_blouse_with_bow" "green_blouse_with_bow" 
Data[Data$Dyad=="D02" & Data$PictureName=="blouse", c("HeadNoun")] <- "blouse"

Data[Data$Dyad=="D03" & Data$PictureName=="blouse", c("Description")]
# "green_shirt_with_bow" "green_shirt_with_bow"
Data[Data$Dyad=="D03" & Data$PictureName=="blouse", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D04" & Data$PictureName=="blouse", c("Description")]
# "green_top"            "green_shirt"          "green_shirt_with_bow" "green_top_with_bow" 
Data[Data$Dyad=="D04" & Data$PictureName=="blouse" & Data$Description == "green_shirt_with_bow", c("HeadNoun")] <- "shirt"
Data[Data$Dyad=="D04" & Data$PictureName=="blouse" & Data$Description == "green_top_with_bow", c("HeadNoun")] <- "top"

Data[Data$Dyad=="D09" & Data$PictureName=="blouse", c("Description")]
# "cute_green_little_blouse_with_bowtie_in_front" "green_blouse"              "green_blouse_with_bow" 
Data[Data$Dyad=="D09" & Data$PictureName=="blouse", c("HeadNoun")] <- "blouse"

Data[Data$Dyad=="D14" & Data$PictureName=="blouse", c("Description")]
# "green_womans_shirt_with_bow_at_front" "female_green_shirt_with_bow_in_front"
Data[Data$Dyad=="D14" & Data$PictureName=="blouse", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D16" & Data$PictureName=="blouse", c("Description")]
# "green_shirt_with_bow_tie" "green_with_bow"
Data[Data$Dyad=="D16" & Data$PictureName=="blouse", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D21" & Data$PictureName=="blouse", c("Description")]
# "green_shirt_with_bow"        "green_female_shirt_with_bow"
Data[Data$Dyad=="D21" & Data$PictureName=="blouse", c("HeadNoun")] <- "shirt"
##########################################################
###################### BUTTERSCOTCH ######################
Data[Data$Dyad=="D04" & Data$PictureName=="butterscotch", c("Description")]
#  "little_candy"        "lozenge"              "lozenge"               "orange_lozenge_with_yellow_wrapper"
Data[Data$Dyad=="D04" & Data$PictureName=="butterscotch" & Data$Description == "orange_lozenge_with_yellow_wrapper", c("HeadNoun")] <- "lozenge"

Data[Data$Dyad=="D05" & Data$PictureName=="butterscotch", c("Description")]
# "butterscotch_candy"           "yellow_hard_candy"            "yellow_hard_candy_in_wrapper"
Data[Data$Dyad=="D05" & Data$PictureName=="butterscotch" & Data$Description == "yellow_hard_candy_in_wrapper", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D06" & Data$PictureName=="butterscotch", c("Description")]
# "candy"                          "yellow_candy"                   "yellow_candy_twist_up_in_paper"
Data[Data$Dyad=="D06" & Data$PictureName=="butterscotch" & Data$Description == "yellow_candy_twist_up_in_paper", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D09" & Data$PictureName=="butterscotch", c("Description")]
# "yellow_werther's_candy_or_cheap_generic_kind" "butterscotch_candy"
Data[Data$Dyad=="D09" & Data$PictureName=="butterscotch" & Data$Description == "yellow_werther's_candy_or_cheap_generic_kind", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D10" & Data$PictureName=="butterscotch", c("Description")]
# "old_school_candy"         "old_school_candy_you_eat" "butterscotch_candy"
Data[Data$Dyad=="D10" & Data$PictureName=="butterscotch" & Data$Description == "old_school_candy_you_eat", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D14" & Data$PictureName=="butterscotch", c("Description")]
# "nasty_candy_that_looks_like_a_goldfish"      "butterscotch"      "butterscotch"      "butterscotch_yellow_candy_in_yellow_wrapper"
Data[Data$Dyad=="D14" & Data$PictureName=="butterscotch" & Data$Description == "nasty_candy_that_looks_like_a_goldfish", c("HeadNoun")] <- "candy"
Data[Data$Dyad=="D14" & Data$PictureName=="butterscotch" & Data$Description == "butterscotch_yellow_candy_in_yellow_wrapper", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D19" & Data$PictureName=="butterscotch", c("Description")]
# "orange_candy_thingish" "butterscotch_candy"
Data[Data$Dyad=="D19" & Data$PictureName=="butterscotch" & Data$Description == "orange_candy_thingish", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D20" & Data$PictureName=="butterscotch", c("Description")]
# "piece_of_candy"            "peppermint"                "yellow_candy"              "red_candy"
# "piece_of_candy"            "tan_orange_candy"          "orange_candy_with_wrapper"
Data[Data$Dyad=="D20" & Data$PictureName=="butterscotch" & Data$Description == "orange_candy_with_wrapper", c("HeadNoun")] <- "candy"
##########################################################
####################### BUTTONDOWN #######################
Data[Data$Dyad=="D02" & Data$PictureName=="buttondown", c("Description")]
# "tan_men's_shirt_button_down" "tan_men's_shirt_button_down"
Data[Data$Dyad=="D02" & Data$PictureName=="buttondown", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D09" & Data$PictureName=="buttondown", c("Description")]
# "tan_button_down_collared_long_sleeve" "tan_button_down_long_sleeve"
Data[Data$Dyad=="D09" & Data$PictureName=="buttondown", c("HeadNoun")] <- "buttondown"

Data[Data$Dyad=="D14" & Data$PictureName=="buttondown", c("Description")]
# "tan_collared_shirt"   "long_sleeve_tan_collared_shirt"   "tan_button_up_collar_shirt_long_sleeve_with_pocket_on_front"
Data[Data$Dyad=="D14" & Data$PictureName=="buttondown" & Data$Description == "tan_button_up_collar_shirt_long_sleeve_with_pocket_on_front", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D17" & Data$PictureName=="buttondown", c("Description")]
# "tan_long_sleeve" "brown_button_up"
Data[Data$Dyad=="D17" & Data$PictureName=="buttondown", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D21" & Data$PictureName=="buttondown", c("Description")]
# "tan_button_up_with_pocket" "tan_button_up_with_pocket"
Data[Data$Dyad=="D21" & Data$PictureName=="buttondown", c("HeadNoun")] <- "buttonup"
##########################################################
####################### cowboyboot #######################
Data[Data$Dyad=="D14" & Data$PictureName=="cowboyboot", c("Description")]
# "brown_cowboy_boot"         "brown_cowboy_boot"       "brown_cowboy_boot_with_design_on_front"
Data[Data$Dyad=="D14" & Data$PictureName=="cowboyboot" & Data$Description == "brown_cowboy_boot_with_design_on_front", c("HeadNoun")] <- "boot"
##########################################################
####################### cowboyhat ########################
Data[Data$Dyad=="D07" & Data$PictureName=="cowboyhat", c("Description")]
# "brown_item_with_silver_lining" "top_piece"                     "brown_cowboy_hat"             
Data[Data$Dyad=="D07" & Data$PictureName=="cowboyhat" & Data$Description == "brown_item_with_silver_lining", c("HeadNoun")] <- "item"
##########################################################
####################### dressshoe ########################
Data[Data$Dyad=="D05" & Data$PictureName=="dressshoe", c("Description")]
# "brown_mans_shoe_shoelaces"       "brown_mans_shoe_black_shoelaces"
Data[Data$Dyad=="D05" & Data$PictureName=="dressshoe", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D07" & Data$PictureName=="dressshoe", c("Description")]
# "brown_shoe_that_i_would_wear" "brown_shoe"    
Data[Data$Dyad=="D07" & Data$PictureName=="dressshoe" & Data$Description == "brown_shoe_that_i_would_wear", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D21" & Data$PictureName=="dressshoe", c("Description")]
# "sandal"                              "shoe"                                "brown_old_mans_shoe"                
# "shoe_like_bert_and_ernie_with_laces" "shoe"                                "brown_shoe_with_five_laces"         
Data[Data$Dyad=="D21" & Data$PictureName=="dressshoe" & Data$Description == "shoe_like_bert_and_ernie_with_laces", c("HeadNoun")] <- "shoe"
Data[Data$Dyad=="D21" & Data$PictureName=="dressshoe" & Data$Description == "brown_shoe_with_five_laces", c("HeadNoun")] <- "shoe"
##########################################################
####################### fancyboot ########################
Data[Data$Dyad=="D01" & Data$PictureName=="fancyboot", c("Description")]
# "high_heeled_shoe_kneehigh" "knee_boot"                 "knee_boot"                
Data[Data$Dyad=="D01" & Data$PictureName=="fancyboot" & Data$Description == "high_heeled_shoe_kneehigh", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D02" & Data$PictureName=="fancyboot", c("Description")]
# "long_high_heel"       "high_heel_boot_black" "high_heel_boot_black" "long_high_heel"
Data[Data$Dyad=="D02" & Data$PictureName=="fancyboot", c("HeadNoun")] <- "highheel"
Data[Data$Dyad=="D02" & Data$PictureName=="fancyboot" & Data$Description == "high_heel_boot_black", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D10" & Data$PictureName=="fancyboot", c("Description")]
# "something_you_would_wear_when_we_go_out_to_the_bar_on_your_feet" "same_boots_to_the_bar"
Data[Data$Dyad=="D10" & Data$PictureName=="fancyboot" & Data$Description == "something_you_would_wear_when_we_go_out_to_the_bar_on_your_feet", c("HeadNoun")] <- "something"
Data[Data$Dyad=="D10" & Data$PictureName=="fancyboot" & Data$Description == "same_boots_to_the_bar", c("HeadNoun")] <- "boots"

Data[Data$Dyad=="D16" & Data$PictureName=="fancyboot", c("Description")]
# "black_boot"                          "black_knee_high_boot"                "black_knee_high_boot_with_one_strap"
Data[Data$Dyad=="D16" & Data$PictureName=="fancyboot" & Data$Description == "black_knee_high_boot_with_one_strap", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D17" & Data$PictureName=="fancyboot", c("Description")]
# "black_long_boot"                     "black_long_boot_with_heel_and_strap" "black_long_boot"                    
# "long_black_boot"                     "long_black_boot_with_heel"          
Data[Data$Dyad=="D17" & Data$PictureName=="fancyboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D21" & Data$PictureName=="fancyboot", c("Description")]
# "black_boot_with_buckle_at_top"               "black_boot_2_3_inch_heel_with_buckle_at_top"
# "boot_with_buckle_at_top"                     "black_boot_with_buckle_at_top_with_heel"   "black_boot"                                 
Data[Data$Dyad=="D21" & Data$PictureName=="fancyboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D22" & Data$PictureName=="fancyboot", c("Description")]
# "stiletto"            "boot"                "black_boot"          "stiletto"            "stiletto_black_heel"
Data[Data$Dyad=="D22" & Data$PictureName=="fancyboot" & Data$Description == "stiletto_black_heel", c("HeadNoun")] <- "stiletto"
##########################################################
######################## furcoat #########################
Data[Data$Dyad=="D05" & Data$PictureName=="furcoat", c("Description")]
# "complete_ladies_fur_coat"       "standard_ladies_fur_coat_fairly_long_just_above_knees"    "ladies_fur_coat"                                      
Data[Data$Dyad=="D05" & Data$PictureName=="furcoat" & Data$Description == "standard_ladies_fur_coat_fairly_long_just_above_knees", c("HeadNoun")] <- "coat"
##########################################################
####################### hersheykiss ######################
Data[Data$Dyad=="D19" & Data$PictureName=="hersheykiss", c("Description")]
# "candy"        "candy_with_and_without_wrapper" "kisses"  "aluminum_foil_candy"  "aluminum_foil_candy"           
Data[Data$Dyad=="D19" & Data$PictureName=="hersheykiss" & Data$Description == "candy_with_and_without_wrapper", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D21" & Data$PictureName=="hersheykiss", c("Description")]
# "hershey's_kiss_one_open_one_not"          "hershey's_kiss_one_wrapped_one_unwrapped"
Data[Data$Dyad=="D21" & Data$PictureName=="hersheykiss", c("HeadNoun")] <- "kiss"
##########################################################
####################### kitchenknife #####################
Data[Data$Dyad=="D07" & Data$PictureName=="kitchenknife", c("Description")]
# "silver_titanium_with_handle_sharp" "butcher_knife"                     "butcher_knife"                    
Data[Data$Dyad=="D07" & Data$PictureName=="kitchenknife" & Data$Description == "silver_titanium_with_handle_sharp", c("HeadNoun")] <- "titanium"

Data[Data$Dyad=="D14" & Data$PictureName=="kitchenknife", c("Description")]
# "butcher_knife_with_black_handle"     "sharp_kitchen_knife"         "sharp_kitchen_knife_with_black_handle"                  
# "sharp_knife_with_black_handle_with_hells_kitchen_symbol"     "sharp_knife_with_black_handle_with_trident"
Data[Data$Dyad=="D14" & Data$PictureName=="kitchenknife", c("HeadNoun")] <- "knife"
##########################################################
######################## lollipop ########################
Data[Data$Dyad=="D10" & Data$PictureName=="lollipop", c("Description")]
# "something_amani_likes_to_eat" "red_taffy"                   
Data[Data$Dyad=="D10" & Data$PictureName=="lollipop", c("HeadNoun")] <- "something"

Data[Data$Dyad=="D14" & Data$PictureName=="lollipop", c("Description")]
# "red_lollipop"                        "red_lollipop_candy_with_white_stick"
Data[Data$Dyad=="D14" & Data$PictureName=="lollipop" & Data$Description == "red_lollipop_candy_with_white_stick", c("HeadNoun")] <- "candy"

Data[Data$Dyad=="D16" & Data$PictureName=="lollipop", c("Description")]
Data[Data$Dyad=="D16" & Data$PictureName=="lollipop" & Data$Description == "red_lolipop", c("Description")] <- "red_lollipop"

Data[Data$Dyad=="D21" & Data$PictureName=="lollipop", c("Description")]
# "red_lollipop_white_stick"        "single_red_lollipop_white_stick" "one_lollipop"      "lollipop"  
Data[Data$Dyad=="D21" & Data$PictureName=="lollipop" & Data$Description == "red_lollipop_white_stick", c("HeadNoun")] <- "lollipop"
##########################################################
######################## longsleeve ######################
Data[Data$Dyad=="D16" & Data$PictureName=="longsleeve", c("Description")]
# "brown_and_white_long_sleeve_shirt"      "brown_and_white_shirt"      "brown_and_white_shirt_with_the_stripes"          
Data[Data$Dyad=="D16" & Data$PictureName=="longsleeve" & Data$Description == "brown_and_white_shirt_with_the_stripes", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D19" & Data$PictureName=="longsleeve", c("Description")]
# "shirt"                  "shirt_with_stripes"     "red_shirt"              "red_shirt_with_stripes"
Data[Data$Dyad=="D19" & Data$PictureName=="longsleeve" & Data$Description == "shirt_with_stripes", c("HeadNoun")] <- "shirt"
Data[Data$Dyad=="D19" & Data$PictureName=="longsleeve" & Data$Description == "red_shirt_with_stripes", c("HeadNoun")] <- "shirt"

Data[Data$Dyad=="D21" & Data$PictureName=="longsleeve", c("Description")]
# "burgundy_shirt_with_white_stripes" "burgundy_shirt_with_white_stripes"
Data[Data$Dyad=="D21" & Data$PictureName=="longsleeve", c("HeadNoun")] <- "shirt"
##########################################################
######################## metalchair #####################
Data[Data$Dyad=="D10" & Data$PictureName=="metalchair", c("Description")]
# "something_you_sit_in" "cook_out_chair"       "fold_up_chair"
Data[Data$Dyad=="D10" & Data$PictureName=="metalchair" & Data$Description == "something_you_sit_in", c("HeadNoun")] <- "something"

Data[Data$Dyad=="D14" & Data$PictureName=="metalchair", c("Description")]
# "church_chair"        "always_cold_or_hot_when_in_sun"        "chair_everyone_sits_in_when_church_overflows"              
# "tannish_brown_chair_everyone_sits_in_when_church_overflows"  "church_chair"      "folding_chair"     "church_chair"                                              
# "folding_chair"     "folding_chair"
Data[Data$Dyad=="D14" & Data$PictureName=="metalchair", c("HeadNoun")] <- "chair"

Data[Data$Dyad=="D19" & Data$PictureName=="metalchair", c("Description")]
# "folding_chair"    "chair_in_my_room"
Data[Data$Dyad=="D19" & Data$PictureName=="metalchair" & Data$Description == "chair_in_my_room", c("HeadNoun")] <- "chair"
##########################################################
######################## officechair #####################
Data[Data$Dyad=="D05" & Data$PictureName=="officechair", c("Description")]
# "arms_rollers_chair"                  "moveable_up_and_down_different_heights_black_office_chair_on_wheels"
Data[Data$Dyad=="D05" & Data$PictureName=="officechair" & Data$Description == "moveable_up_and_down_different_heights_black_office_chair_on_wheels", c("HeadNoun")] <- "chair"

Data[Data$Dyad=="D09" & Data$PictureName=="officechair", c("Description")]
# "comfy_black_office_chair"           "comfy_black_office_chair_with_arms" "comfy_office_chair"                
Data[Data$Dyad=="D09" & Data$PictureName=="officechair" & Data$Description == "comfy_black_office_chair_with_arms", c("HeadNoun")] <- "chair"

Data[Data$Dyad=="D17" & Data$PictureName=="officechair", c("Description")]
# "office_chair"             "office_chair_with_wheels" "black_office_chair"      
Data[Data$Dyad=="D17" & Data$PictureName=="officechair" & Data$Description == "office_chair_with_wheels", c("HeadNoun")] <- "chair"

Data[Data$Dyad=="D21" & Data$PictureName=="officechair", c("Description")]
# "rolly_chair_three_handles_that_pump_you_up"      "black_chair_three_handle_that_pump_you_up" 
# "black_rolly_chair_three_handle_that_pump_you_up" "black_rolling_office_chair"
Data[Data$Dyad=="D21" & Data$PictureName=="officechair", c("HeadNoun")] <- "chair"
##########################################################
########################### peacoat ######################
Data[Data$Dyad=="D03" & Data$PictureName=="peacoat", c("Description")]
# "black_peacoat_double_buttons" "black_peacoat"               
Data[Data$Dyad=="D03" & Data$PictureName=="peacoat" & Data$Description == "black_peacoat_double_buttons", c("HeadNoun")] <- "peacoat"

Data[Data$Dyad=="D04" & Data$PictureName=="peacoat", c("Description")]
# "black_wool_coat"             "peacoat"                "black_wool_coat"    "black_wool_coat_two_sets_of_buttons"                   
# "black_wool_coat_two_sets_of_buttons_going_up_the_front" "peacoat"     
Data[Data$Dyad=="D04" & Data$PictureName=="peacoat" & Data$Description == "black_wool_coat_two_sets_of_buttons_going_up_the_front", c("HeadNoun")] <- "coat"
Data[Data$Dyad=="D04" & Data$PictureName=="peacoat" & Data$Description == "black_wool_coat_two_sets_of_buttons", c("HeadNoun")] <- "coat"

Data[Data$Dyad=="D05" & Data$PictureName=="peacoat", c("Description")]
# "blackish_blue_front_button_down_navy_peacoat" "black_peacoat_eight_buttons_front_navy_style"
Data[Data$Dyad=="D05" & Data$PictureName=="peacoat" & Data$Description == "black_peacoat_eight_buttons_front_navy_style", c("HeadNoun")] <- "peacoat"

Data[Data$Dyad=="D19" & Data$PictureName=="peacoat", c("Description")]
# "coat"                          "coat_with_collar"              "black_coat_with_collar"
# "miss_monica_coat"              "coat_she_wears_every_day"      "black_coat_with_black_buttons"
Data[Data$Dyad=="D19" & Data$PictureName=="peacoat", c("HeadNoun")] <- "coat"
##########################################################
######################## peppermint ######################
Data[Data$Dyad=="D14" & Data$PictureName=="peppermint", c("Description")]
# "round_peppermint_candy_with_stripes" "peppermint_candy"
Data[Data$Dyad=="D14" & Data$PictureName=="peppermint" & Data$Description == "round_peppermint_candy_with_stripes", c("HeadNoun")] <- "candy"
##########################################################
######################## rainboot ########################
Data[Data$Dyad=="D14" & Data$PictureName=="rainboot", c("Description")]
# "blue_rain_boot_with_polka_dots"      "light_blue_rainboot_with_polka_dots"
Data[Data$Dyad=="D14" & Data$PictureName=="rainboot", c("HeadNoun")] <- "rainboot"

Data[Data$Dyad=="D21" & Data$PictureName=="rainboot", c("Description")]
# "same_color_as_this_building_rainboot_with_white_polka_dots"    "greenish_rainboot_with_polka_dots"
Data[Data$Dyad=="D21" & Data$PictureName=="rainboot", c("HeadNoun")] <- "rainboot"
##########################################################
######################## raincoat ########################
Data[Data$Dyad=="D10" & Data$PictureName=="raincoat", c("Description")]
# "something_to_stop_the_rain_from_getting_you_wet" "yellow_rain_jacket"  
Data[Data$Dyad=="D10" & Data$PictureName=="raincoat" & Data$Description == "something_to_stop_the_rain_from_getting_you_wet", c("HeadNoun")] <- "something"

Data[Data$Dyad=="D21" & Data$PictureName=="raincoat", c("Description")]
# "yellow_raincoat_with_blue_polka_dots" "yellow_raincoat"
Data[Data$Dyad=="D21" & Data$PictureName=="raincoat" & Data$Description == "yellow_raincoat_with_blue_polka_dots", c("HeadNoun")] <- "raincoat"
##########################################################
######################### sandal #########################
Data[Data$Dyad=="D04" & Data$PictureName=="sandal", c("Description")]
# "shoe_with_holes_on_side" "sandal"                  "sandal"                  "sandal_shoe"
Data[Data$Dyad=="D04" & Data$PictureName=="sandal" & Data$Description == "shoe_with_holes_on_side", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D14" & Data$PictureName=="sandal", c("Description")]
# "old_man_sandal"       "old_man_sandal_no_laces_with_pull_contraption"      "old_man_shoe"         "sandal"  
# "sandal_with_string_pull_at_front_no_laces"
Data[Data$Dyad=="D14" & Data$PictureName=="sandal" & Data$Description == "old_man_sandal_no_laces_with_pull_contraption", c("HeadNoun")] <- "sandal"
Data[Data$Dyad=="D14" & Data$PictureName=="sandal" & Data$Description == "sandal_with_string_pull_at_front_no_laces", c("HeadNoun")] <- "sandal"

Data[Data$Dyad=="D18" & Data$PictureName=="sandal", c("Description")]
# "hiking_shoe_with_holes"       "brown_hiking_shoe_with_holes" "hiking_shoe"
Data[Data$Dyad=="D18" & Data$PictureName=="sandal", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D21" & Data$PictureName=="sandal", c("Description")]
# "brown_sandal"                        "brown_sandal_with_black_pull_string" "brown_sandals_with_pull_string"
Data[Data$Dyad=="D21" & Data$PictureName=="sandal", c("HeadNoun")] <- "sandal"
##########################################################
######################### sneaker ########################
Data[Data$Dyad=="D02" & Data$PictureName=="sneaker", c("Description")]
# "van_shoe_black_with_white_stripe" "black_sneaker_with_white_strip"   "vans"        "vans"
Data[Data$Dyad=="D02" & Data$PictureName=="sneaker" & Data$Description == "black_sneaker_with_white_strip", c("Description")] <- "black_sneaker_with_white_stripe"
Data[Data$Dyad=="D02" & Data$PictureName=="sneaker" & Data$Description == "black_sneaker_with_white_strip", c("HeadNoun")] <- "sneaker"
Data[Data$Dyad=="D02" & Data$PictureName=="sneaker" & Data$Description == "van_shoe_black_with_white_stripe", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D09" & Data$PictureName=="sneaker", c("Description")]
# "vans_shoe_black_with_white_stripe" "black_and_white_vans"
Data[Data$Dyad=="D09" & Data$PictureName=="sneaker" & Data$Description == "vans_shoe_black_with_white_stripe", c("HeadNoun")] <- "shoe"

Data[Data$Dyad=="D10" & Data$PictureName=="sneaker", c("Description")]
# "unintelligable_school_sneakers"              "low_cut_unintelligable_school_sneakers"                    
# "unintelligable's_first_low_cut_school_sneakers_last_year's"      "low_cut_black_sneakers" 
Data[Data$Dyad=="D10" & Data$PictureName=="sneaker", c("HeadNoun")] <- "sneakers"

Data[Data$Dyad=="D14" & Data$PictureName=="sneaker", c("Description")]
# "black_sneaker_looks_like_vans"         "sneaker"             "vans_sneaker"     
# "black_vans_sneaker"                   "black_vans_sneaker_with_white_strings"
Data[Data$Dyad=="D14" & Data$PictureName=="sneaker" & Data$Description == "black_sneaker_looks_like_vans", c("HeadNoun")] <- "sneaker"
Data[Data$Dyad=="D14" & Data$PictureName=="sneaker" & Data$Description == "black_vans_sneaker_with_white_strings", c("HeadNoun")] <- "sneaker"
##########################################################
######################### sunhat #########################
Data[Data$Dyad=="D02" & Data$PictureName=="sunhat", c("Description")]
# "tan_with_black_ribbon_female_hat" "farm_hat_brown_with_black_stripe" "straw_hat"
Data[Data$Dyad=="D02" & Data$PictureName=="sunhat" & Data$Description == "farm_hat_brown_with_black_stripe", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D03" & Data$PictureName=="sunhat", c("Description")]
# "straw_hat_with_black_ribbon" "straw_hat_black_ribbon"   
Data[Data$Dyad=="D03" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D04" & Data$PictureName=="sunhat", c("Description")]
# "gardeners_hat"               "hat_with_black_thing_around" "fishers_hat"                
# "wicker_hat"                  "wicker_gardening_hat"   
Data[Data$Dyad=="D04" & Data$PictureName=="sunhat" & Data$Description == "hat_with_black_thing_around", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D05" & Data$PictureName=="sunhat", c("Description")]
# "black_mat_straw_hat"     "black_band_straw_hat"    "black_band_straw_hat"    "black_band_straw_hat"
# "black_band_straw_hat"    "summer_sun_hat"          "sun_hat_with_black_band" "beachy_type_hat" 
Data[Data$Dyad=="D05" & Data$PictureName=="sunhat" & Data$Description == "sun_hat_with_black_band", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D07" & Data$PictureName=="sunhat", c("Description")]
# "tan_hat_with_black_ribbon" "tan_hat_with_scarf"
Data[Data$Dyad=="D07" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D09" & Data$PictureName=="sunhat", c("Description")]
# "tan_wicker_hat_with_black_little_thing_around_it" "ugly_wicker_hat_with_bow_on_it" 
Data[Data$Dyad=="D09" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D10" & Data$PictureName=="sunhat", c("Description")]
# "church_hat_with_black"        "church_hat_with_black_around"
Data[Data$Dyad=="D10" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D11" & Data$PictureName=="sunhat", c("Description")]
# "hat"                                      "ladies_tan_hat_with_black_band_around_it"
Data[Data$Dyad=="D11" & Data$PictureName=="sunhat" & Data$Description == "ladies_tan_hat_with_black_band_around_it", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D14" & Data$PictureName=="sunhat", c("Description")]
# "woman's_straw_hat_with_black_scarf_tied_around_it" "straw_hat_with_black_ribbon"  "female_hat"
Data[Data$Dyad=="D14" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D16" & Data$PictureName=="sunhat", c("Description")]
# "tan_hat_with_black_ribbon_around_it" "tan_hat_with_ribbon"  
Data[Data$Dyad=="D16" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D17" & Data$PictureName=="sunhat", c("Description")]
# "tan_hat_with_black_strap" "black_strap"              "black_tan_hat"      "tan_hat"    "tan_hat_with_black" 
Data[Data$Dyad=="D17" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"
Data[Data$Dyad=="D17" & Data$PictureName=="sunhat" & Data$Description == "black_strap", c("HeadNoun")] <- "strap"

Data[Data$Dyad=="D18" & Data$PictureName=="sunhat", c("Description")]
# "straw_hat_with_black_ribbon" "straw_hat_with_black_thing"
Data[Data$Dyad=="D18" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D21" & Data$PictureName=="sunhat", c("Description")]
# "straw_hat_black_tie"       "straw_hat_with_black_band"
Data[Data$Dyad=="D21" & Data$PictureName=="sunhat", c("HeadNoun")] <- "hat"
##########################################################
##################### swissarmyknife #####################
Data[Data$Dyad=="D14" & Data$PictureName=="swissarmyknife", c("Description")]
# "utility_knife_with_red_handle"     "utility_knife_with_red_handle_with_all_the_different_things" "utility_knife"
Data[Data$Dyad=="D14" & Data$PictureName=="swissarmyknife", c("HeadNoun")] <- "knife"

Data[Data$Dyad=="D20" & Data$PictureName=="swissarmyknife", c("Description")]
# "box_cutter"                 "knife_set"        "knife_set_has_five_different_kinds_of_knives"      "knife_set"
Data[Data$Dyad=="D20" & Data$PictureName=="swissarmyknife" & Data$Description == "knife_set", c("HeadNoun")] <- "knife"

Data[Data$Dyad=="D22" & Data$PictureName=="swissarmyknife", c("Description")]
# "pocket_knife_thingy_with_multi-task_situation" "multi_pocket_knife"
Data[Data$Dyad=="D22" & Data$PictureName=="swissarmyknife" & Data$Description == "pocket_knife_thingy_with_multi-task_situation", c("HeadNoun")] <- "knife"
##########################################################
######################## tanktop #########################
Data[Data$Dyad=="D02" & Data$PictureName=="tanktop", c("Description")]
# "girl's_pink_tshirt_arms_out" "pink_women's_tshirt"  
Data[Data$Dyad=="D02" & Data$PictureName=="tanktop" & Data$Description == "girl's_pink_tshirt_arms_out", c("HeadNoun")] <- "tshirt"

Data[Data$Dyad=="D14" & Data$PictureName=="tanktop", c("Description")]
# "pink_female_t-shirt"            "pink_female_t-shirt_no_sleeves" "sleeveless_pink_female_tshirt"
Data[Data$Dyad=="D14" & Data$PictureName=="tanktop" & Data$Description == "pink_female_t-shirt_no_sleeves", c("Description")] <- "pink_female_tshirt_no_sleeves"
Data[Data$Dyad=="D14" & Data$PictureName=="tanktop" & Data$Description == "pink_female_tshirt_no_sleeves", c("HeadNoun")] <- "tshirt"
##########################################################
##################### utilityknife #######################
Data[Data$Dyad=="D02" & Data$PictureName=="utilityknife", c("Description")]
# "carpet_cutter_knife_orange" "box_cutter_orange"          "orange_knife"
Data[Data$Dyad=="D02" & Data$PictureName=="utilityknife" & Data$Description == "cutter", c("Description")] <- "carpet_cutter_knife_orange"
Data[Data$Dyad=="D02" & Data$PictureName=="utilityknife" & Data$Description == "carpet_cutter_knife_orange", c("HeadNoun")] <- "knife"
Data[Data$Dyad=="D02" & Data$PictureName=="utilityknife" & Data$Description == "box_cutter_orange", c("HeadNoun")] <- "cutter"

Data[Data$Dyad=="D03" & Data$PictureName=="utilityknife", c("Description")]
# "cutting_knife_slide_up_and_down" "xacto_knife"     "orange_xacto_knife"  "orange_xacto_knife"  
Data[Data$Dyad=="D03" & Data$PictureName=="utilityknife" & Data$Description == "cutting_knife_slide_up_and_down", c("HeadNoun")] <- "knife"

Data[Data$Dyad=="D05" & Data$PictureName=="utilityknife", c("Description")]
# "orange_thing"    "orange_box_cutter_blade_out" "box_cutter"   "orange_box_cutter"
Data[Data$Dyad=="D05" & Data$PictureName=="utilityknife" & Data$Description == "orange_box_cutter_blade_out", c("HeadNoun")] <- "cutter"
##########################################################
##################### winterboot #########################
Data[Data$Dyad=="D01" & Data$PictureName=="winterboot", c("Description")]
# "ugg_boot"           "tall_boot_with_fur" "ugg_boot"           "ugg_boot"  
Data[Data$Dyad=="D01" & Data$PictureName=="winterboot" & Data$Description == "tall_boot_with_fur", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D02" & Data$PictureName=="winterboot", c("Description")]
# "winter_female_boot_with_the_fur" "winter_boot_with_the_fur"  
Data[Data$Dyad=="D02" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D04" & Data$PictureName=="winterboot", c("Description")]
# "boot_with_fur"       "black_boot_with_fur"
Data[Data$Dyad=="D04" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D05" & Data$PictureName=="winterboot", c("Description")]
# "boot_with_fur"      "fur_top_boot"       "black_fur_top_boot" "fur_top_boot"
Data[Data$Dyad=="D05" & Data$PictureName=="winterboot" & Data$Description == "boot_with_fur", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D06" & Data$PictureName=="winterboot", c("Description")]
# "black_boot"          "black_boot"          "black_coat"          "black_boot_with_fur"
Data[Data$Dyad=="D06" & Data$PictureName=="winterboot" & Data$Description == "black_boot_with_fur", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D07" & Data$PictureName=="winterboot", c("Description")]
# "boot_with_fur" "boot_with_fur"
Data[Data$Dyad=="D07" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D09" & Data$PictureName=="winterboot", c("Description")]
# "black_winter_boot_with_fuzzy"         "black_winter_boot_with_fuzzy_no_heel"
# "flat_black_winter_boot_with_fuzzy"    "black_winter_boot"    "flat_black_winter_boot"    
Data[Data$Dyad=="D09" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D10" & Data$PictureName=="winterboot", c("Description")]
# "black_furry_boot"         "rainboots_with_fur"       "black_rainboots_with_fur"
Data[Data$Dyad=="D10" & Data$PictureName=="winterboot", c("HeadNoun")] <- "rainboots"
Data[Data$Dyad=="D10" & Data$PictureName=="winterboot" & Data$Description == "black_furry_boot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D11" & Data$PictureName=="winterboot", c("Description")]
# "waterproof_boot_with_fur_around_it" "waterproof_boot"
Data[Data$Dyad=="D11" & Data$PictureName=="winterboot" & Data$Description == "waterproof_boot_with_fur_around_it", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D12" & Data$PictureName=="winterboot", c("Description")]
# "boot_with_fur"       "black_boot"          "black_boot_with_fur"
Data[Data$Dyad=="D12" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D16" & Data$PictureName=="winterboot", c("Description")]
# "black_boot_with_straps"      "black_boot_with_five_straps"
Data[Data$Dyad=="D16" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D17" & Data$PictureName=="winterboot", c("Description")]
# "boot_with_fur" "fur_boot"    
Data[Data$Dyad=="D17" & Data$PictureName=="winterboot" & Data$Description == "boot_with_fur", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D18" & Data$PictureName=="winterboot", c("Description")]
# "black_fuzzy_boot"            "black_fuzzy_boot"            "black_boot_with_fuzz_on_top"
Data[Data$Dyad=="D18" & Data$PictureName=="winterboot" & Data$Description == "black_boot_with_fuzz_on_top", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D20" & Data$PictureName=="winterboot", c("Description")]
# "winter_boots"        "black_winter_boots"          "black_winter_boots_with_fur"  "winter_boots_with_fur" 
Data[Data$Dyad=="D20" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"

Data[Data$Dyad=="D21" & Data$PictureName=="winterboot", c("Description")]
# "winter_boot_with_fur_on_top_with_five_buckles"       "black_boot_with_fur_on_top_with_five_buckles"       
# "black_winter_boot_with_fur_on_top_with_five_buckles"
Data[Data$Dyad=="D21" & Data$PictureName=="winterboot", c("HeadNoun")] <- "boot"
##########################################################
##################### wintercoat #########################
Data[Data$Dyad=="D02" & Data$PictureName=="wintercoat", c("Description")]
# "burgundy_coat"                                       "burgundy_winter_coat"    "violet_winter_coat"                                  "burgundy_winter_coat"                               
# "burgundy_winter_coat_with_the_fur_around_the_collar" "burgundy_coat"               "burgundy_coat" 
Data[Data$Dyad=="D02" & Data$PictureName=="wintercoat" & Data$Description == "burgundy_winter_coat_with_the_fur_around_the_collar", c("HeadNoun")] <- "coat"

Data[Data$Dyad=="D05" & Data$PictureName=="wintercoat", c("Description")]
# "pink_ladies_parka"            "pinkish_red_parka_fur_at_top" "pinkish_red_parka_fur"       
Data[Data$Dyad=="D05" & Data$PictureName=="wintercoat", c("HeadNoun")] <- "parka"

Data[Data$Dyad=="D09" & Data$PictureName=="wintercoat", c("Description")]
# "purplish_winter_coat_with_fur_around_hood" "purplish_winter_coat_with_fur_around_hood"
Data[Data$Dyad=="D09" & Data$PictureName=="wintercoat", c("HeadNoun")] <- "coat"

Data[Data$Dyad=="D10" & Data$PictureName=="wintercoat", c("Description")]
# "something_you_wear_in_the_winter"        "something_you_wear_in_the_winter"                        
# "something_to_keep_you_warm_in_the_winter"  "something_to_keep_you_warm_in_the_winter_with_fur_around"
Data[Data$Dyad=="D10" & Data$PictureName=="wintercoat", c("HeadNoun")] <- "something"

Data[Data$Dyad=="D11" & Data$PictureName=="wintercoat", c("Description")]
# "burgundy_female_winter_coat"          "burgundy_female_winter_coat_with_fur"
Data[Data$Dyad=="D11" & Data$PictureName=="wintercoat" & Data$Description == "burgundy_female_winter_coat_with_fur", c("HeadNoun")] <- "coat"

Data[Data$Dyad=="D14" & Data$PictureName=="wintercoat", c("Description")]
# "red_coat"                       "coat_you_would_buy_me"          "that_red_coat_you_would_buy_me"
Data[Data$Dyad=="D14" & Data$PictureName=="wintercoat", c("HeadNoun")] <- "coat"

Data[Data$Dyad=="D19" & Data$PictureName=="wintercoat", c("Description")]
# "purple_puff_up_coat"         "coat_with_fur_at_top"        "purple_coat_with_fur_at_top"
Data[Data$Dyad=="D19" & Data$PictureName=="wintercoat", c("HeadNoun")] <- "coat"

Data[Data$Dyad=="D21" & Data$PictureName=="wintercoat", c("Description")]
# "purlish_maroon_parka_with_grey_fur_on_hood" "purpleish_redish_parka_with_fur_collar"    
Data[Data$Dyad=="D21" & Data$PictureName=="wintercoat", c("HeadNoun")] <- "parka"
##########################################################
###################### winterhat #########################
Data[Data$Dyad=="D02" & Data$PictureName=="winterhat", c("Description")]
# "blue_grey_winter_hat_with_ball_on_top" "blue_hat"    "blue_hat_with_the_ball_on_top" "blue_hat_with_the_ball_on_top"
Data[Data$Dyad=="D02" & Data$PictureName=="winterhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D09" & Data$PictureName=="winterhat", c("Description")]
# "blue_hat_with_fuzzy_ball_on_top"         "blue_kitted_hat_with_little_fuzzy_thing"
Data[Data$Dyad=="D09" & Data$PictureName=="winterhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D12" & Data$PictureName=="winterhat", c("Description")]
# "skully_hat_with_ball_on_top" "skully_hat"                 
Data[Data$Dyad=="D12" & Data$PictureName=="winterhat" & Data$Description == "skully_hat_with_ball_on_top", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D14" & Data$PictureName=="winterhat", c("Description")]
# "winter_hat_with_ball_on_top_grey_light_blue_three_shades_of_blue"
# "winter_hat_with_ball_at_top"                                     
# "winter_hat_with_ball_at_top_dark_blue_then_light_blue_then_grey" 
Data[Data$Dyad=="D14" & Data$PictureName=="winterhat", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D16" & Data$PictureName=="winterhat", c("Description")]
# "blue_wool_hat"           "blue_wool_hat"           "blue_wool_hat_with_puff" "blue_wool_hat"          
# "blue_hat"                "blue_hat"      
Data[Data$Dyad=="D16" & Data$PictureName=="winterhat" & Data$Description == "blue_wool_hat_with_puff", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D17" & Data$PictureName=="winterhat", c("Description")]
# "hat_with_fuzzy_on_top" "blue_puffy_hat"       
Data[Data$Dyad=="D17" & Data$PictureName=="winterhat" & Data$Description == "hat_with_fuzzy_on_top", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D18" & Data$PictureName=="winterhat", c("Description")]
# "fuzzy_hat_with_ball" "knitted_hat"        
Data[Data$Dyad=="D18" & Data$PictureName=="winterhat" & Data$Description == "fuzzy_hat_with_ball", c("HeadNoun")] <- "hat"

Data[Data$Dyad=="D21" & Data$PictureName=="winterhat", c("Description")]
# "blue_tricolor_hat_with_pompom" "blue_tricolor_hat_with_pompom"
Data[Data$Dyad=="D21" & Data$PictureName=="winterhat", c("HeadNoun")] <- "hat"
##########################################################
##################### woodenchair ########################
Data[Data$Dyad=="D05" & Data$PictureName=="woodenchair", c("Description")]
# "straight_backed_chair"                        "brown_straight_backed_chair"                 
# "four_legged_brown_straight_backed_chair"      "brown_straight_back_four_legged_chair_with_x"
# "x_back_brown_chair" 
Data[Data$Dyad=="D05" & Data$PictureName=="woodenchair" & Data$Description == "brown_straight_back_four_legged_chair_with_x", c("HeadNoun")] <- "chair"

Data[Data$Dyad=="D10" & Data$PictureName=="woodenchair", c("Description")]
# "something_you_sit_in" "kitchen_chair"        "wood_kitchen_chair"
Data[Data$Dyad=="D10" & Data$PictureName=="woodenchair" & Data$Description == "something_you_sit_in", c("HeadNoun")] <- "something"

Data[Data$Dyad=="D14" & Data$PictureName=="woodenchair", c("Description")]
# "brown_chair_suitable_for_dining_room" "wooden_brown_chair_x_across_back"    "brown_dining_room_chair"
Data[Data$Dyad=="D14" & Data$PictureName=="woodenchair", c("HeadNoun")] <- "chair"
##########################################################

## STEP 2: Cleaning the descriptions from variations that are not important for current purposes
## For instance, the difference between highheel and highheels is not important and these two versions
## must be treated as the same.

## first, one must identify the origins of those variations. First, I need to know if deleting
## all s before _ or last in the string will affect words that are not plurals.

## First, get rid of the 's. Then, devise a way around the plural s.
Data$DescriptionCleanup <- str_replace_all(Data$Description, pattern="'s", "")

## Second, look at the remaining cases with s before _
grep("s_", Data$DescriptionCleanup, value = TRUE)
## I can look at all of the cases (134) and list all the issues: 
## dress, swiss, vans, kiss, sleeveless, ladies, jesus. I'm going to delete those s and then 
## correct for those issues.

## Third, look at those cases where the s at the end of the word is at the end of the
## description:
Data[grep("s", str_sub(Data$DescriptionCleanup, -1, -1)), c("DescriptionCleanup")]
## In addition to 'kiss, 'dress', we have kisses, knives.

## Fourth, delete those 's' at the end of descriptions, and at the end of words
Data$DescriptionCleanup <- sub("s$", "", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("s_", "_", Data$DescriptionCleanup)


## Checking that these s have all been deleted.
Data[grep("s", str_sub(Data$DescriptionCleanup, -1, -1)), c("DescriptionCleanup")]

## Fifth, fixing kisse and knive, kis, dres, jesu, ladie, sleeveles, van, swis
Data$DescriptionCleanup <- gsub("knive", "knife", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("kis_", "kiss_", Data$DescriptionCleanup)
Data$DescriptionCleanup <- sub("kis$", "kiss", Data$DescriptionCleanup)

Data$DescriptionCleanup <- gsub("kisse", "kiss", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("dres", "dress", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("dre", "dress", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("jesu", "jesus", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("ladie", "lady", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("sleeveles", "sleeveless", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("van", "vans", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("swis", "swiss", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("mis", "miss", Data$DescriptionCleanup)


grep("s_", Data$DescriptionCleanup, value = TRUE)
Data[grep("s", str_sub(Data$DescriptionCleanup, -1, -1)), c("DescriptionCleanup")]


## Sixth, find all of the morphologically complex words and turn them into their stem.
Data$DescriptionCleanup <- gsub("striped", "stripe", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("stripey", "stripe", Data$DescriptionCleanup)

Data$DescriptionCleanup <- gsub("thingish", "thing", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("brownish", "brown", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("redish", "red", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("tannish", "tan", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("blackish", "black", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("greenish", "green", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("pinkish", "pink", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("purplish", "purple", Data$DescriptionCleanup)

Data$DescriptionCleanup <- gsub("furry", "fur", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("wooden", "wood", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("sleeved", "sleeve", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("toed", "toe", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("fuzzy", "fuzz", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("backed", "back", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("knitted", "knit", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("backed", "back", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("thingy", "thing", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("folding", "fold", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("foldable", "fold", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("hiker", "hike", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("hiking", "hike", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("beachy", "beach", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("gardening", "garden", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("gardener", "garden", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("comfy", "comfortable", Data$DescriptionCleanup)
Data$DescriptionCleanup <- gsub("skully", "skull", Data$DescriptionCleanup)

## BEGIN: Fixing typos on the data file (as opposed to the labels of the Textgrid themselves), April 30 2019
## The next lines allow me to clean up the data. When I started working on getting a list of description types, I looked at them and noticed many cases where two
## different types were listed even though they only differed on number or other morphological variations (e.g., long_sleeved_shirt vs. long_sleeve_shirt), 
## on treating a complex word as one word or a phrase (box_cutter vs. boxcutter). 

ddply(Data, .(PictureCode, Description), nrow)

Data <- Data %>% mutate(Description= ifelse(Data$Description=="red_heels", "red_heel", 
                                            ifelse(Data$Description=="red_high_heels", "red_high_heel", 
                                                   ifelse(Data$Description=="red_shoes", "red_shoe",
                                                          ifelse(Data$Description=="church_shoes", "church_shoe",
                                                                 ifelse(Data$Description=="black_and_white_sneakers", "black_and_white_sneaker",
                                                                        ifelse(Data$Description=="sneakers", "sneaker",
                                                                               ifelse(Data$Description=="vans", "van",
                                                                                      ifelse(Data$Description=="brown_sandals", "brown_sandal",
                                                                                             ifelse(Data$Description=="sandals", "sandal",
                                                                                                    ifelse(Data$Description=="llong_sleeved_shirt", "long_sleeve_shirt",
                                                                                                           ifelse(Data$Description=="box_cutter", "boxcutter",
                                                                                                                  ifelse(Data$Description=="Orange_box_cutter", "orange_boxcutter",
                                                                                                                         ifelse(Data$Description=="purlish_maroon_parka_with_grey_fur_on_hood", "purplish_maroon_parka_with_grey_fur_on_hood",
                                                                                                                                ifelse(Data$Description=="purpleish_redish_parka_with_fur_collar", "purplish_redish_parka_with_fur_collar",
                                                                                                                                       ifelse(Data$Description=="switch_army_knife", "swiss_army_knife", as.character(Description))))))))))))))))) 
                                                                               
## END: Fixing typos on the data file (as opposed to the labels of the Textgrid themselves), April 30 2019


## BEGIN: Analyses of the description lengths on each mention, with the goal of trying to describe what people do 
## and how what they do changes between the 2 mentions

Data$NbWordsInDescription <- (str_count(Data$Description, "_")) + 1


## BEGIN: marking the description rank 
# Dealing with the fact that some trials were split into several (usually 2) portions; getting this information in so that the description rank can be chronologically ordered
Data$TrialPortion <- ifelse(nchar(as.character(Data$Filename))==14, as.numeric(substr(Data$Filename, 14, 14)), 1)
DataO <- ddply(Data[order(Data$Dyad, Data$TrialNbDesign, Data$TrialPortion, Data$Onset_sec), ],.(Dyad, TrialNbDesign),transform, DescripRank = order(TrialPortion, Onset_sec))
## END: marking the description rank 

## trying to characterize what people are producing on those trials with more than 1 description.
# How many descriptions are we talking about?
nrow(DataO[DataO$DescripRank!="1", ])
nrow(DataO)
# 386 / 1662, 23% of descriptions produced were produced later in the trial.



####################################################################
## NAME DISPERSION / UNCERTAINTY FOR EACH CARD
## How many description types were produced for each card,        ##
## across all dyads?                                              ##
## This may be taken as possible uncertainty on how to refer to   ##
## that card, which should lead to more descriptions on mention 1 ##
####################################################################

## BEGIN: Creating a dataframe with a measure of naming / describing uncertainty for each picture.
## I am using the measure U, presented in Lachman (1973), about uncertainty in naming pictures. The greater u, the more uncertainty in the description to
## assign to the picture.

DescriptionType <- ddply((ddply(Data, .(Dyad, PictureCode, PictureName, Description), nrow)), .(PictureCode, PictureName, Description), nrow)
DescriptionType <- rename(DescriptionType, c("V1"="NbDyads"))


# !!!!! ATTENTION: July 2020: I noticed what seems like a mistake: the description 'sandal'
# was used for the dress shoes, S1A2. 
## Also: S1A3: "low_cut_unintelligable_school_sneakers"
#              unintelligable's_first_low_cut_school_sneakers_last_year's
#              unintelligable_school_sneakers
#        S1B2: change tank top into tanktop (or vice versa?)

DescriptionType_TEMP <- ddply(DescriptionType, .(PictureCode, PictureName), function(x){ 
  Total=sum(x$NbDyads)
  for(i in 1:nrow(x)){
    Prob=x[i, c("NbDyads")]/Total
    Problog = log2((x[i, c("NbDyads")]/Total))
    x[i, c("plog2p")]<- Prob*Problog
    x[i, c("TotalFreq")]<- Total
  }
  return(x)
})

PicDescription_Uncertainty<- ddply(DescriptionType_TEMP, .(PictureCode, PictureName), summarise, U=-(sum(plog2p)))
write.table(PicDescription_Uncertainty, "PicDescription_Uncertainty_IlaStudyBooklet.txt")


##########################################################
#### END: Preparing data on descriptions #################
##########################################################

##########################################################################
#### BEGIN: Creating a dataframe with # descriptions per trial per speaker
##########################################################################


## BEGIN: Adding the number of description tokens produced on each trial by each participant
NbDescriptionsPerTrial <-ddply(DataO, .(Dyad, PictureCode, Mention, TrialNbDesign, DyadSpeaker, Role, ChoiceEval), nrow)
NbDescriptionsPerTrial <- rename(NbDescriptionsPerTrial, c("V1"="NbDescriptionsPerTrial"))
NbDescriptionsPerTrial$DyadSpeakerID<- str_replace_all(NbDescriptionsPerTrial$DyadSpeaker, pattern = "-", "_")
table(NbDescriptionsPerTrial$NbDescriptionsPerTrial, useNA="always")

xtabs(~Mention + NbDescriptionsPerTrial + Role, NbDescriptionsPerTrial)
#, , Role = director
#
#NbDescriptionsPerTrial
#Mention   1   2   3   4   5
#1 502 105  22   7   0
#2 543  81  14   1   1
#
#, , Role = matcher
#
#NbDescriptionsPerTrial
#Mention   1   2   3   4   5
#1  38   8   3   1   0
#2  27   3   0   0   0

xtabs(~Dyad + NbDescriptionsPerTrial, NbDescriptionsPerTrial[NbDescriptionsPerTrial$Mention==1, ])
xtabs(~Dyad + NbDescriptionsPerTrial, NbDescriptionsPerTrial[NbDescriptionsPerTrial$Mention==2, ])
## There are more trials with only 1 description in mention 2 than in mention 1, illustrating the fact
## that people engage into a collaboration in order to coordinate meaning-understanding MORE the first
## time they discuss a card than the second time. 
## There is quite a bit of variability among dyads.

## !!!!IMPORTANT!!! HERE, I WANT TO KNOW HOW MANY DESCRIPTIONS WERE PRODUCED PER ROLE, INCLUDING WHEN NONE WAS PRODUCED. 
## The .drop=FALSE option keeps for the conditions for which there is no data listed in the resulting dataframe.

Summary_NbDescriptionsPerTrialPerRole <-ddply(NbDescriptionsPerTrial, .(Dyad, TrialNbDesign, Role), summarise, NbDescripPerRole=sum(NbDescriptionsPerTrial), .drop=FALSE)


## To delete the lines corresponding to the missing trials
Summary_NbDescriptionsPerTrialPerRole[Summary_NbDescriptionsPerTrialPerRole$Dyad=="D02" & Summary_NbDescriptionsPerTrialPerRole$TrialNbDesign=="2-06", ]
# rows 203 and 204
Summary_NbDescriptionsPerTrialPerRole[Summary_NbDescriptionsPerTrialPerRole$Dyad=="D02" & Summary_NbDescriptionsPerTrialPerRole$TrialNbDesign=="2-05", ]
# rows 201 and 202
Summary_NbDescriptionsPerTrialPerRole[Summary_NbDescriptionsPerTrialPerRole$Dyad=="D14" & Summary_NbDescriptionsPerTrialPerRole$TrialNbDesign=="1-06", ]
# rows 1547 and 1548
Summary_NbDescriptionsPerTrialPerRole[Summary_NbDescriptionsPerTrialPerRole$Dyad=="D14" & Summary_NbDescriptionsPerTrialPerRole$TrialNbDesign=="1-05", ]
# rows 1545 and 1546
nrow(Summary_NbDescriptionsPerTrialPerRole[c(1:200, 205:1544, 1549:2560), ])
Summary_NbDescriptionsPerTrialPerRole_noMiss<-Summary_NbDescriptionsPerTrialPerRole[c(1:200, 205:1544, 1549:2560), ] 

TrialDesignVideoCode_datC$DyadDirectorID <- paste(TrialDesignVideoCode_datC$Dyad, TrialDesignVideoCode_datC$DirectorID, sep="_")
TrialDesignVideoCode_datC$DyadMatcherID <- paste(TrialDesignVideoCode_datC$Dyad, TrialDesignVideoCode_datC$MatcherID, sep="_")

TrialDesignVideoCode_datCtemp <-merge(TrialDesignVideoCode_datC, ParticipantsDemographBooklet[, c("ID", "Age", "Gender", "Extraversion", "Agreeableness","Conscientiousness", "Neuroticism", "Openness", "OverallADHD", "Attention", "SESEduc")], 
                                      by.x=c("DyadDirectorID"), by.y=c("ID"), all.x=TRUE, all.y=FALSE)

TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Age"="AgeDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Gender"="GenderDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Extraversion"="ExtraversionDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Agreeableness"="AgreeablenessDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Conscientiousness"="ConscientiousnessDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Neuroticism"="NeuroticismDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Openness"="OpennessDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("OverallADHD"="OverallADHDDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("Attention"="AttentionDir"))
TrialDesignVideoCode_datCtemp <- rename(TrialDesignVideoCode_datCtemp, c("SESEduc"="SESEducDir"))

Trial_Data <-merge(TrialDesignVideoCode_datCtemp, ParticipantsDemographBooklet[, c("ID", "Age", "Gender", "Extraversion", "Agreeableness","Conscientiousness", "Neuroticism", "Openness", "OverallADHD", "Attention", "SESEduc")], 
                   by.x=c("DyadMatcherID"), by.y=c("ID"), all.x=TRUE, all.y=FALSE)

Trial_Data <- rename(Trial_Data, c("Age"="AgeMatch"))
Trial_Data <- rename(Trial_Data, c("Gender"="GenderMatch"))
Trial_Data <- rename(Trial_Data, c("Extraversion"="ExtraversionMatch"))
Trial_Data <- rename(Trial_Data, c("Agreeableness"="AgreeablenessMatch"))
Trial_Data <- rename(Trial_Data, c("Conscientiousness"="ConscientiousnessMatch"))
Trial_Data <- rename(Trial_Data, c("Neuroticism"="NeuroticismMatch"))
Trial_Data <- rename(Trial_Data, c("Openness"="OpennessMatch"))
Trial_Data <- rename(Trial_Data, c("OverallADHD"="OverallADHDMatch"))
Trial_Data <- rename(Trial_Data, c("Attention"="AttentionMatch"))
Trial_Data <- rename(Trial_Data, c("SESEduc"="SESEducMatch"))

# I would like to label each trial with its rank / order as the experiment took place. I need to know 
# which set people were tested on first.

TestingList <-read.table('ParticipantTesting_List.txt', header=TRUE)

Trial_DataCTEMP <- merge(Trial_Data, TestingList[ , c("Dyad", "SetOrder")], by=c("Dyad"))

## BEGIN: Creating a dataframe with the total number of descriptions in a trial, regardless of who produced them.
NbDescriptionsPerTrial_Total <-ddply(DataO, .(Dyad, TrialNbDesign), nrow)
NbDescriptionsPerTrial_Total <- rename(NbDescriptionsPerTrial_Total, c("V1"="NbDescriptionsPerTrial"))
## END: Creating a dataframe with the total number of descriptions in a trial, regardless of who produced them.


Trial_DataC<- merge(Trial_DataCTEMP, NbDescriptionsPerTrial_Total[ , c("Dyad", "TrialNbDesign", "NbDescriptionsPerTrial")], by.x=c("Dyad", "TrialNb"),
                    by.y=c("Dyad", "TrialNbDesign"), all.x=TRUE, all.y=FALSE)

Trial_DataC$TrialOrder <- ifelse(Trial_DataC$SetOrder=="1-2", ifelse(substr(Trial_DataC$TrialNb, 1, 1)=="1", as.numeric(substr(Trial_DataC$TrialNb, 3, 4)), 
                                                                     (as.numeric(substr(Trial_DataC$TrialNb, 3, 4))+32)), 
                                 ifelse(substr(Trial_DataC$TrialNb, 1, 1)=="1", (as.numeric(substr(Trial_DataC$TrialNb, 3, 4))+32), 
                                        as.numeric(substr(Trial_DataC$TrialNb, 3, 4))))

Trial_DataC_TEMP <- merge(Trial_DataC, PicDescription_Uncertainty, by=c("PictureCode", "PictureName"), all.x=TRUE, all.y=FALSE)

Trial_Data_noMiss<- Trial_DataC_TEMP[Trial_DataC_TEMP$ChoiceEval!="missed", ]


NbDescriptionsPerTrialPerRole_C<- merge(Summary_NbDescriptionsPerTrialPerRole_noMiss, Trial_Data_noMiss[ , c("Dyad", "DyadMatcherID", "DyadDirectorID", 
                                                                                                             "TrialNb", "PictureCode", "Mention", "ChoiceEval", "TrialOrder")], by.x=c("Dyad", "TrialNbDesign"),
                                        by.y=c("Dyad", "TrialNb"), all.x=TRUE, all.y=TRUE)

NbDescriptionsPerTrialPerRole_C$DyadSpeakerID <- ifelse(NbDescriptionsPerTrialPerRole_C$Role=="director", 
                                                        NbDescriptionsPerTrialPerRole_C$DyadDirectorID, NbDescriptionsPerTrialPerRole_C$DyadMatcherID)


## Merging with Participants demographic data
Data_NbDescriptionPerRolePerTrial_TEMP<- merge(NbDescriptionsPerTrialPerRole_C, ParticipantsDemographBooklet[ , c("ID", "Gender", "Age", "Extraversion", 
                                                                                                      "Agreeableness", "Conscientiousness", "Neuroticism", "Openness", "OverallADHD", "Attention", "SESEduc")], 
                                          by.x=c("DyadSpeakerID"), by.y=c("ID"))

table(Data_NbDescriptionPerRolePerTrial_TEMP$NbDescripPerRole, useNA="always")

## Adding a measure of name dispersion  / uncertainty for each picture / card.

Data_NbDescriptionPerRolePerTrial<- merge(Data_NbDescriptionPerRolePerTrial_TEMP, PicDescription_Uncertainty, by=c("PictureCode"), all.x=TRUE, all.y=TRUE)

## END: Adding the number of description tokens produced on each trial by each participant


##########################################################################
#### END: Creating a dataframe with # descriptions per trial per speaker
##########################################################################



###################
## modeling errors
###################

# Looking to identity what can predict whether a trial will be correct (1) or incorrect (0).
# Predictors: Trial order (from 1 to 64), Mention (1 or 2), matcher's characteristics, 
# director's characteristics.

Trial_Data_noMiss$CorrectCode <- ifelse(Trial_Data_noMiss$ChoiceEval=="correct", 1, 0)


Trial_Data_noMiss$TrialOrder.c <- Trial_Data_noMiss$TrialOrder-mean(Trial_Data_noMiss$TrialOrder)

Trial_Data_noMiss$U.c <- Trial_Data_noMiss$U-mean(Trial_Data_noMiss$U)


Trial_Data_noMiss$Mention.Sum <- as.factor(Trial_Data_noMiss$Mention)
contrasts(Trial_Data_noMiss$Mention.Sum) <- cbind("1"=c(1, -1))

Trial_Data_noMiss$NbDescriptionsPerTrial.c <- Trial_Data_noMiss$NbDescriptionsPerTrial - mean(Trial_Data_noMiss$NbDescriptionsPerTrial)

#Modeling Director's characteristics--SKIP FOR NOW
#Trial_Data_noMiss$GenderDir.Sum <- as.factor(Trial_Data_noMiss$GenderDir)
#contrasts(Trial_Data_noMiss$GenderDir.Sum) <- cbind("female"=c(1, -1))
## base group is negative, "male", so the contrast says sth about prob of reuse when speaker is female

#Trial_Data_noMiss$AgeDir.c <- Trial_Data_noMiss$AgeDir - mean(Trial_Data_noMiss$AgeDir, na.rm=TRUE)

#Trial_Data_noMiss$ExtraversionDir <- as.numeric(Trial_Data_noMiss$ExtraversionDir)
#Trial_Data_noMiss$ExtraversionDir.c <- Trial_Data_noMiss$ExtraversionDir - mean(Trial_Data_noMiss$ExtraversionDir, na.rm=TRUE)

#Trial_Data_noMiss$AgreeablenessDir <- as.numeric(Trial_Data_noMiss$AgreeablenessDir)
#Trial_Data_noMiss$AgreeablenessDir.c <- Trial_Data_noMiss$AgreeablenessDir - mean(Trial_Data_noMiss$AgreeablenessDir, na.rm=TRUE)

#Trial_Data_noMiss$ConscientiousnessDir <- as.numeric(Trial_Data_noMiss$ConscientiousnessDir)
#Trial_Data_noMiss$ConscientiousnessDir.c <- Trial_Data_noMiss$ConscientiousnessDir - mean(Trial_Data_noMiss$ConscientiousnessDir, na.rm=TRUE)

#Trial_Data_noMiss$NeuroticismDir <- as.numeric(Trial_Data_noMiss$NeuroticismDir)
#Trial_Data_noMiss$NeuroticismDir.c <- Trial_Data_noMiss$NeuroticismDir - mean(Trial_Data_noMiss$NeuroticismDir, na.rm=TRUE)

#Trial_Data_noMiss$OpennessDir <- as.numeric(Trial_Data_noMiss$OpennessDir)
#Trial_Data_noMiss$OpennessDir.c <- Trial_Data_noMiss$OpennessDir - mean(Trial_Data_noMiss$OpennessDir, na.rm=TRUE)

#Trial_Data_noMiss$OverallADHDDir <- as.numeric(Trial_Data_noMiss$OverallADHDDir)
#Trial_Data_noMiss$OverallADHDDir.c <- Trial_Data_noMiss$OverallADHDDir - mean(Trial_Data_noMiss$OverallADHDDir, na.rm=TRUE)

#Trial_Data_noMiss$AttentionDir <- as.numeric(Trial_Data_noMiss$AttentionDir)
#Trial_Data_noMiss$AttentionDir.c <- Trial_Data_noMiss$AttentionDir - mean(Trial_Data_noMiss$AttentionDir, na.rm=TRUE)

#Trial_Data_noMiss$SESEducDir <- as.numeric(Trial_Data_noMiss$SESEducDir)
#Trial_Data_noMiss$SESEducDir.c <- Trial_Data_noMiss$SESEducDir - mean(Trial_Data_noMiss$SESEducDir, na.rm=TRUE)


Trial_Data_noMiss$GenderMatch.Sum <- as.factor(Trial_Data_noMiss$GenderMatch)
contrasts(Trial_Data_noMiss$GenderMatch.Sum) <- cbind("female"=c(1, -1))
## base group is negative, "male", so the contrast says sth about prob of reuse when speaker is female

Trial_Data_noMiss$AgeMatch.c <- Trial_Data_noMiss$AgeMatch - mean(Trial_Data_noMiss$AgeMatch, na.rm=TRUE)

Trial_Data_noMiss$ExtraversionMatch <- as.numeric(Trial_Data_noMiss$ExtraversionMatch)
Trial_Data_noMiss$ExtraversionMatch.c <- Trial_Data_noMiss$ExtraversionMatch - mean(Trial_Data_noMiss$ExtraversionMatch, na.rm=TRUE)

Trial_Data_noMiss$AgreeablenessMatch <- as.numeric(Trial_Data_noMiss$AgreeablenessMatch)
Trial_Data_noMiss$AgreeablenessMatch.c <- Trial_Data_noMiss$AgreeablenessMatch - mean(Trial_Data_noMiss$AgreeablenessMatch, na.rm=TRUE)

Trial_Data_noMiss$ConscientiousnessMatch <- as.numeric(Trial_Data_noMiss$ConscientiousnessMatch)
Trial_Data_noMiss$ConscientiousnessMatch.c <- Trial_Data_noMiss$ConscientiousnessMatch - mean(Trial_Data_noMiss$ConscientiousnessMatch, na.rm=TRUE)

Trial_Data_noMiss$NeuroticismMatch <- as.numeric(Trial_Data_noMiss$NeuroticismMatch)
Trial_Data_noMiss$NeuroticismMatch.c <- Trial_Data_noMiss$NeuroticismMatch - mean(Trial_Data_noMiss$NeuroticismMatch, na.rm=TRUE)

Trial_Data_noMiss$OpennessMatch <- as.numeric(Trial_Data_noMiss$OpennessMatch)
Trial_Data_noMiss$OpennessMatch.c <- Trial_Data_noMiss$OpennessMatch - mean(Trial_Data_noMiss$OpennessMatch, na.rm=TRUE)

Trial_Data_noMiss$OverallADHDMatch <- as.numeric(Trial_Data_noMiss$OverallADHDMatch)
Trial_Data_noMiss$OverallADHDMatch.c <- Trial_Data_noMiss$OverallADHDMatch - mean(Trial_Data_noMiss$OverallADHDMatch, na.rm=TRUE)

Trial_Data_noMiss$AttentionMatch <- as.numeric(Trial_Data_noMiss$AttentionMatch)
Trial_Data_noMiss$AttentionMatch.c <- Trial_Data_noMiss$AttentionMatch - mean(Trial_Data_noMiss$AttentionMatch, na.rm=TRUE)

Trial_Data_noMiss$SESEducMatch <- as.numeric(Trial_Data_noMiss$SESEducMatch)
Trial_Data_noMiss$SESEducMatch.c <- Trial_Data_noMiss$SESEducMatch - mean(Trial_Data_noMiss$SESEducMatch, na.rm=TRUE)


Error.m0 <- glmer(CorrectCode ~ TrialOrder.c + Mention.Sum + NbDescriptionsPerTrial.c + NbDescriptionsPerTrial.c:Mention.Sum + 
                    (1 | Dyad) + (1 | PictureCode),
                    data=Trial_Data_noMiss, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))

#Fixed effects:
#(Intercept)                            6.19487    0.97888   6.329 2.48e-10 ***
#  TrialOrder.c                           0.08282    0.01898   4.363 1.28e-05 ***
#  Mention.Sum1                           0.15584    0.25351   0.615    0.539    
#NbDescriptionsPerTrial.c              -0.18775    0.28875  -0.650    0.516    
#Mention.Sum1:NbDescriptionsPerTrial.c  0.13628    0.28469   0.479    0.632    

Error.m1 <- glmer(CorrectCode ~ TrialOrder.c + Mention.Sum + NbDescriptionsPerTrial.c + NbDescriptionsPerTrial.c:Mention.Sum +
                  (1 + TrialOrder.c | Dyad) + (1 | PictureCode),
                  data=Trial_Data_noMiss, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))

#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)    
#Estimate Std. Error z value Pr(>|z|)    
#(Intercept)                            5.82441    1.04553   5.571 2.54e-08 ***
#TrialOrder.c                           0.05914    0.04263   1.387    0.165    
#Mention.Sum1                           0.17302    0.26237   0.659    0.510    
#NbDescriptionsPerTrial.c              -0.19138    0.29188  -0.656    0.512    
#Mention.Sum1:NbDescriptionsPerTrial.c  0.13547    0.28909   0.469    0.639  

# ==> the effect of trial number is no longer significant if I let the slope of the effect vary for each dyad... 

## MODEL THAT TRIES TO PREDICT TRIAL ACCURACY BASED ON THE MATCHER'S FEATURES.
### UPDATE April 18 2019: It looks like predicting accuracy on a trial basis yields no
## significant predictors (cf. preceding models). Perhaps a more fruitful approach is to
## predict at the individual level (matcher).
## perhaps predicting the number of errors?

Error.m2 <- glmer(CorrectCode ~ TrialOrder.c + Mention.Sum + NbDescriptionsPerTrial.c + NbDescriptionsPerTrial.c:Mention.Sum + U.c + 
                  #                            GenderDir.Sum +
                  #                            AgeDir.c +
                  #                            SESEducDir.c +
                  #                            ExtraversionDir.c +
                  #                            AgreeablenessDir.c +
                  #                            ConscientiousnessDir.c +
                  #                            NeuroticismDir.c +
                  #                            OpennessDir.c +
                  #                            OverallADHDDir.c +
                  #                            AttentionDir.c +
                                     GenderMatch.Sum +
                                     AgeMatch.c +
                                     SESEducMatch.c +
                  #                   SESEducMatch.c:U.c +
                                     ExtraversionMatch.c +
                                     AgreeablenessMatch.c +
                                     ConscientiousnessMatch.c +
                                     NeuroticismMatch.c +
                                     OpennessMatch.c +
                                     OverallADHDMatch.c +
                                     AttentionMatch.c +
                (1 | DyadMatcherID) + (1 | PictureCode),    
 #               (1 + TrialOrder.c | DyadMatcherID) + (1 | PictureCode),
                data=Trial_Data_noMiss, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"), na.action=na.exclude)

#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)    
#(Intercept)                            6.66571    0.97047   6.869 6.49e-12 ***
#TrialOrder.c                           0.07761    0.01982   3.917 8.98e-05 ***
#Mention.Sum1                          -0.03276    0.27986  -0.117   0.9068    
#NbDescriptionsPerTrial.c              -0.32672    0.30020  -1.088   0.2764    
#GenderMatch.Sumfemale                  0.81545    0.46182   1.766   0.0774 .  
#AgeMatch.c                            -0.12069    0.05780  -2.088   0.0368 *  
#SESEducMatch.c                         0.69352    0.40315   1.720   0.0854 .  
#ExtraversionMatch.c                   -0.11850    0.11846  -1.000   0.3171    
#AgreeablenessMatch.c                  -0.17781    0.11823  -1.504   0.1326    
#ConscientiousnessMatch.c               0.05717    0.14837   0.385   0.7000    
#NeuroticismMatch.c                     0.03005    0.08017   0.375   0.7078    
#OpennessMatch.c                        0.23383    0.14827   1.577   0.1148    
#OverallADHDMatch.c                    -0.12191    0.05814  -2.097   0.0360 *  
#AttentionMatch.c                       0.40358    0.18977   2.127   0.0334 *  
#Mention.Sum1:NbDescriptionsPerTrial.c  0.20242    0.29140   0.695   0.4873    

### ATTENTION: The model with random slope for TrialOrder doesn't converge, but I 
## suspect if it did, the effect of trial order would not be significant anymore.

## MODEL PREDICTING ACCURACY BASED ON MATCHER'S AND DIRECTOR'S FEATURES
Error.m3 <- glmer(CorrectCode ~ TrialOrder.c + Mention.Sum +
                                   GenderDir.Sum +
                                    AgeDir.c +
                                    SESEducDir.c +
                                    ExtraversionDir.c +
                                    AgreeablenessDir.c +
                                    ConscientiousnessDir.c +
                                    NeuroticismDir.c +
                                    OpennessDir.c +
                                    OverallADHDDir.c +
                                    AttentionDir.c +
      GenderMatch.Sum +
      AgeMatch.c +
        SESEducMatch.c +
        ExtraversionMatch.c +
        AgreeablenessMatch.c +
        ConscientiousnessMatch.c +
        NeuroticismMatch.c +
        OpennessMatch.c +
        OverallADHDMatch.c +
        AttentionMatch.c +
        (1 + TrialOrder.c | Dyad) + (1 | PictureCode),
      data=Trial_Data_noMiss, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"), na.action=na.exclude)

## This model had no sign effect for speaker specific features....the complexity of the model is probably too great given the number of data points.


## MODEL THAT TRIES TO PREDICT TRIAL ACCURACY BASED ON THE DIRECTOR'S FEATURES.

#Error.m4 <- glmer(CorrectCode ~ TrialOrder.c + Mention.Sum +
#                    GenderDir.Sum +
#                    AgeDir.c +
#                    SESEducDir.c +
#                    ExtraversionDir.c +
#                    AgreeablenessDir.c +
#                    ConscientiousnessDir.c +
#                    NeuroticismDir.c +
#                    OpennessDir.c +
#                    OverallADHDDir.c +
#                    AttentionDir.c +
#                    GenderMatch.Sum +
#                    AgeMatch.c +
#                    SESEducMatch.c +
#                    ExtraversionMatch.c +
#                    AgreeablenessMatch.c +
#                    ConscientiousnessMatch.c +
#                    NeuroticismMatch.c +
#                    OpennessMatch.c +
#                    OverallADHDMatch.c +
#                    AttentionMatch.c +
#                    (1 + TrialOrder.c | DyadDirectorID) + (1 | PictureCode),
#                  data=Trial_Data_noMiss, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"), na.action=na.exclude)

#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)    
#(Intercept)             6.46936    1.03854   6.229 4.69e-10 ***
#TrialOrder.c            0.10144    0.03999   2.536   0.0112 *  
#Mention.Sum1            0.15127    0.28636   0.528   0.5973    
#GenderDir.Sumfemale     0.39430    0.44869   0.879   0.3795    
#AgeDir.c               -0.05507    0.03803  -1.448   0.1476    
#SESEducDir.c            0.59996    0.27411   2.189   0.0286 *  
#ExtraversionDir.c       0.01049    0.08970   0.117   0.9069    
#AgreeablenessDir.c     -0.11501    0.09405  -1.223   0.2214    
#ConscientiousnessDir.c  0.01367    0.11330   0.121   0.9040    
#NeuroticismDir.c       -0.13252    0.07258  -1.826   0.0679 .  
#OpennessDir.c           0.15134    0.11661   1.298   0.1943    
#OverallADHDDir.c        0.04067    0.04825   0.843   0.3993    
#AttentionDir.c         -0.15879    0.15804  -1.005   0.3150

##BEGIN: Correlations between accuracy and speaker's characteristics. Now that regressions work, not necessary.
Dyad_Accuracy_SpeakerCharacteristics <- cbind(ddply(Trial_Data_noMiss, .(Dyad), summarise, Accuracy=sum(as.numeric(CorrectCode)), AgeMatcher=mean(AgeMatch), EducMatcher=mean(SESEducMatch), 
      AgeDirector=mean(AgeDir), EducDirector=mean(SESEducDir)), ddply(Trial_Data_noMiss, .(Dyad), nrow))

Dyad_Accuracy_SpeakerCharacteristics <- rename(Dyad_Accuracy_SpeakerCharacteristics, c("V1"="TotalNbTrials"))

Dyad_Accuracy_SpeakerCharacteristics$AccuracyRate <- Dyad_Accuracy_SpeakerCharacteristics$Accuracy / Dyad_Accuracy_SpeakerCharacteristics$TotalNbTrials

# Relationship between accuracy and age of the matcher?
cor.test(x=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AccuracyRate")], y=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AgeMatcher")], method = 'pearson')
#
#	Pearson's product-moment correlation
#data:  Dyad_Accuracy_SpeakerCharacteristics[, c("AccuracyRate")] and Dyad_Accuracy_SpeakerCharacteristics[, c("AgeMatcher")]
#t = -2.1215, df = 18, p-value = 0.04802
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#  -0.742763589 -0.005893678
#sample estimates:
#  cor 
#-0.447248 

# Relationship between accuracy and age of the director? (NOTE: I use the Pearson method rather than the Spearman (rank) method because there are many dyads at level
## accuracy of 1... so many ties!)
cor.test(x=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AccuracyRate")], y=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AgeDirector")], method = 'pearson')
#
#	Pearson's product-moment correlation
#data:  Dyad_Accuracy_SpeakerCharacteristics[, c("AccuracyRate")] and Dyad_Accuracy_SpeakerCharacteristics[, c("AgeDirector")]
#t = -2.1043, df = 18, p-value = 0.04967
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#  -0.74112910 -0.00225761
#sample estimates:
#  cor 
#-0.4443345 

# ==> There is a sign and negative correlation between age of either participant and accuracy: The older, the worse their performance

# Relationship between accuracy and SES?
cor.test(x=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AccuracyRate")], y=Dyad_Accuracy_SpeakerCharacteristics[ ,c("EducMatcher")], method = 'pearson')
#data:  Dyad_Accuracy_SpeakerCharacteristics[, c("AccuracyRate")] and Dyad_Accuracy_SpeakerCharacteristics[, c("EducMatcher")]
#t = 2.4879, df = 16, p-value = 0.02426
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#  0.081331 0.798200
#sample estimates:
#  cor 
#0.5281468 

cor.test(x=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AccuracyRate")], y=Dyad_Accuracy_SpeakerCharacteristics[ ,c("EducDirector")], method = 'pearson')
#data:  Dyad_Accuracy_SpeakerCharacteristics[, c("AccuracyRate")] and Dyad_Accuracy_SpeakerCharacteristics[, c("EducDirector")]
#t = 2.4814, df = 16, p-value = 0.02457
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#  0.0799715 0.7977029
#sample estimates:
#  cor 
#0.5271594

#==> SES appears to also correlate with performance: The more educated the participants (director or matcher) or their parents, the better their performance

## Relationship between age and SES? (NOTE: I use the Pearson method rather than the Spearman (rank) method because there are many dyads at level
## accuracy of 1... so many ties!)

cor.test(x=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AgeMatcher")], y=Dyad_Accuracy_SpeakerCharacteristics[ ,c("EducMatcher")], method = 'pearson')

cor.test(x=Dyad_Accuracy_SpeakerCharacteristics[ ,c("AgeDirector")], y=Dyad_Accuracy_SpeakerCharacteristics[ ,c("EducDirector")], method = 'pearson')

#==> The two dimensions of age and SES are very weakly (and negatively) correlated 

##END: Correlations between accuracy and speaker's characteristics. Now that regressions work, not necessary.

## CONCLUSIONS: Although there are few errors (33 / 1276, 2.6% of the data), there errors are not randomly distributed across the dyads. The rate of accuracy for 
## a dyad is partly correlated with the age  of the matchers and the education of the participants; matcher's gender seems to make a difference too, as well as their
## agreeableness (the more agreeable, the less accurate). Also, errors declined in the course of the study, with practice with the procedure/setting.

###################
## END: modeling errors
###################


#######################################################################
#### ANALYSES OF NUMBER OF DESCRIPTIONS PER TRIAL PER PARTICIPANT #####
#######################################################################

## How many trials with more than 1 description produced are there? Across mentions and roles?

xtabs(~ Mention + NbDescripPerRole + Role, Data_NbDescriptionPerRolePerTrial)
#, , Role = director
#
#          NbDescripPerRole
#Mention   0   1   2   3   4   5
#      1   0 502 105  22   7   0
#      2   0 543  81  14   1   1
#
#, , Role = matcher
#
#          NbDescripPerRole
#Mention   0   1   2   3   4   5
#      1 586  38   8   3   1   0
#      2 610  27   3   0   0   0

## => more such trials on mention 2 than mention 1, unsurprisingly, and that's true of both roles. 

## How are they distributed across cards?
xtabs(~ PictureCode + Role + Mention, Data_NbDescriptionPerRolePerTrial[Data_NbDescriptionPerRolePerTrial$NbDescripPerRole>1, ])

##==> Those multiple-description trials are widely distributed across cards, although some cards have more of them than others. 

## Plotting the relationship between the U value associated with each card, and its average number of descriptions per trial.

NbDescriptionU_bothroles <- ddply(Data_NbDescriptionPerRolePerTrial, .(U, Mention), summarise, meanNbDescrip=mean(NbDescripPerRole))

NbDescriptionU_directorsonly <- ddply(Data_NbDescriptionPerRolePerTrial[Data_NbDescriptionPerRolePerTrial$Role=="director", ], .(U, Mention), summarise, meanNbDescrip=mean(NbDescripPerRole))

png(filename = "NbDescriptionU_directorsonly_Rplot.png")
ggplot(NbDescriptionU_directorsonly, aes(U, meanNbDescrip)) +
     geom_point() + 
  stat_smooth(method="lm", color="black") +
   facet_wrap(~Mention) + 
  labs(x="U (name uncertainty / dispersion)", 
       y="number of descriptions produced per card") +
  theme(text = element_text(size=20)) + 
  ggtitle("Mean number of descriptions\non 1st or 2nd mention")
dev.off()

# ==> There is a strong correlation between the mean number of descriptions (per mention) produced by directors and the value of name uncertainty
#     across the entire sample. This suggests that directors may feel the dispersion or lack of clear convention for some pictures and work harder on
#     those. 


## Plotting the relationship between the SES Educ associated with each participant, and average number of descriptions per trial.

NbDescriptionSESEduc_bothroles <- ddply(na.omit(Data_NbDescriptionPerRolePerTrial), .(DyadSpeakerID, SESEduc, Mention, Role), summarise, meanNbDescrip=mean(NbDescripPerRole))

png(filename = "NbDescriptionSESEduc_bothroles_Rplot.png")
ggplot(NbDescriptionSESEduc_bothroles, aes(SESEduc, meanNbDescrip, color=Role)) +
  geom_point() + 
  stat_smooth(method="lm") +
  facet_wrap(~Role + Mention) + 
  geom_hline(yintercept = 0, size=1, linetype="dashed") + 
  geom_hline(yintercept = 1, size=1, linetype="dashed") + 
  labs(x="Participant's level of education", 
       y="number of descriptions\nproduced per card") +
  theme(text = element_text(size=20)) + 
  ggtitle("Mean number of descriptions\non 1st or 2nd mention")
dev.off()


## Preparing for regression

Data_NbDescriptionPerRolePerTrial$U.c <- Data_NbDescriptionPerRolePerTrial$U - mean(Data_NbDescriptionPerRolePerTrial$U)

Data_NbDescriptionPerRolePerTrial$Mention.Sum <- as.factor(Data_NbDescriptionPerRolePerTrial$Mention)
contrasts(Data_NbDescriptionPerRolePerTrial$Mention.Sum) <- cbind("1"=c(1, -1))

Data_NbDescriptionPerRolePerTrial$ChoiceEval.Sum <- as.factor(Data_NbDescriptionPerRolePerTrial$ChoiceEval)
contrasts(Data_NbDescriptionPerRolePerTrial$ChoiceEval.Sum) <- cbind("correct"=c(1, -1))

Data_NbDescriptionPerRolePerTrial$Role.Sum <- as.factor(Data_NbDescriptionPerRolePerTrial$Role)
contrasts(Data_NbDescriptionPerRolePerTrial$Role.Sum) <- cbind("director"=c(1, -1))


Data_NbDescriptionPerRolePerTrial$Gender.Sum <- as.factor(Data_NbDescriptionPerRolePerTrial$Gender)
contrasts(Data_NbDescriptionPerRolePerTrial$Gender.Sum) <- cbind("female"=c(1, -1))
## base group is negative, "male", so the contrast says sth about prob of reuse when speaker is female

Data_NbDescriptionPerRolePerTrial$Age.c <- Data_NbDescriptionPerRolePerTrial$Age - mean(Data_NbDescriptionPerRolePerTrial$Age, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$Extraversion <- as.numeric(Data_NbDescriptionPerRolePerTrial$Extraversion)
Data_NbDescriptionPerRolePerTrial$Extraversion.c <- Data_NbDescriptionPerRolePerTrial$Extraversion - mean(Data_NbDescriptionPerRolePerTrial$Extraversion, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$Agreeableness <- as.numeric(Data_NbDescriptionPerRolePerTrial$Agreeableness)
Data_NbDescriptionPerRolePerTrial$Agreeableness.c <- Data_NbDescriptionPerRolePerTrial$Agreeableness - mean(Data_NbDescriptionPerRolePerTrial$Agreeableness, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$Conscientiousness <- as.numeric(Data_NbDescriptionPerRolePerTrial$Conscientiousness)
Data_NbDescriptionPerRolePerTrial$Conscientiousness.c <- Data_NbDescriptionPerRolePerTrial$Conscientiousness - mean(Data_NbDescriptionPerRolePerTrial$Conscientiousness, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$Neuroticism <- as.numeric(Data_NbDescriptionPerRolePerTrial$Neuroticism)
Data_NbDescriptionPerRolePerTrial$Neuroticism.c <- Data_NbDescriptionPerRolePerTrial$Neuroticism - mean(Data_NbDescriptionPerRolePerTrial$Neuroticism, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$Openness <- as.numeric(Data_NbDescriptionPerRolePerTrial$Openness)
Data_NbDescriptionPerRolePerTrial$Openness.c <- Data_NbDescriptionPerRolePerTrial$Openness - mean(Data_NbDescriptionPerRolePerTrial$Openness, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$OverallADHD <- as.numeric(Data_NbDescriptionPerRolePerTrial$OverallADHD)
Data_NbDescriptionPerRolePerTrial$OverallADHD.c <- Data_NbDescriptionPerRolePerTrial$OverallADHD - mean(Data_NbDescriptionPerRolePerTrial$OverallADHD, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$Attention <- as.numeric(Data_NbDescriptionPerRolePerTrial$Attention)
Data_NbDescriptionPerRolePerTrial$Attention.c <- Data_NbDescriptionPerRolePerTrial$Attention - mean(Data_NbDescriptionPerRolePerTrial$Attention, na.rm=TRUE)

Data_NbDescriptionPerRolePerTrial$SESEduc <- as.numeric(Data_NbDescriptionPerRolePerTrial$SESEduc)
Data_NbDescriptionPerRolePerTrial$SESEduc.c <- Data_NbDescriptionPerRolePerTrial$SESEduc - mean(Data_NbDescriptionPerRolePerTrial$SESEduc, na.rm=TRUE)


nbDescrip.m0 <- lmer(NbDescripPerRole ~ Mention.Sum + Role.Sum + ChoiceEval.Sum + 
        Gender.Sum +
        Age.c +
        SESEduc.c +
        Extraversion.c +
        Agreeableness.c +
        Conscientiousness.c +
        Neuroticism.c +
        Openness.c +
        OverallADHD.c +
        Attention.c +
        (1 + Mention.Sum + Role.Sum | DyadSpeakerID) + (1 | PictureCode),
      data=Data_NbDescriptionPerRolePerTrial, na.action=na.exclude)

#Fixed effects:
#  Estimate Std. Error t value
#(Intercept)            0.6853431  0.0363879   18.83
#Mention.Sum1           0.0342681  0.0108332    3.16 <=
#Role.Sumdirector       0.5683852  0.0161224   35.25 <=
#ChoiceEval.Sumcorrect -0.0070573  0.0289715   -0.24
#Gender.Sumfemale        -0.0547281  0.0278611  1.96 <-
#Age.c                  0.0025567  0.0014565    1.76
#SESEduc.c              0.0179878  0.0091099    1.97 <-
#Extraversion.c        -0.0017426  0.0029167   -0.60
#Agreeableness.c       -0.0049317  0.0037082   -1.33
#Conscientiousness.c    0.0033006  0.0049321    0.67
#Neuroticism.c         -0.0037410  0.0022255   -1.68
#Openness.c             0.0002668  0.0042136    0.06
#OverallADHD.c          0.0010447  0.0013177    0.79
#Attention.c            0.0024847  0.0045103    0.55


nbDescrip.m1 <- lmer(NbDescripPerRole ~ Mention.Sum + Role.Sum + ChoiceEval.Sum + Mention.Sum:Role.Sum +
                       Gender.Sum +
                       Age.c +
                       SESEduc.c + SESEduc.c:Mention.Sum +
                       Extraversion.c +
                       Agreeableness.c +
                       Conscientiousness.c +
                       Neuroticism.c +
                       Openness.c +
                       OverallADHD.c +
                       Attention.c +
                       (1 + Mention.Sum + Role.Sum | DyadSpeakerID) + (1 | PictureCode),
                     data=Data_NbDescriptionPerRolePerTrial, na.action=na.exclude)

#Fixed effects:
#  Estimate Std. Error t value
#(Intercept)             0.6896169  0.0363577   18.97
#Mention.Sum1            0.0341956  0.0095236    3.59 <=
#Role.Sumdirector        0.5683853  0.0161644   35.16 <=
#ChoiceEval.Sumcorrect  -0.0105765  0.0288750   -0.37
#Gender.Sumfemale       -0.0563693  0.0283380    1.99 <=
#Age.c                   0.0024458  0.0014814    1.65
#SESEduc.c               0.0240216  0.0093405    2.57 <=
#Extraversion.c         -0.0018515  0.0029660   -0.62
#Agreeableness.c        -0.0049962  0.0037717   -1.32
#Conscientiousness.c     0.0034703  0.0050163    0.69
#Neuroticism.c          -0.0037181  0.0022634   -1.64
#Openness.c              0.0002258  0.0042856    0.05
#OverallADHD.c           0.0010767  0.0013402    0.80
#Attention.c             0.0024041  0.0045871    0.52
#Mention.Sum1:Role.Sumdirector  0.0067490  0.0085072    0.79
#Mention.Sum1:SESEduc.c  0.0224159  0.0057602    3.89 <=

## More descriptions on mention 1 than on mention 2, many more from directors than matchers, more from female speakers than male speakers, more from
## higly educated speakers than less educated ones, and the difference betwen mentions 1 and 2 is greater for highly educated speakers. 

nbDescrip.m2 <- lmer(NbDescripPerRole ~ Mention.Sum + Role.Sum + ChoiceEval.Sum + Mention.Sum:Role.Sum +
                       Gender.Sum +
                       Age.c +
                       SESEduc.c + SESEduc.c:Mention.Sum + SESEduc.c:Role.Sum +
                       Extraversion.c +
                       Agreeableness.c +
                       Conscientiousness.c +
                       Neuroticism.c +
                       Openness.c +
                       OverallADHD.c +
                       Attention.c +
                       (1 + Mention.Sum + Role.Sum | DyadSpeakerID) + (1 | PictureCode),
                     data=Data_NbDescriptionPerRolePerTrial, na.action=na.exclude)

#Fixed effects:
#  Estimate Std. Error t value
#(Intercept)                    0.6615209  0.0325708   20.31
#Mention.Sum1                   0.0341985  0.0095216    3.59 <=
#Role.Sumdirector               0.5683751  0.0161285   35.24 <=
#ChoiceEval.Sumcorrect         -0.0106983  0.0288773   -0.37
#Gender.Sumfemale               0.0281756  0.0141712    1.99 <=?
#Age.c                          0.0024465  0.0014816    1.65
#SESEduc.c                      0.0303655  0.0109396    2.78 <=
#Extraversion.c                -0.0018699  0.0029664   -0.63
#Agreeableness.c               -0.0049897  0.0037723   -1.32
#Conscientiousness.c            0.0034740  0.0050171    0.69
#Neuroticism.c                 -0.0037163  0.0022638   -1.64
#Openness.c                     0.0002288  0.0042863    0.05
#OverallADHD.c                  0.0010788  0.0013404    0.80
#Attention.c                    0.0024017  0.0045878    0.52
#Mention.Sum1:Role.Sumdirector  0.0067619  0.0085073    0.79
#Mention.Sum1:SESEduc.c         0.0211012  0.0058799    3.59 <=
#Role.Sumdirector:SESEduc.c     0.0111069  0.0099844    1.11

## I added the interaction between Role and SES to see if I could show that educated people speak more as matcher than less educated people. 
## The interaction sign, positive, goes in the opposite direction. Perhaps what is missing is interaction with mention. 

nbDescrip.m3 <- lmer(NbDescripPerRole ~ Mention.Sum + Role.Sum + Mention.Sum:Role.Sum + U.c + U.c:Mention.Sum + U.c:Role.Sum +
                       Gender.Sum +
                       Age.c +
                       SESEduc.c + SESEduc.c:Mention.Sum + SESEduc.c:Role.Sum + SESEduc.c:Role.Sum:Mention.Sum +
#                      SESEduc.c:U.c +
                       Extraversion.c +
                       Agreeableness.c +
                       Conscientiousness.c +
                       Neuroticism.c +
                       Openness.c +
                       OverallADHD.c +
                       Attention.c +
                       (1 + Mention.Sum | DyadSpeakerID) + (1 | PictureCode),
                     data=Data_NbDescriptionPerRolePerTrial, na.action=na.exclude)

#Fixed effects:
#  Estimate Std. Error t value
#(Intercept)                              0.6606167  0.0325361   20.30
#Mention.Sum1                             0.0340673  0.0094425    3.61 <=
#Role.Sumdirector                         0.5680943  0.0086757   65.48 <=
#ChoiceEval.Sumcorrect                   -0.0098145  0.0291749   -0.34
#U.c                                      0.0417573  0.0114378    3.65 <=
#Gender.Sumfemale                         0.0320244  0.0185577    1.73
#Age.c                                    0.0030198  0.0019405    1.56
#SESEduc.c                                0.0300949  0.0121261    2.48 <=
#Extraversion.c                          -0.0023272  0.0038761   -0.60
#Agreeableness.c                         -0.0034634  0.0049375   -0.70
#Conscientiousness.c                      0.0031207  0.0065689    0.48
#Neuroticism.c                           -0.0024939  0.0029637   -0.84
#Openness.c                               0.0019686  0.0056147    0.35
#OverallADHD.c                            0.0018653  0.0017555    1.06
#Attention.c                              0.0012851  0.0060065    0.21
#Mention.Sum1:Role.Sumdirector            0.0064526  0.0086760    0.74
#Mention.Sum1:U.c                         0.0003495  0.0096593    0.04
#Mention.Sum1:SESEduc.c                   0.0211187  0.0058308    3.62 <=
#Role.Sumdirector:SESEduc.c               0.0105370  0.0053770    1.96 <=
#Mention.Sum1:Role.Sumdirector:SESEduc.c  0.0058350  0.0053776    1.09

## Nope!! But the lack
## of significant interaction and the sign of the estimate may only reflect the fact that matchers were heavily involved as well. So I can perhaps think of the
## lack of interaction as suggesting that it is not just that educated directors spoke more than less educated ones, but that both directors AND matchers did it.

# I added the interaction between the effects of SES and U, and although it was in the right direction, it did not approach significance.


############################################################################
#### END: ANALYSES OF NUMBER OF DESCRIPTIONS PER TRIAL PER PARTICIPANT #####
############################################################################

NbDescriptionsPerTrial_Total <-ddply(DataO, .(Dyad, TrialNbDesign), nrow)
NbDescriptionsPerTrial_Total <- rename(NbDescriptionsPerTrial_Total, c("V1"="NbDescriptionsPerTrial"))


DataC<- merge(DataO, NbDescriptionsPerTrial_Total[ , c("Dyad", "TrialNbDesign", "NbDescriptionsPerTrial")], by=c("Dyad", "TrialNbDesign"), all.x=TRUE, all.y=FALSE)

# Here I characterize a description as being last or not last, based on the total number of descriptions on this trial
DataC$LastNotLast <- ifelse(DataC$DescripRank==DataC$NbDescriptionsPerTrial, "last", "notlast")

xtabs( ~ DescripRank + Role, DataC)
#                Role
#DescripRank director matcher
#1              1276       0
#2               191      61
#3                67      25
#4                20       9
#5                 5       5
#6                 3       0

xtabs( ~ DescripRank + Role, DataC[DataC$NbDescriptionsPerTrial!=1, ])
#                Role
#DescripRank director matcher
#1               252       0
#2               191      61
#3                67      25
#4                20       9
#5                 5       5
#6                 3       0

# How many trials had only one description? distribution across role?
xtabs( ~ DescripRank + Role, DataC[DataC$NbDescriptionsPerTrial==1, ])
#Role
#DescripRank director
#       1     1024
# Unsurprisingly, all of the 1024 trials that has only one description were from the director.

xtabs( ~ DescripRank + Role, DataC[DataC$NbDescriptionsPerTrial==2, ])
#Role
#DescripRank director matcher
#      1      160       0
#      2      141      19
# This is the distribution of those 160 trials that ended with two descriptions produced: In the large majority of the cases,
# the 2nd description was also from the director (141 out of 160). 

xtabs( ~ DescripRank + Role, DataC[DataC$NbDescriptionsPerTrial>2, ])
#Role
#DescripRank director matcher
#      1       92       0
#      2       50      42
#      3       67      25
#      4       20       9
#      5        5       5
#      6        3       0
# of those that do not stop after 2 descriptions (92), who produced the 2nd description is more balanced (50 from director, 42 from matcher)


xtabs( ~ DescripRank + Role, DataC[DataC$NbDescriptionsPerTrial==3, ])
#Role
#DescripRank director matcher
#      1       63       0
#      2       34      29
#      3       53      10

xtabs( ~ DescripRank + Role, DataC[DataC$NbDescriptionsPerTrial>3, ])
#Role
#DescripRank director matcher
#      1       29       0
#      2       16      13
#      3       14      15
#      4       20       9
#      5        5       5
#      6        3       0


# It seems like the director is much more likely to be the one producing the last description than the matcher, regardless of the total number of descriptions produced
# on a trial. Can I show this using 'LastNotLast' field?

xtabs( ~ DescripRank + Role, DataC[DataC$LastNotLast=="last", ])
#Role
#DescripRank director matcher
#       1     1024       0       100%
#       2      141      19        88%
#       3       53      10        84%
#       4       15       4        78%
#       5        5       2        71%
#       6        3       0       100%

##

# I want to compare the descriptions produced on DescripRank 1 with those produced on DescripRank 2 on the description's number of words 
# This is in order to see (roughly) if a new description tends to be an expansion or a revision of the preceding one.
#ddply(DataC[DataC$NbDescriptionsPerTrial!=1, ], .(Role, DescripRank), summarise, MeanNbWords=mean(NbWordsInDescription))
# This approach is just not useful because it is not clear which description precedes or follows which one. 

DataC$Description <- tolower(DataC$Description)

#write.table(DataC, "DataRawDescriptions_Ila_Booklet.txt")

## I am concerned that the simplication description cases (in Mention 2, compared to descriptions in Mention 1) may sometimes be cases of underspecification. That is less likely to be 
## the case for descriptions with more than 1 word, perhaps. What is the distribution of descriptions of various length?

xtabs(~Mention + NbWordsInDescription, DataC)
#NbWordsInDescription
#Mention   1   2   3   4   5   6   7   8   9  10  11  13  14
#1       113 382 175  84  52  27  18   8   7   4   1   1   1
#2        78 384 173  59  41  21  16   7   5   1   3   1   0

## What do those (frequent) 1-word descriptions like?
unique(DataC[DataC$NbWordsInDescription==1 & DataC$Mention==1, ]$Description)
# It's a mix bag: some 1-word descriptions use specific words (stiletto) while others use what may be like underspecified terms (knife, shoe, shirt). 
unique(DataC[DataC$NbWordsInDescription==1 & DataC$Mention==2, ]$Description)

# This may be a problem if I try to compare each description in mention 2 to each description in mention 1, because some of those will be underspecified but corrected.

## Let's first try to characterize what people do when they produce more than one description per card on Mention1; this may help decide how to handle those cases when
## comparing descriptions produced on mention 2 to those on mention 1.

xtabs(~NbWordsInDescription + DescripRank, DataC[DataC$Mention==1, ])
# with this format, it's hard to see what happens to description over time (i.e., rank)

#####################################################################
## CHARACTERIZATION OF ALL DESCRIPTIONS (tokens) PRODUCED BY DYADS ##
#####################################################################

#################################################################
## BEGIN: LOOKING AT DEFINITENESS on each mention independently
#################################################################

# Do dyads differ in their use of definite NP on Mention 1 (ignoring the few cases where definiteness could not be coded)?
Def_Prob_Mention1 <- prop.table((xtabs(~ Dyad + Definiteness, subset(DataC, (DataC$Mention=="1" & DataC$Definiteness != "uncodable")))), 1)
# Some dyads used no article on most referring expressions used on Mention 1 (D01, D02, D06, D20)
Def_Prob_Mention2 <- prop.table((xtabs(~ Dyad + Definiteness, subset(DataC, (DataC$Mention=="2" & DataC$Definiteness != "uncodable")))), 1)

# Evaluating whether dyads increased their use of definiteness between mentions 1 and 2
cbind(Def_Prob_Mention1[, 1], Def_Prob_Mention2[ ,1])

# It varies: It may be that if use of definiteness within a particular range on mention 1, it increased; if it was high, it didn't; 
# if it was too low, it didn't either.

## Mean % of def on mentions 1 and 2
mean(Def_Prob_Mention1[, 1])
mean(Def_Prob_Mention2[, 1])
## % of def exp went from 36% to 46%

## Mean % of indef on mentions 1 and 2
mean(Def_Prob_Mention1[, 2])
mean(Def_Prob_Mention2[, 2])
## % of indef exp went from 23% to 14%

## Mean % of no article on mentions 1 and 2
mean(Def_Prob_Mention1[, 3])
mean(Def_Prob_Mention2[, 3])
## % of no article exp stayed at 40% on both mentions
# A more proper analysis should be conducted on a trial by trial basis. The % are only computed per dyad, except that this is difficult to do because
# there are more than 1 description on some trials...

## Is there a difference in the way the different forms of the NP (def, indef, or no) are used in the first description offered and in the subsequent ones?
# separately for each dyad
xtabs(~ Dyad + Definiteness, subset(DataC, (DataC$Mention=="1" & DataC$Definiteness != "uncodable" & DataC$DescripRank==1)))
xtabs(~ Dyad + Definiteness, subset(DataC, (DataC$Mention=="1" & DataC$Definiteness != "uncodable" & DataC$DescripRank>1)))

# all dyads together
xtabs(~ Definiteness + DescripRank, subset(DataC, (DataC$Mention=="1" & DataC$Definiteness != "uncodable")))
#                  DescripRank
#Definiteness   1   2   3   4   5   6
#   def       241  44  13   5   1   1
#   indef     167  29  11   3   3   1
#   no        221  73  34  13   5   0
#   uncodable   0   0   0   0   0   0

prop.table((xtabs(~ Definiteness + DescripRank, subset(DataC, (DataC$Mention=="1" & DataC$Definiteness != "uncodable")))), 2)

#                                           DescripRank
#Definiteness         1         2         3         4         5         6
#def       0.3831479 0.3013699 0.2241379 0.2380952 0.1111111 0.5000000
#indef     0.2655008 0.1986301 0.1896552 0.1428571 0.3333333 0.5000000
#no        0.3513514 0.5000000 0.5862069 0.6190476 0.5555556 0.0000000

## => There is some evidence that the rate of definite NP used on Mention 1 decreases across multiple descriptions: 
## They are more frequent on the first description offered (about 38%) than on the subsequent ones (decreases from 30% to 11%).
## BUT the rate of indef NP decreases too... What does increase is the rate of bare NP.

## Could it be, though, a by-product of the number of descriptions total in the trial? It is not that the second description
## tends to use fewer Def NP than the first, it is in fact that on those trials when more than 1 description is
## produced, ALL of the descriptions tend to use something different than def NPs....


## I can address this by selecting those trials with more than 1 descriptions and see how the % of def NP changes with rank.

prop.table((xtabs(~ Definiteness + DescripRank, subset(DataC, (DataC$Mention=="1" & DataC$Definiteness != "uncodable" & DataC$NbDescriptionsPerTrial>1)))), 2)

#                   DescripRank
#Definiteness         1         2         3         4         5         6
#   def       0.4206897 0.3013699 0.2241379 0.2380952 0.1111111 0.5000000
#   indef     0.2965517 0.1986301 0.1896552 0.1428571 0.3333333 0.5000000
#   no        0.2827586 0.5000000 0.5862069 0.6190476 0.5555556 0.0000000
#   uncodable 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000

# ==> this confirms that on those trials with more than 1 description produced, there are more def NPs at first, and then people tend to move to producing 
# more bare NPs.


## Same analysis related to the use of definiteNP and the rank of the description, but on Mention 2

prop.table((xtabs(~ Definiteness + DescripRank, subset(DataC, (DataC$Mention=="2" & DataC$Definiteness != "uncodable")))), 2)
#                     DescripRank
#Definiteness          1          2          3          4          5          6
#   def       0.45682889 0.49038462 0.50000000 0.50000000 0.00000000 1.00000000
#   indef     0.14442700 0.10576923 0.08823529 0.12500000 0.00000000 0.00000000
#   no        0.39874411 0.40384615 0.41176471 0.37500000 1.00000000 0.00000000
#   uncodable 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000

## Here, and contrary to what was observed on Mention 1, the % of def NP doesn't decrease on descriptions of rank greater than 1.

prop.table((xtabs(~ Definiteness + DescripRank, subset(DataC, (DataC$Mention=="2" & DataC$Definiteness != "uncodable" & DataC$NbDescriptionsPerTrial>1)))), 2)

#                     DescripRank
#Definiteness          1          2          3          4          5          6
#   def       0.54807692 0.49038462 0.50000000 0.50000000 0.00000000 1.00000000
#   indef     0.16346154 0.10576923 0.08823529 0.12500000 0.00000000 0.00000000
#   no        0.28846154 0.40384615 0.41176471 0.37500000 1.00000000 0.00000000
#   uncodable 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000

# Even when looking only at those trials with more than 1 descriptions, the rate of def NP stays stable between the first and second descriptions



# Predicting for the use of definite NP (vs. not definite, which includes indefinite and no article) as a function of order of description and mention 
# and speaker's characteristics?

DataC$DefiniteCode <- ifelse(DataC$Definiteness=="def", 1, ifelse(DataC$Definiteness=="indef" | DataC$Definiteness=="no", 0, NA))

DataC$BareCode <- ifelse(DataC$Definiteness=="no", 1, ifelse(DataC$Definiteness=="def" | DataC$Definiteness=="indef", 0, NA))

DataC$DyadSpeakerID<- str_replace_all(DataC$DyadSpeaker, pattern = "-", "_")

# BEGIN: Include Picture Naming uncertainty and speaker's characteristics
DataC_TEMP<- merge(DataC, PicDescription_Uncertainty, by=c("PictureCode", "PictureName"), all.x=TRUE, all.y=FALSE)
DataC_SpeakerInfo <-merge(DataC_TEMP, ParticipantsDemographBooklet[, c("ID", "Age", "Gender", "Extraversion", 
              "Agreeableness","Conscientiousness", "Neuroticism", "Openness", "OverallADHD", "Attention", "SESEduc")], by.x=c("DyadSpeakerID"), 
              by.y=c("ID"), all.x=TRUE, all.y=FALSE)
# END: Include Picture Naming uncertainty and speaker's characteristics

PercentageDefBare_SESEduc_DyadSpeakerMention1 <- ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable" & DataC_SpeakerInfo$Mention=="1", ], .(DyadSpeakerID, SESEduc), summarise, DefiniteNP=mean(DefiniteCode), 
           BareNP=mean(BareCode))
PercentageDefBare_SESEduc_DyadSpeakerMention2 <- ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable" & DataC_SpeakerInfo$Mention=="2", ], .(DyadSpeakerID, SESEduc), summarise, DefiniteNP=mean(DefiniteCode), 
                                                       BareNP=mean(BareCode))

# Is there a correlation between SES educ and % definiteNP on mention 1 and 2?
cor.test(as.numeric(PercentageDefBare_SESEduc_DyadSpeakerMention1$SESEduc), PercentageDefBare_SESEduc_DyadSpeakerMention1$DefiniteNP)
#t = 0.64162, df = 36, p-value = 0.525
cor.test(as.numeric(PercentageDefBare_SESEduc_DyadSpeakerMention2$SESEduc), PercentageDefBare_SESEduc_DyadSpeakerMention2$DefiniteNP)
#t = 2.5185, df = 36, p-value = 0.01637

# Is there a correlation between SES educ and % BareNP on mention 1 and 2?
cor.test(as.numeric(PercentageDefBare_SESEduc_DyadSpeakerMention1$SESEduc), PercentageDefBare_SESEduc_DyadSpeakerMention1$BareNP)
# = -1.9927, df = 36, p-value = 0.05392
cor.test(as.numeric(PercentageDefBare_SESEduc_DyadSpeakerMention2$SESEduc), PercentageDefBare_SESEduc_DyadSpeakerMention2$BareNP)
#t = -2.223, df = 36, p-value = 0.03258


## !!!!!!!!!!! IMPORTANT REGARDING HOW TO INTERPRET THE RELATIONSHIP BETWEEN DEFINITENESS AND SES EDUC
## IMPORTANT: The relationship between defNP and SES Educ is largely due to the relationship between participants' education level
## and their propensity to use bare nouns. This, I believe, is related to trying to say as little as possible (although it may
## be argued that the difference between a bare NP and an def NP is minimal...)




ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable", ], .(Mention, RoleMatch, Definiteness), nrow)
# Mention RoleMatch Definiteness  V1
#       1        na          def 305
#       1        na        indef 214
#       1        na           no 346
#       2 different          def 175
#       2 different        indef  55
#       2 different           no 165
#       2      same          def 189
#       2      same        indef  52
#       2      same           no 149


## Effect of U on the production of def NP
ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable", ], .(Mention, U), summarise, DefiniteNP=mean(DefiniteCode))

png(filename = "DefNP_Rate_U_directorsonly_Rplot.png")
ggplot(ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable", ], .(Mention, U), summarise, DefiniteNP=mean(DefiniteCode)), aes(U, DefiniteNP)) +
      geom_point() +
      stat_smooth(method="lm", color="black") +
      facet_wrap(~Mention) +
      labs(x="U (name uncertainty / dispersion)", 
      y="Proportion of definite NPs\nproduced by directors") +
      theme(text = element_text(size=20)) + 
      ggtitle("Proportion of definite Noun Phrases\non 1st or 2nd mention")
dev.off()

# ==> on mention 1, no effect of U on def NP production. on mention 2, the higher U, the higher def NP.

median(PicDescription_Uncertainty$U)
# 4.090815

#looking at the effect of mention over U by doing a median split of the pictures in terms of their U value

ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable" & DataC_SpeakerInfo$U<4.09, ], .(Mention), summarise, DefNP=mean(DefiniteCode))
# Rate of defNP production on the half of the pictures with LOW U VALUES
#Mention     DefNP
#1       1 0.3530864
#2       2 0.4393531

ddply(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable" & DataC_SpeakerInfo$U>4.09, ], .(Mention), summarise, DefNP=mean(DefiniteCode))
# Rate of defNP production on the half of the pictures with HIGH U VALUES
#Mention     DefNP
#1       1 0.3521739
#2       2 0.4855072

## Plotting the relationship between SES Educ and Def NP rate

DefNPRatePerParticipant_SESEduc <- ddply(na.omit(DataC_SpeakerInfo[DataC_SpeakerInfo$Definiteness!="uncodable" & DataC_SpeakerInfo$Role=="director", ]), .(DyadSpeakerID, SESEduc, Mention), summarise, DefNP=mean(DefiniteCode))

png(filename = "DefNP_Rate_SESEduc_directors_Rplot.png")
ggplot(DefNPRatePerParticipant_SESEduc, aes(SESEduc, DefNP)) +
  geom_point(position = position_jitter(width=0.01)) + 
  stat_smooth(method="lm", color="black") +
  facet_wrap(~ Mention) + 
  labs(x="Participant's level of education", 
       y="Proportion of definite Noun Phrases\nproduced by the director") +
  theme(text = element_text(size=20)) + 
  ggtitle("Proportion of definite Noun Phrases\non 1st or 2nd mention")
dev.off()



## Preparing for regression analysis on prob of producing a definite NP 

DataC_SpeakerInfo$Mention.Dev <- as.factor(DataC_SpeakerInfo$Mention)
contrasts(DataC_SpeakerInfo$Mention.Dev) <- cbind("1"=c(1/2, -1/2))

#DataC_SpeakerInfo$ChoiceEval.Sum <- as.factor(DataC_SpeakerInfo$ChoiceEval)
#contrasts(DataC_SpeakerInfo$ChoiceEval.Sum) <- cbind("correct"=c(1, -1))

DataC_SpeakerInfo$Role.Dev <- as.factor(DataC_SpeakerInfo$Role)
contrasts(DataC_SpeakerInfo$Role.Dev) <- cbind("director"=c(1/2, -1/2))


DataC_SpeakerInfo$Gender.Dev <- as.factor(DataC_SpeakerInfo$Gender)
contrasts(DataC_SpeakerInfo$Gender.Dev) <- cbind("female"=c(1/2, -1/2))
## base group is negative, "male", so the contrast says sth about prob of reuse when speaker is female

DataC_SpeakerInfo$DescripRank.c <- DataC_SpeakerInfo$DescripRank - mean(DataC_SpeakerInfo$DescripRank, na.rm=TRUE)

DataC_SpeakerInfo$NbDescriptionsPerTrial.c <- DataC_SpeakerInfo$NbDescriptionsPerTrial - mean(DataC_SpeakerInfo$NbDescriptionsPerTrial)

DataC_SpeakerInfo$Age.c <- DataC_SpeakerInfo$Age - mean(DataC_SpeakerInfo$Age, na.rm=TRUE)

DataC_SpeakerInfo$Extraversion <- as.numeric(DataC_SpeakerInfo$Extraversion)
DataC_SpeakerInfo$Extraversion.c <- DataC_SpeakerInfo$Extraversion - mean(DataC_SpeakerInfo$Extraversion, na.rm=TRUE)

DataC_SpeakerInfo$Agreeableness <- as.numeric(DataC_SpeakerInfo$Agreeableness)
DataC_SpeakerInfo$Agreeableness.c <- DataC_SpeakerInfo$Agreeableness - mean(DataC_SpeakerInfo$Agreeableness, na.rm=TRUE)

DataC_SpeakerInfo$Conscientiousness <- as.numeric(DataC_SpeakerInfo$Conscientiousness)
DataC_SpeakerInfo$Conscientiousness.c <- DataC_SpeakerInfo$Conscientiousness - mean(DataC_SpeakerInfo$Conscientiousness, na.rm=TRUE)

DataC_SpeakerInfo$Neuroticism <- as.numeric(DataC_SpeakerInfo$Neuroticism)
DataC_SpeakerInfo$Neuroticism.c <- DataC_SpeakerInfo$Neuroticism - mean(DataC_SpeakerInfo$Neuroticism, na.rm=TRUE)

DataC_SpeakerInfo$Openness <- as.numeric(DataC_SpeakerInfo$Openness)
DataC_SpeakerInfo$Openness.c <- DataC_SpeakerInfo$Openness - mean(DataC_SpeakerInfo$Openness, na.rm=TRUE)

DataC_SpeakerInfo$OverallADHD <- as.numeric(DataC_SpeakerInfo$OverallADHD)
DataC_SpeakerInfo$OverallADHD.c <- DataC_SpeakerInfo$OverallADHD - mean(DataC_SpeakerInfo$OverallADHD, na.rm=TRUE)

DataC_SpeakerInfo$Attention <- as.numeric(DataC_SpeakerInfo$Attention)
DataC_SpeakerInfo$Attention.c <- DataC_SpeakerInfo$Attention - mean(DataC_SpeakerInfo$Attention, na.rm=TRUE)

DataC_SpeakerInfo$SESEduc <- as.numeric(DataC_SpeakerInfo$SESEduc)
DataC_SpeakerInfo$SESEduc.c <- DataC_SpeakerInfo$SESEduc - mean(DataC_SpeakerInfo$SESEduc, na.rm=TRUE)

DataC_SpeakerInfo$U.c <- DataC_SpeakerInfo$U - mean(DataC_SpeakerInfo$U, na.rm=TRUE)


## Regression model for predicting the production of a Definite NP (vs. indef or no article), as a function of:
# (1) mention: people may use define NP more on mention 2 than mention 1
# (2) role: directors see a booklet, they may take a more generic (indef, attributive) approach to the object than the matcher
# (3) DescripRank: people may use definite more on their first description, and then may use attribute later
# (4) individual characteristics of the speaker producing the [referring] expression.

#DefiniteProd.m0 <- glmer(DefiniteCode ~ Mention.Sum + Role.Sum + DescripRank.c + 
#                       Gender.Sum +
#                       Age.c +
#                       SESEduc.c +
#                       Extraversion.c +
#                       Agreeableness.c +
#                       Conscientiousness.c +
#                       Neuroticism.c +
#                       Openness.c +
#                       OverallADHD.c +
#                       Attention.c +
#                       (1 + Mention.Sum + Role.Sum | DyadSpeakerID) + (1 | PictureCode),
#                     data=DataC_SpeakerInfo, na.action=na.exclude, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))

#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)   
#(Intercept)         -0.83646    0.33553  -2.493  0.01267 * 
#Mention.Sum1        -0.28122    0.19192  -1.465  0.14285   
#Role.Sumdirector     0.08384    0.19612   0.428  0.66900   
#DescripRank.c       -0.19506    0.11060  -1.764  0.07780 . 
#Gender.Sumfemale    -0.94009    0.32114  -2.927  0.00342 **
#Age.c                0.01058    0.03086   0.343  0.73163   
#SESEduc.c            0.11312    0.18883   0.599  0.54914   
#Extraversion.c       0.08618    0.05754   1.498  0.13424   
#Agreeableness.c      0.05747    0.07719   0.744  0.45655   
#Conscientiousness.c  0.10004    0.10223   0.978  0.32781   
#Neuroticism.c        0.01113    0.04972   0.224  0.82293   
#Openness.c          -0.28215    0.10217  -2.762  0.00575 **
#OverallADHD.c        0.06468    0.02891   2.237  0.02529 * 
#Attention.c         -0.14538    0.09498  -1.531  0.12586

## ==> There is a marginal effect of rank of description: The greater the rank, the less likely a definite NP is. That makes sense: Speakers move from
## referring to the card to describing what type is on the card.
## Effect of gender, openness, and overallADHD, but not sure what to make of them.

## ADDING INTERACTION BETWEEN MENTION AND IND DIFF (AGE AND SESEDUC), AS WELL AS U
DefiniteProd.m1 <- glmer(DefiniteCode ~ Mention.Dev + Role.Dev + DescripRank.c + NbDescriptionsPerTrial.c + U.c + U.c:Mention.Dev +
                           Gender.Dev +
                           Age.c +
                           SESEduc.c +
                           Extraversion.c +
                           Agreeableness.c +
                           Conscientiousness.c +
                           Neuroticism.c +
                           Openness.c +
                           OverallADHD.c +
                           Attention.c + Age.c:Mention.Dev + SESEduc.c:Mention.Dev + 
                           (1 + Mention.Dev + Role.Dev | DyadSpeakerID) + (1 | PictureCode),
                         data=DataC_SpeakerInfo, na.action=na.exclude, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))

## model nearly unidentifiable. I simplify the random structure by removing the slope for Role.

DefiniteProd.m2 <- glmer(DefiniteCode ~ Mention.Dev + Role.Dev + DescripRank.c + NbDescriptionsPerTrial.c + U.c + U.c:Mention.Dev +
           Gender.Dev +
           Age.c +
           SESEduc.c +
           Extraversion.c +
           Agreeableness.c +
           Conscientiousness.c +
           Neuroticism.c +
           Openness.c +
           OverallADHD.c +
           Attention.c + Age.c:Mention.Dev + SESEduc.c:Mention.Dev + 
           (1 + Mention.Dev | DyadSpeakerID) + (1 | PictureCode),
         data=DataC_SpeakerInfo, na.action=na.exclude, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))


#Estimate Std. Error z value Pr(>|z|)    
#(Intercept)              -0.864860   0.402640  -2.148 0.031716 *  
#Mention.Dev1             -0.582408   0.351359  -1.658 0.097401 .  
#Role.Devdirector          0.317486   0.333680   0.951 0.341367    
#DescripRank.c            -0.231208   0.138690  -1.667 0.095497 .  
#NbDescriptionsPerTrial.c  0.028667   0.100549   0.285 0.775564    
#U.c                       0.148343   0.090220   1.644 0.100126    
#Gender.Devfemale         -2.347756   0.676234  -3.472 0.000517 ***
#Age.c                    -0.008735   0.039634  -0.220 0.825562    
#SESEduc.c                 0.842933   0.256044   3.292 0.000994 ***
#Extraversion.c            0.057403   0.059200   0.970 0.332223    
#Agreeableness.c           0.087479   0.080092   1.092 0.274734    
#Conscientiousness.c       0.141548   0.103850   1.363 0.172879    
#Neuroticism.c             0.008899   0.046617   0.191 0.848614    
#Openness.c               -0.371328   0.089914  -4.130 3.63e-05 ***
#OverallADHD.c             0.053085   0.026975   1.968 0.049077 *  
#Attention.c              -0.151606   0.092371  -1.641 0.100741    
#Mention.Dev1:U.c         -0.253724   0.158560  -1.600 0.109560    
#Mention.Dev1:Age.c       -0.037752   0.031298  -1.206 0.227735    
#Mention.Dev1:SESEduc.c   -0.633107   0.220208  -2.875 0.004040 ** 

## ==> Effect of mention (less in mention 1 than in mention 2) is now marginal, and importantly, it interacts with SESEduc so that the more educated
## the speakers, the bigger the difference between mention  1 (fewer def NP) and mention 2 (yes!). 
## Female speakers produce fewer def NP (??)
## Educated speakers produce more de NP
## Open speakers produce less (??)
## While Overal ADHD is significant, it goes in the opposite direction from the more limited Attention variable, which makes me cautious about those.

## THIS IS THE MODEL UNDERLYING THE PSYCHONOMICS TALK: Same model but limited to directors (so no main effect of Role)

DefiniteProd.m3 <- glmer(DefiniteCode ~ Mention.Dev + DescripRank.c + NbDescriptionsPerTrial.c + U.c + U.c:Mention.Dev +
                           Gender.Dev +
                           Age.c +
                           SESEduc.c +
                           Extraversion.c +
                           Agreeableness.c +
                           Conscientiousness.c +
                           Neuroticism.c +
                           Openness.c +
                           OverallADHD.c +
                           Attention.c + Age.c:Mention.Dev + SESEduc.c:Mention.Dev + 
                           (1 + Mention.Dev | DyadSpeakerID) + (1 | PictureCode),
                         data=DataC_SpeakerInfo[DataC_SpeakerInfo$Role=="director", ], na.action=na.exclude, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))


#(Intercept)              -0.72408    0.39887  -1.815 0.069472 .  
#Mention.Dev1             -0.53216    0.41380  -1.286 0.198437    
#DescripRank.c            -0.23549    0.14984  -1.572 0.116041    
#NbDescriptionsPerTrial.c  0.03685    0.10728   0.343 0.731245    
#U.c                       0.16844    0.08468   1.989 0.046673 *  
#Gender.Devfemale         -2.33389    0.64664  -3.609 0.000307 ***
#Age.c                    -0.01729    0.04142  -0.417 0.676402    
#SESEduc.c                 0.96959    0.27181   3.567 0.000361 ***
#Extraversion.c            0.04198    0.05964   0.704 0.481485    
#Agreeableness.c           0.09164    0.07935   1.155 0.248170    
#Conscientiousness.c       0.15543    0.10258   1.515 0.129727    
#Neuroticism.c             0.01175    0.04683   0.251 0.801894    
#Openness.c               -0.39173    0.08953  -4.375 1.21e-05 ***
#OverallADHD.c             0.05629    0.02697   2.087 0.036843 *  
#Attention.c              -0.14804    0.09118  -1.624 0.104442    
#Mention.Dev1:U.c         -0.27125    0.16491  -1.645 0.100000    
#Mention.Dev1:Age.c       -0.03010    0.03688  -0.816 0.414402    
#Mention.Dev1:SESEduc.c   -0.75672    0.26411  -2.865 0.004168 ** 

# ==> The production of a def NP is influenced by the factors such as demographics (SES Educ, and also gender[male > female], and Openness),
#     as well as the experience of having discussed the card before (but this is true only for more educated people). The card
#     being referred to has an impact as well: Cards characterized by a greater name dispersion are referred to with a def NP more
#     than cards with a dominant name.


###############################################################
## END: LOOKING AT DEFINITENESS on each mention independently
###############################################################

####################################################################################################################
## BEGIN: *****Characterizing how subsequent descriptions produced on mention 1 related to immediately preceding one 
## as identical, expansion or just different**** 
####################################################################################################################

nrow(DataC)
nrow(DataC[DataC$Mention==1, ])
nrow(DataC[DataC$Mention==2, ])
## FIRST VERSION
#Description_Comparison_Mention1_FIRSTVERSION <- ddply(DataC[DataC$Mention==1, ], .(Dyad, PictureCode), function(x){
#  for(i in 1:nrow(x)){
#   if(i>1){
#       j= i-1
#       priorDescripSpeaker <- x[j, c("DyadSpeaker")]
#       priorRole <- x[j, c("Role")]
#       priorDescripRank <- x[j, c("DescripRank")]
#       priorDescription <- x[j, c("Description")]
#       setprior <- str_split(x[j, c("Description")], "_", simplify=TRUE)
#       setcurrent <- str_split(x[i, c("Description")], "_", simplify=TRUE)
#       compIdentity = setequal(setprior, setcurrent)
#        if(compIdentity=="TRUE"){
#         x[i, c("ComparisonWithPrior")] <- "SameDescrip"
#           }
#        else{
#          compExtansion = setdiff(setprior, setcurrent)
#            if(length(compExtansion)==0){
#              x[i, c("ComparisonWithPrior")] <- "ExpandedDescrip"
#              } 
#            else{
#              x[i, c("ComparisonWithPrior")] <- "NewDescrip"
#              }
#           }
#       x[i, c("PriorDescripSpeaker")] <- priorDescripSpeaker
#       x[i, c("PriorDescripRank")] <- priorDescripRank
#       x[i, c("PriorDescription")] <- priorDescription
#       x[i, c("PriorRole")] <- priorRole
#        } 
#   else{
#         x[i, c("ComparisonWithPrior")] <- "NoPriorDescription"
#         x[i, c("PriorDescripSpeaker")] <- "NoPriorDescripSpeaker" 
#         x[i, c("PriorDescripRank")] <- 0
#         x[i, c("PriorDescription")] <- "none"
#         x[i, c("PriorRole")] <- "noPriorRole"
#           }
#     }
#  return(x)
#   })

## SECOND VERSION

## Here, revising the program so that it goes through a for loop reviewing all preceding descriptions until it finds the same, if not, 
## a shorter (leading to conclude that the current description is an expansion from a preceding one), if not, conclude it is a new
## description. Using only the preceding one misses quite a few cases where an identical one happened before, not the one immediately before.
## The program splits the dataframe into small chunks, the set of lines that correspond to the segment about a given picture for a given dyad.
## It goes through each of the lines, in succession (using i as the counter). For each line that is later than the first one, it looks at the description
## used and test whether it presents to the same description as the line j, i-1, immediately preceding it (if so, sameDescrip, and stop the loop) or is an expansion of that immediately
## preceding line (if so, ExpandedDescrip, and stop the loop). 
## If neither same nor expanded, the current description (on line i) is compared to line j as i-2, and assess whether the current description is identical or an expansion on that i-2 description.
## Because the for loop is broken as son as an equality or expansion is found, going from the most proximal to the least proximal, I have chosen to give more weight to proximity than
## to other factors (for example, to identity, or whether the descriptions are from same vs. different speakers, etc...). The rationale for this choice is based on 
## a view of coordination, minimal joint project, and adjacency pairs, where people are supposed to uptake current project as soon as possible. 
## if the current description is neither identical nor expansion AND the for loop over j has reached the end (with j=1), then the program concludes that the current description
## is new (No prior Instance). 

## First, create a file with only types, based on the exact description.

DataType_temp <- ddply(DataC, 
                       .(Dyad, DyadSpeaker, TrialNbDesign, Description, PictureCode, RoleMatch, LastNotLast, Role, Mention, NbWordsInDescription), nrow)

DataType_temp <-rename(DataType_temp, c("V1"="NbOccurrences"))

# Second, generating a list of description types within a dyad, marking each one with who produced it.
DataType <- ddply(DataType_temp, 
                  .(Dyad, TrialNbDesign, Description, NbWordsInDescription, PictureCode, RoleMatch, Mention), summarise, Speaker=toString(substr(DyadSpeaker, 5, 5)), Role=toString(Role), 
                  TotalOccurrences = sum(NbOccurrences), TrialPosition=toString(substr(LastNotLast, 1, 3)))

table(DataType$Speaker, useNA="always")
#I need to consolidate the Speaker and Role column by summarizing whether the description was produced by A / director only, B / matcher only, or both.
# The logic of the procedure is was follows (for Speaker): If "A" is not in the concatenated Speaker column at all, the Speaker is "B" only; if "A" is present, 
# and if "B" is also present, then it's "AB"; if "B" is not also present, then it is "A" only.

DataType<- DataType%>% mutate(Speaker = ifelse(grepl("A", Speaker)==FALSE, "B", ifelse(grepl("B", Speaker)==TRUE, "AB", "A")))
table(DataType$Speaker, useNA="always")

table(DataType$Role, useNA="always")
DataType<- DataType%>% mutate(Role = ifelse(grepl("director", Role)==FALSE, "matcher", ifelse(grepl("matcher", Role)==TRUE, "both", "director")))
table(DataType$Role, useNA="always")

DataType<- DataType%>% mutate(TrialPosition = ifelse(grepl("las", TrialPosition), "Last", "NotLast"))
table(DataType$TrialPosition, useNA="always")


table((ddply(DataType[DataType$Mention=="1", ], .(Dyad, Mention, PictureCode), nrow))$V1, useNA="always")
#   1    2    3    4    5 <NA> 
# 501   94   28   11    2    0 

## PLEASE NOTE: The program tested for identity between two descriptions associated with the same card, by
## the same Dyad. This was done before working on Type data set. With types, this is no longer relevant.

Description_Comparison_Mention1 <- ddply(DataType[DataType$Mention=="1", ], .(Dyad, PictureCode), function(x){
  for(i in 1:nrow(x)){
    if(i > 1){
      setcurrent <- str_split(x[i, c("Description")], "_", simplify=TRUE)
      for(j in (i-1):1){
        setprior <- str_split(x[j, c("Description")], "_", simplify=TRUE)
#       compIdentity = setequal(setprior, setcurrent)
        compExtansion = setdiff(setprior, setcurrent)
#        if(compIdentity=="TRUE"){
#          x[i, c("ComparisonWithPrior")] <- "SameDescrip"
#          priorSpeaker <- x[j, c("Speaker")]
#          priorRole <- x[j, c("Role")]
#          priorDescription <- x[j, c("Description")]
#          x[i, c("PriorSpeaker")] <- priorSpeaker
#          x[i, c("PriorDescription")] <- priorDescription
#          x[i, c("PriorRole")] <- priorRole
#          break
#        }
#        else{
          if(length(compExtansion)==0){
            x[i, c("ComparisonWithPrior")] <- "ExpandedDescrip"
            priorSpeaker <- x[j, c("Speaker")]
            priorRole <- x[j, c("Role")]
            priorDescription <- x[j, c("Description")]
            x[i, c("PriorSpeaker")] <- priorSpeaker
            x[i, c("PriorDescription")] <- priorDescription
            x[i, c("PriorRole")] <- priorRole
            break
          } 
          else{
            if(j==1){
              x[i, c("ComparisonWithPrior")] <- "NewDescrip"
              x[i, c("PriorSpeaker")] <- "NoPriorInstanceSpeaker"
              x[i, c("PriorDescription")] <- "NoPriorInstanceDescription"
              x[i, c("PriorRole")] <-"NoPriorInstanceRole"
            }
          }
      }
    }
    else{
      x[i, c("ComparisonWithPrior")] <- "NewDescrip"
      x[i, c("PriorSpeaker")] <- "NoPriorInstanceSpeaker"
      x[i, c("PriorDescription")] <- "NoPriorInstanceDescription"
      x[i, c("PriorRole")] <-"NoPriorInstanceRole"
      }
    }
  return(x)
})

nrow(Description_Comparison_Mention1)

## Here, I need to adjust who used each description, based on "priorinstance" information. Right now, "Speaker" and "Role" list
## only the individuals who used each type description, with no considerations for expansions. I need to 
## specify who the speaker(s) and role(s) are, based on not only the exact descriptions but also 
## expansions of it.

table(DataType$Speaker, useNA="always")
table(DataType$Role, useNA="always")

Description_Comparison_Mention1$RevisedSpeaker <- ifelse((Description_Comparison_Mention1$Speaker=="A" & grepl("B", Description_Comparison_Mention1$PriorSpeaker)), "AB", 
                                                ifelse((Description_Comparison_Mention1$Speaker=="B" & grepl("A", Description_Comparison_Mention1$PriorSpeaker)), "AB", Description_Comparison_Mention1$Speaker))

Description_Comparison_Mention1$RevisedRole <- ifelse(Description_Comparison_Mention1$ComparisonWithPrior=="NewDescrip", Description_Comparison_Mention1$Role, 
                                             ifelse((Description_Comparison_Mention1$Role=="director" & grepl("h", Description_Comparison_Mention1$PriorRole)), "both", 
                                                    ifelse((Description_Comparison_Mention1$Role=="matcher" & grepl("o", Description_Comparison_Mention1$PriorRole)), "both", Description_Comparison_Mention1$Role)))

table(Description_Comparison_Mention1$ComparisonWithPrior)
#ExpandedDescrip         NewDescrip
#       96                 731

# who uses expansion or new descrip?
xtabs(~Role + ComparisonWithPrior, Description_Comparison_Mention1)
#           ComparisonWithPrior
#Role       ExpandedDescrip NewDescrip
#both                   4          20
#director              81         681
#matcher               13          29

prop.table(xtabs(~Role + ComparisonWithPrior, Description_Comparison_Mention1), 1)
#          ComparisonWithPrior
#Role       ExpandedDescrip NewDescrip
#both           0.1666667  0.8333333
#director       0.1062992  0.8937008
#matcher        0.3095238  0.6904762

xtabs(~Role  + PriorRole+ ComparisonWithPrior, Description_Comparison_Mention1)
#, , ComparisonWithPrior = ExpandedDescrip
#
#           PriorRole
#Role       both director matcher NoPriorInstanceRole
#both        0        4       0                   0
#director    5       73       3                   0
#matcher     2       11       0                   0
#
#, , ComparisonWithPrior = NewDescrip
#
#           PriorRole
#Role       both director matcher NoPriorInstanceRole
#both        0        0       0                  21
#director    0        0       0                 710
#matcher     0        0       0                  32

# Directors expand on a description they themselves introduced, and rarely on one that the matcher used.(I interpret the 2 instances of expansion by director on description used by both
# as being driven by the director's own use of that description.) By contrast, when a matcher expands on a description, it is much more likely to be on a director's. 
# But.... perhaps it is because they aren't many descriptions offered by matchers.... so this may be due to base rate...!!


PercentExpansionPerDyad_mention1 <- prop.table(xtabs(~Dyad + ComparisonWithPrior, Description_Comparison_Mention1), 1)
#      ComparisonWithPrior
#Dyad  ExpandedDescrip NewDescrip
#D01      0.02777778 0.97222222
#D02      0.07692308 0.92307692
#D03      0.04444444 0.95555556
#D04      0.15384615 0.84615385
#D05      0.14000000 0.86000000
#D06      0.00000000 1.00000000
#D07      0.02702703 0.97297297
#D08      0.02941176 0.97058824
#D09      0.05405405 0.94594595
#D10      0.05000000 0.95000000
#D11      0.02439024 0.97560976
#D12      0.02857143 0.97142857
#D14      0.16326531 0.83673469
#D16      0.05405405 0.94594595
#D17      0.09523810 0.90476190
#D18      0.02857143 0.97142857
#D19      0.20000000 0.80000000
#D20      0.08695652 0.91304348
#D21      0.11320755 0.88679245
#D22      0.04651163 0.95348837

# Some dyads (D04, D05, D14, D19, D21) did produce some expansions while others, very rarely.

PercentRolePerDyad_mention1<- prop.table(xtabs(~Dyad + Role, Description_Comparison_Mention1), 1)
#           Role
#Dyad        both   director    matcher
#D01 0.00000000 1.00000000 0.00000000
#D02 0.07692308 0.92307692 0.00000000
#D03 0.02222222 0.88888889 0.08888889
#D04 0.11538462 0.75000000 0.13461538
#D05 0.02000000 0.90000000 0.08000000
#D06 0.00000000 1.00000000 0.00000000
#D07 0.02702703 0.94594595 0.02702703
#D08 0.00000000 1.00000000 0.00000000
#D09 0.02702703 0.91891892 0.05405405
#D10 0.00000000 0.97500000 0.02500000
#D11 0.04878049 0.92682927 0.02439024
#D12 0.00000000 1.00000000 0.00000000
#D14 0.02040816 0.95918367 0.02040816
#D16 0.00000000 1.00000000 0.00000000
#D17 0.00000000 0.95238095 0.04761905
#D18 0.00000000 1.00000000 0.00000000
#D19 0.00000000 0.93333333 0.06666667
#D20 0.06521739 0.84782609 0.08695652
#D21 0.03773585 0.83018868 0.13207547
#D22 0.06976744 0.81395349 0.11627907

# Some dyads (D03, D04, D20, D21, D22) has the matcher produced some descriptions while for others, matcher made very few such contributions.

cor.test(x=PercentExpansionPerDyad_mention1[ ,1], y=PercentRolePerDyad_mention1[ ,2], method = 'spearman')
# S = 1972.5, p-value = 0.03096
#alternative hypothesis: true rho is not equal to 0
#sample estimates:
#  rho 
#-0.4830729 
# Rank correlation (across dyads) between % of expanded descriptions and % of descriptions produced by director alone
# The less matchers participate (and the more directors monopolize), the less expanded descriptions are offered


cor.test(x=PercentExpansionPerDyad_mention1[ ,1], y=PercentRolePerDyad_mention1[ ,3], method = 'spearman')
#data:  PercentExpansionPerDyad_mention1[, 1] and PercentRolePerDyad_mention1[, 3]
#S = 637.76, p-value = 0.01864
#alternative hypothesis: true rho is not equal to 0
#sample estimates:
#  rho 
#0.520481 
# Rank correlation (across dyads) between % of expanded descriptions and % of descriptions produced by matcher alone
# The more matchers participate, the more expanded descriptions are offered
## This supports the collaborative model of coordination, with expansions like installment or trial NP, or self repair, and matcher's involvement 
## supported that coordination is established jointly, with matchers displaying understanding (as opposed to just asserting it or leaving it presupposed).


#################################################################
## CONCEPTUAL PACT: 
## BEGIN : Which descriptions produced on Mention 1 #############
## will be reused and why #######################################
## taking into account same and simplified ######################
#################################################################


## let's do the same thing with Descriptions in mention 2

Description_Comparison_Mention2 <- ddply(DataType[DataType$Mention=="2", ], .(Dyad, PictureCode), function(x){
  for(i in 1:nrow(x)){
    if(i > 1){
      setcurrent <- str_split(x[i, c("Description")], "_", simplify=TRUE)
      for(j in (i-1):1){
        setprior <- str_split(x[j, c("Description")], "_", simplify=TRUE)
        #       compIdentity = setequal(setprior, setcurrent)
        compExtansion = setdiff(setprior, setcurrent)
        #        if(compIdentity=="TRUE"){
        #          x[i, c("ComparisonWithPrior")] <- "SameDescrip"
        #          priorSpeaker <- x[j, c("Speaker")]
        #          priorRole <- x[j, c("Role")]
        #          priorDescription <- x[j, c("Description")]
        #          x[i, c("PriorSpeaker")] <- priorSpeaker
        #          x[i, c("PriorDescription")] <- priorDescription
        #          x[i, c("PriorRole")] <- priorRole
        #          break
        #        }
        #        else{
        if(length(compExtansion)==0){
          x[i, c("ComparisonWithPrior")] <- "ExpandedDescrip"
          priorSpeaker <- x[j, c("Speaker")]
          priorRole <- x[j, c("Role")]
          priorDescription <- x[j, c("Description")]
          x[i, c("PriorSpeaker")] <- priorSpeaker
          x[i, c("PriorDescription")] <- priorDescription
          x[i, c("PriorRole")] <- priorRole
          break
        } 
        else{
          if(j==1){
            x[i, c("ComparisonWithPrior")] <- "NewDescrip"
            x[i, c("PriorSpeaker")] <- "NoPriorInstanceSpeaker"
            x[i, c("PriorDescription")] <- "NoPriorInstanceDescription"
            x[i, c("PriorRole")] <-"NoPriorInstanceRole"
          }
        }
      }
    }
    else{
      x[i, c("ComparisonWithPrior")] <- "NewDescrip"
      x[i, c("PriorSpeaker")] <- "NoPriorInstanceSpeaker"
      x[i, c("PriorDescription")] <- "NoPriorInstanceDescription"
      x[i, c("PriorRole")] <-"NoPriorInstanceRole"
    }
  }
  return(x)
})

Description_Comparison_Mention2$RevisedSpeaker <- ifelse((Description_Comparison_Mention2$Speaker=="A" & grepl("B", Description_Comparison_Mention2$PriorSpeaker)), "AB", 
                                                         ifelse((Description_Comparison_Mention2$Speaker=="B" & grepl("A", Description_Comparison_Mention2$PriorSpeaker)), "AB", Description_Comparison_Mention2$Speaker))

Description_Comparison_Mention2$RevisedRole <- ifelse(Description_Comparison_Mention2$ComparisonWithPrior=="NewDescrip", Description_Comparison_Mention2$Role, 
                                                      ifelse((Description_Comparison_Mention2$Role=="director" & grepl("h", Description_Comparison_Mention2$PriorRole)), "both", 
                                                             ifelse((Description_Comparison_Mention2$Role=="matcher" & grepl("o", Description_Comparison_Mention2$PriorRole)), "both", Description_Comparison_Mention2$Role)))


Description_Comparison <- rbind(Description_Comparison_Mention1, Description_Comparison_Mention2)


ReuseMention1Description <- ddply(Description_Comparison, .(Dyad, PictureCode), function(x){
  for(i in 1:nrow(x)){
    z <- x[x$Mention==2, ]
     if(x[i, c("Mention")]==1){
      setmention1 <- str_split(x[i, c("Description")], "_", simplify=TRUE)
      for(j in 1:nrow(z)){
        setmention2 <- str_split(z[j, c("Description")], "_", simplify=TRUE)
        compIdentity = setequal(setmention2, setmention1)
        compSimpli = setdiff(setmention2, setmention1)
        if(compIdentity=="TRUE"){
          x[i, c("reuseStatus")] <- "Reused"
          break
        }
        else {
          if(length(compSimpli)==0){
            x[i, c("reuseStatus")]  <- "Simplified"
            break
          }
          else{
            if(j==nrow(z)){
              x[i, c("reuseStatus")] <- "notReused"
            }
          }
        }
      }
    }
    else{
      x[i, c("reuseStatus")] <- "NotApplicable"
    }
  }
  return(x)
})

xtabs(~Mention + reuseStatus, ReuseMention1Description)
#reuseStatus
#Mention NotApplicable notReused Reused Simplified
#1             0       373    329        125
#2           744         0      0          0
#
# on Mention1, not reused: 45.2%

# Is it less likely for descriptions that have been used by both speakers to be reused?
xtabs(~ reuseStatus + RevisedRole, ReuseMention1Description[ReuseMention1Description$Mention==1, ])

#            RevisedRole
#reuseStatus  both director matcher
#notReused    25      330      19
#Reused        8      312      11
#Simplified    5      116       2

# Hard to tell because there are so few descriptions that were produced by both....If anything, it looks like there would be the opposite tendency.

# Does the number of times a description was used inversely predict the probability of being reused later?
xtabs(~ reuseStatus + TotalOccurrences, ReuseMention1Description[ReuseMention1Description$Mention==1, ])
#               TotalOccurrences
#reuseStatus    1   2   3   4
#notReused    349  24   0   1
#Reused       319  11   1   0
#Simplified   119   3   1   0

prop.table(xtabs(~ reuseStatus + TotalOccurrences, ReuseMention1Description[ReuseMention1Description$Mention==1, ]), 2)
#                     TotalOccurrences
#reuseStatus           1          2          3          4
#notReused       0.44345616 0.63157895 0.00000000 1.00000000
#Reused          0.40533672 0.28947368 0.50000000 0.00000000
#Simplified      0.15120712 0.07894737 0.50000000 0.00000000

# Ignoring the cases where the description was used 3 or 4 times (too few data points!), it looks like the rate of descriptions not being reused is greater when those
# descriptions had been used twice, as opposed to only once... :(

# Are reused descriptions shorter in words than the ones that will be simplified? What about the not reused ones?

ddply(ReuseMention1Description[ReuseMention1Description$Mention==1, ], .(reuseStatus), summarise, NbWords= mean(NbWordsInDescription))
#reuseStatus  NbWords
#1   notReused 3.034759 
#2      Reused 2.232628
#3  Simplified 4.178862
# Evidence supporting the notion that descriptions that will get simplified were longer than the other descriptions. Number of words in descriptions can be used to predict what 
# will happen to the description on mention 2... But I need more than 2 categories. I need 3.

# Is there a relationship between the number of words in a description and its reuse? I am wondering if some descriptions are reused because the 1-word label is
# straightforward (e.g., lollipop)...

xtabs(~reuseStatus + NbWordsInDescription, ReuseMention1Description[ReuseMention1Description$Mention==1, ])
#                NbWordsInDescription
# reuseStatus    1   2   3   4   5   6   7   8   9  10  11  13  14
#   notReused   62 142  64  37  28  15  12   4   5   2   1   1   1
#   Reused      41 202  68  12   7   0   1   0   0   0   0   0   0
#   Simplified   0  17  36  29  17  11   5   4   2   2   0   0   0

prop.table(xtabs(~reuseStatus + NbWordsInDescription, ReuseMention1Description[ReuseMention1Description$Mention==1, ]), 2)
#reuseStatus           1          2          3          4          5          6          7          8          9         10         11         13         14
#notReused    0.60194175 0.39335180 0.38095238 0.47435897 0.53846154 0.57692308 0.66666667 0.50000000 0.71428571 0.50000000 1.00000000 1.00000000 1.00000000
#Reused       0.39805825 0.55955679 0.40476190 0.15384615 0.13461538 0.00000000 0.05555556 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000
#Simplified   0.00000000 0.04709141 0.21428571 0.37179487 0.32692308 0.42307692 0.27777778 0.50000000 0.28571429 0.50000000 0.00000000 0.00000000 0.00000000

#It looks like the % of not being reused increases as the number of words increases, *except* for 1-word descriptions, which are particularly less
# likely to be reused than longer descriptions..... so it is rather the opposite of what I was thinking. 
# Are people more likely to reuse descriptions that they expanded than others?
xtabs(~reuseStatus + ComparisonWithPrior, ReuseMention1Description[ReuseMention1Description$Mention==1, ])
#            ComparisonWithPrior
#reuseStatus  ExpandedDescrip NewDescrip
#notReused               33        341
#Reused                   8        323
#Simplified              24         99
prop.table(xtabs(~reuseStatus + ComparisonWithPrior, ReuseMention1Description[ReuseMention1Description$Mention==1, ]), 2)
#             ComparisonWithPrior
#reuseStatus  ExpandedDescrip NewDescrip
#notReused          0.5076923  0.4469201
#Reused             0.1230769  0.4233290
#Simplified         0.3692308  0.1297510
#Looking at % of NotReused for expanded description (50.8%), it is not very different from % of NotReused for NewDescriptions (44.7%), so expanding a description
# does not really predict much reuse. Having been expanded does predict a higher rate of simplication later than new descriptions...

# Does the position of a description predict its probability of being reused on mention 2?
xtabs(~reuseStatus + TrialPosition, ReuseMention1Description[ReuseMention1Description$Mention==1, ])
#           TrialPosition
#reuseStatus  Last NotLast
#notReused    234     140
#Reused       302      27
#Simplified   100      25

prop.table(xtabs(~reuseStatus + TrialPosition, ReuseMention1Description[ReuseMention1Description$Mention==1, ]), 2)
#               TrialPosition
#reuseStatus       Last   NotLast
#notReused    0.3679245 0.7291667
#Reused       0.4748428 0.1406250
#Simplified   0.1572327 0.1302083
## YES, it looks like about 37% of description used last on Mention 1 were not reused, while that number is 73% for non-last ones!!

ReuseStatus_Mention1_PerDyad<-prop.table(xtabs(~ Dyad + reuseStatus, ReuseMention1Description[ReuseMention1Description$Mention==1, ]), 1)
#
#reuseStatus
#Dyad   notReused     Reused Simplified
#D01 0.19444444 0.75000000 0.05555556
#D02 0.46153846 0.38461538 0.15384615
#D03 0.40000000 0.44444444 0.15555556
#D04 0.57692308 0.30769231 0.11538462
#D05 0.56000000 0.22000000 0.22000000
#D06 0.53125000 0.37500000 0.09375000
#D07 0.32432432 0.54054054 0.13513514
#D08 0.29411765 0.58823529 0.11764706
#D09 0.43243243 0.37837838 0.18918919
#D10 0.62500000 0.35000000 0.02500000
#D11 0.56097561 0.39024390 0.04878049
#D12 0.28571429 0.54285714 0.17142857
#D14 0.63265306 0.16326531 0.20408163
#D16 0.24324324 0.48648649 0.27027027
#D17 0.42857143 0.26190476 0.30952381
#D18 0.37142857 0.48571429 0.14285714
#D19 0.57777778 0.40000000 0.02222222
#D20 0.54347826 0.39130435 0.06521739
#D21 0.39622642 0.24528302 0.35849057
#D22 0.39534884 0.55813953 0.04651163

# Can I correlate this rate with which dyads failed to reuse descriptions (i.e., NotReused) with the rate with which directors alone produced descriptions?

PropDescriptions_RevisedRolePerDyad_mention1 <- prop.table(xtabs(~ Dyad + RevisedRole, ReuseMention1Description[ReuseMention1Description$Mention==1, ]), 1)
#            RevisedRole
#Dyad        both   director    matcher
#D01 0.00000000 1.00000000 0.00000000
#D02 0.07692308 0.92307692 0.00000000
#D03 0.06666667 0.88888889 0.04444444
#D04 0.19230769 0.73076923 0.07692308
#D05 0.04000000 0.88000000 0.08000000
#D06 0.00000000 1.00000000 0.00000000
#D07 0.05405405 0.94594595 0.00000000
#D08 0.00000000 1.00000000 0.00000000
#D09 0.02702703 0.91891892 0.05405405
#D10 0.00000000 0.97500000 0.02500000
#D11 0.04878049 0.92682927 0.02439024
#D12 0.00000000 1.00000000 0.00000000
#D14 0.04081633 0.93877551 0.02040816
#D16 0.00000000 1.00000000 0.00000000
#D17 0.00000000 0.95238095 0.04761905
#D18 0.00000000 1.00000000 0.00000000
#D19 0.02222222 0.93333333 0.04444444
#D20 0.08695652 0.84782609 0.06521739
#D21 0.09433962 0.81132075 0.09433962
#D22 0.06976744 0.81395349 0.11627907

cor.test(x=ReuseStatus_Mention1_PerDyad[ ,1], y=PropDescriptions_RevisedRolePerDyad_mention1[ ,2], method = 'spearman', ties.method='average')

#	Spearman's rank correlation rho
#
#data:  ReuseStatus_Mention1_PerDyad[, 1] and PropDescriptions_RevisedRolePerDyad_mention1[, 2]
#S = 1908.7, p-value = 0.05521
#lternative hypothesis: true rho is not equal to 0
#sample estimates:
#  rho 
#-0.4350863 
## This is bizarrely negatively correlated: The more dyads produced descriptions by director alone, the more they reused descriptions...

cor.test(x=ReuseStatus_Mention1_PerDyad[ ,1], y=PropDescriptions_RevisedRolePerDyad_mention1[ ,3], method = 'spearman', ties.method='average')


ReuseMention1Description_Mention1_temp <- ReuseMention1Description[ReuseMention1Description$Mention==1, ]

######
## Regression analysis on probability for a description type produced on mention 1 to be reused ##
#####

table(ReuseMention1Description_Mention1_temp$reuseStatus, useNA="always")

ReuseMention1Description_Mention1_temp$reuseCode <- ifelse(ReuseMention1Description_Mention1_temp$reuseStatus=="notReused", 0, 1)
table(ReuseMention1Description_Mention1_temp$reuseCode, useNA="always")
## adding whether mention 1 trial ended correctly or not correctly.

ReuseMention1Description_Mention1<- merge(ReuseMention1Description_Mention1_temp, Trial_Data_noMiss[ , c("Dyad", "TrialNb", "ChoiceEval")], by.x=c("Dyad", "TrialNbDesign"), by.y=c("Dyad", "TrialNb"), 
       all.x=TRUE, all.y=FALSE)


## REGRESSION MODEL ###

## In the regression analysis, I want to predict reuseCode (1 vs. 0)
## as a function of ((Description-specific variables):
##  
##   (1) total number of occurrences on mention 1;
##   (2) whether the description type had been used by matcher (alone or with director) vs. only by director;
##   (3) Whether the description was used last or not last.


##Setting up EFFECTS (or SUM) coding 
## if we want "deviation from grand mean coding", then use 0.5 or -0.5
# see http://talklab.psy.gla.ac.uk/tvw/catpred/

ReuseMention1Description_Mention1$TotalOccurrences.c <- as.numeric(ReuseMention1Description_Mention1$TotalOccurrences) - mean(as.numeric(ReuseMention1Description_Mention1$TotalOccurrences))

ReuseMention1Description_Mention1$MatcherInvolv <- ifelse(ReuseMention1Description_Mention1$RevisedRole=="director", "no", "yes")

ReuseMention1Description_Mention1$MatcherInvolv.Sum <- as.factor(factor(ReuseMention1Description_Mention1$MatcherInvolv))
contrasts(ReuseMention1Description_Mention1$MatcherInvolv.Sum) <- cbind("yes"=c(-1,1))
# base group is negative, i.e,when the matcher was not involved

ReuseMention1Description_Mention1$TrialPosition.Sum <- as.factor(factor(ReuseMention1Description_Mention1$TrialPosition))
contrasts(ReuseMention1Description_Mention1$TrialPosition.Sum) <- cbind("last"=c(1,-1))
# base group is negative, i.e,when the description was not last

ReuseMention1Description_Mention1$ChoiceEval.Sum <- as.factor(factor(ReuseMention1Description_Mention1$ChoiceEval))
contrasts(ReuseMention1Description_Mention1$ChoiceEval.Sum) <- cbind("correct"=c(1,-1))


ReuseDescripType_Mention1.m <- glmer(reuseCode ~ TotalOccurrences.c + ChoiceEval.Sum + 
                            MatcherInvolv.Sum +
                            TrialPosition.Sum +
                            (1 |Dyad) + (1 | PictureCode),
                          data=ReuseMention1Description_Mention1, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))

#Fixed effects:
#                      Estimate Std. Error z value Pr(>|z|)    
#(Intercept)           -0.94331    0.30106  -3.133  0.00173 ** 
#TotalOccurrences.c    -0.29747    0.33256  -0.894  0.37106    
#ChoiceEval.Sumcorrect  0.69790    0.23948   2.914  0.00357 ** 
#MatcherInvolv.Sumyes  -0.12239    0.15753  -0.777  0.43719    
#TrialPosition.Sumlast  0.77694    0.09862   7.878 3.33e-15 ***

# The variables that sign predict which descriptions used on mention 1 will be reused on mention 2 are whether the trial ended correctly or not, and
# the position of that description, the last one or not the last one: Descriptions used on correct trials are more likely to be reused than those used on
# incorrect ones, and the last descriptions are more likely to be reused than non-last ones.

#################################################################
## CONCEPTUAL PACT: 
## END : Which descriptions produced on Mention 1 #############
## will be reused and why #######################################
## taking into account same and simplified ######################
#################################################################




#########################################
## CONCEPTUAL PACT/ PRECEDENTS ANALYSES ##
#########################################

## I want to take each description *token* produced on mention 2 for a given card by a member of a given dyad and compare that description with all of the description TYPES
## produced on mention 1 for the same card, by the same dyad. I can work on the tokens from mention 1 because the loop that compares descriptions on mention 2 and those on mention 1
## breaks as soon as it finds a match (or a simplification). 
# Select tokens from mention 1.

DataC_PicUncertainty<- merge(DataC, PicDescription_Uncertainty, by=c("PictureCode", "PictureName"), all.x=TRUE, all.y=TRUE)

OldNewMention2Description <- ddply(DataC_PicUncertainty, .(Dyad, PictureCode), function(x){
  for(i in 1:nrow(x)){
    z <- x[x$Mention==1, ]
    if(x[i, c("Mention")]==2){
      setmention2 <- str_split(x[i, c("Description")], "_", simplify=TRUE)
      for(j in 1:nrow(z)){
        setmention1 <- str_split(z[j, c("Description")], "_", simplify=TRUE)
        compIdentity = setequal(setmention1, setmention2)
        compSimpli = setdiff(setmention2, setmention1)
        if(compIdentity=="TRUE"){
          x[i, c("OldNewStatus")] <- "Old"
          break
        }
        else {
          if(length(compSimpli)==0){
            x[i, c("OldNewStatus")]  <- "OldSimple"
            break
          }
          else{
            if(j==nrow(z)){
              x[i, c("OldNewStatus")] <- "New"
            }
          }
        }
      }
    }
    else{
      x[i, c("OldNewStatus")] <- "NotApplicable"
    }
  }
  return(x)
})


nrow(OldNewMention2Description)
nrow(DataC_PicUncertainty)

nrow(DataC_PicUncertainty[DataC_PicUncertainty$Mention==2, ])
nrow(OldNewMention2Description[OldNewMention2Description$Mention==2, ])
table(OldNewMention2Description$OldNewStatus, useNA="always")

# Do people tend to reuse descriptions more when they are the directors again (same roles) than when the roles were exchanged?
xtabs(~ OldNewStatus + RoleMatch, OldNewMention2Description[OldNewMention2Description$Mention==2, ])
#             RoleMatch
#OldNewStatus different  na same
#New             174   0  150
#Old             165   0  183
#OldSimple        57   0   60
prop.table(xtabs(~ OldNewStatus + RoleMatch, OldNewMention2Description[OldNewMention2Description$Mention==2, ]), 2)
#           RoleMatch
#OldNewStatus different na      same
#New          0.4393939    0.3842239
#Old          0.4191919    0.4656489
#OldSimple    0.1414141    0.1526718

# There is a small difference (44 vs. 38%) between the % of descriptions in mention 2 that were new as a function of the roles having been maintained or switched. 

# Do people produce more olds descriptions first than they do if more descriptions follow?
xtabs(~ OldNewStatus + DescripRank, OldNewMention2Description[OldNewMention2Description$Mention==2, ])
#              DescripRank
#OldNewStatus   1   2   3   4   5   6
#New          242  58  19   5   0   0
#Old          307  30   8   1   1   1
#OldSimple     91  17   7   2   0   0
prop.table(xtabs(~ OldNewStatus + DescripRank, OldNewMention2Description[OldNewMention2Description$Mention==2, ]), 2)
#                     DescripRank
#OldNewStatus         1         2         3         4         5         6
#New          0.3765625 0.5619048 0.5882353 0.6250000 0.0000000 0.0000000
#Old          0.4890625 0.2666667 0.2058824 0.1250000 1.0000000 1.0000000
#OldSimple    0.1343750 0.1714286 0.2058824 0.2500000 0.0000000 0.0000000
# Indeed, it looks like first descriptions tend to be old more than subsequent descriptions. It may reflect the fact that old descriptions are more likely to "end" the trial, 
# rather than the fact that speakers will resort to a new description if the old one is ineffective.

# In order to disentangle the issue raised above, I compare the rate of new vs. old on DescripRank==1 as a function of how many descriptions total were produced:

xtabs(~ OldNewStatus + NbDescriptionsPerTrial, OldNewMention2Description[(OldNewMention2Description$Mention==2 & OldNewMention2Description$DescripRank==1), ])
prop.table(xtabs(~ OldNewStatus + NbDescriptionsPerTrial, OldNewMention2Description[(OldNewMention2Description$Mention==2 & OldNewMention2Description$DescripRank==1), ]), 2)
#                    NbDescriptionsPerTrial
#OldNewStatus         1         2         3         4         6
#New          0.3495327 0.5211268 0.4615385 0.5714286 1.0000000
#Old          0.5252336 0.2957746 0.3461538 0.2857143 0.0000000
#OldSimple    0.1252336 0.1830986 0.1923077 0.1428571 0.0000000

# Descriptions that people produce first are more likely to be old (52% + 12.5%) when that's the only one they produce. When more than one description is produced on that 
# trial, the rate of old descriptions is lower (29+18%, for example).

prop.table(xtabs(~ OldNewStatus + NbDescriptionsPerTrial, OldNewMention2Description[OldNewMention2Description$Mention==2, ]), 2)
#                      NbDescriptionsPerTrial
# OldNewStatus         1         2         3         4         6
#New           0.3495327 0.5563380 0.4743590 0.6428571 0.6666667
#Old           0.5252336 0.2887324 0.2820513 0.1785714 0.3333333
#OldSimple     0.1252336 0.1549296 0.2435897 0.1785714 0.0000000
## The mere effect of NbDescriptionsPerTrial can capture the effect, although it's not as precise as an explanation.

## Is there a relationship between rate of old descriptions and lag (nb of intervening trials) between mentions 1 and 2?

xtabs(~ OldNewStatus + Lag, OldNewMention2Description[OldNewMention2Description$Mention==2, ])
#Lag
#OldNewStatus  2  4  8 10 12 14 16 18 22 24 26
#New       21 14 14 19 45 43 21 45 49 23 31
#Old       26 33 23 23 40 47 17 77 29 18 18
#OldSimple  9  1 11  6 14 11 12 21 17  7  4
# It looks like lag account for some changes in the new description rate!! 

xtabs(~ OldNewStatus + Definiteness, OldNewMention2Description[OldNewMention2Description$Mention==2, ])
#                  Definiteness
#OldNewStatus   def indef  no uncodable
#New            146    48 131         0
#Old            148    46 155         2
#OldSimple       70    13  28         2

prop.table(xtabs(~ OldNewStatus + Definiteness, OldNewMention2Description[OldNewMention2Description$Mention==2, ]), 2)
#                           Definiteness
#OldNewStatus        def      indef         no  uncodable
#New          0.40109890 0.44859813 0.41719745 0.00000000
#Old          0.40659341 0.42990654 0.49363057 0.50000000
#OldSimple    0.19230769 0.12149533 0.08917197 0.50000000

## START: include speaker specific information in order to see if anything we know about each speaker can explain whether they produce old vs. new descriptions on mention 2.

# If, for some reason, participants' info is not included in the dataframe OldNewMention2Description, here is the code to add it.
OldNewMention2Description$DyadSpeakerID<- str_replace_all(OldNewMention2Description$DyadSpeaker, pattern = "-", "_")
Data_OldNew <-merge(OldNewMention2Description, ParticipantsDemographBooklet[, c("ID", "Age", "Gender", "Extraversion", "Agreeableness","Conscientiousness", "Neuroticism", "Openness", "OverallADHD", "Attention", "SESEduc")], 
                       by.x=c("DyadSpeakerID"), by.y=c("ID"), all.x=TRUE, all.y=FALSE)

Data_OldNew_Mention2TEMP <- Data_OldNew[Data_OldNew$Mention==2, ]

##Adding the number of descriptions from mention 1 for each trial
Data_OldNew_Mention2 <-merge(Data_OldNew_Mention2TEMP, Trial_Data_noMiss[Trial_Data_noMiss$Mention==1, c("Dyad", "PictureCode", "NbDescriptionsPerTrial")], 
                             by=c("Dyad", "PictureCode"), all.x=TRUE, all.y=FALSE)

Data_OldNew_Mention2 <- rename(Data_OldNew_Mention2, c("NbDescriptionsPerTrial.x"="NbDescriptionsPerTrial"))
Data_OldNew_Mention2 <- rename(Data_OldNew_Mention2, c("NbDescriptionsPerTrial.y"="PriorNbDescriptionsPerTrial"))

Data_OldNew_Mention2$DefiniteCode <- ifelse(Data_OldNew_Mention2$Definiteness=="def", 1, ifelse(Data_OldNew_Mention2$Definiteness=="indef" | Data_OldNew_Mention2$Definiteness=="no", 0, NA))

table(Data_OldNew_Mention2$PriorNbDescriptionsPerTrial, useNA="always")
#
#   1    2    3    4    5    6 <NA> 
#  586  113   50   18   16    2    4 
# The 4 NA correspond to those mention 2 trials with missing mention 1 trials, those trials that
# are missing because participants skipped a page.


## BEGIN: Do the rates of OldSimple and OldIdentical correlate differently with SESEduc and age?

Rates_OldSimpliNewPerSpeaker  <- prop.table(xtabs(~ DyadSpeakerID + OldNewStatus, Data_OldNew_Mention2), 1)

SESEducAgePerSpeaker <- ddply(Data_OldNew_Mention2, .(DyadSpeakerID), summarise, SESEduc=mean(as.numeric(SESEduc), na.rm=TRUE), Age=mean(as.numeric(Age), na.rm=TRUE))

## correlation between SES Educ and rate of OldSimple

cor.test(Rates_OldSimpliNewPerSpeaker[ , c("OldSimple")], SESEducAgePerSpeaker[ , c("SESEduc")], method = 'pearson')
#Pearson's product-moment correlation
#
#data:  Rates_OldSimpliNewPerSpeaker[, c("OldSimple")] and SESEducAgePerSpeaker[, c("SESEduc")]
#t = 2.9745, df = 36, p-value = 0.005212
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#0.1516623 0.6725749
#sample estimates:
#cor 
#0.44

## This means that more educated people produce (proportionally) more simplified descriptions than less ones.

cor.test(Rates_OldSimpliNewPerSpeaker[ , c("Old")], SESEducAgePerSpeaker[ , c("SESEduc")], method = 'pearson')

#Pearson's product-moment correlation
#
#data:  Rates_OldSimpliNewPerSpeaker[, c("Old")] and SESEducAgePerSpeaker[, c("SESEduc")]
#t = -2.8392, df = 36, p-value = 0.00739
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#  -0.6575084 -0.1251632
#sample estimates:
#  cor 
#-0.4277319 

cor.test(Rates_OldSimpliNewPerSpeaker[ , c("New")], SESEducAgePerSpeaker[ , c("SESEduc")], method = 'pearson')

#Pearson's product-moment correlation
#
#data:  Rates_OldSimpliNewPerSpeaker[, c("New")] and SESEducAgePerSpeaker[, c("SESEduc")]
#t = 1.0102, df = 36, p-value = 0.3191
#alternative hypothesis: true correlation is not equal to 0
#95 percent confidence interval:
#  -0.1622688  0.4612302
#sample estimates:
#  cor 
#0.1660271  

write.table(Data_OldNew_Mention2, "Data_OldNew_Ila_Booklet.txt")

Data_OldNew_Mention2$OldNewCode <- ifelse(grepl("Old", Data_OldNew_Mention2$OldNewStatus), 1, 0)

table(Data_OldNew_Mention2$OldNewStatus)
table(Data_OldNew_Mention2$OldNewCode)



OldRatePerDyadSpeaker <- ddply(Data_OldNew_Mention2, .(DyadSpeaker, SESEduc, Age), summarise, meanOld=mean(OldNewCode))

cor.test(OldRatePerDyadSpeaker$meanOld, OldRatePerDyadSpeaker$Age, method = 'spearman', ties.method='average')
#Spearman's rank correlation rho
#
#data:  OldRatePerDyadSpeaker$meanOld and OldRatePerDyadSpeaker$Age
#S = 13540, p-value = 0.09178
#alternative hypothesis: true rho is not equal to 0
#sample estimates:
#rho 
#-0.2701756
# There is a marginal correlation between age and rate of old descriptions: The older the speaker, the smaller the rate of production of old descriptions.

cor.test(OldRatePerDyadSpeaker$meanOld, as.numeric(OldRatePerDyadSpeaker$SESEduc), method = 'spearman', ties.method='average')
#Spearman's rank correlation rho
#
#data:  OldRatePerDyadSpeaker$meanOld and as.numeric(OldRatePerDyadSpeaker$SESEduc)
#S = 10381, p-value = 0.4158
#alternative hypothesis: true rho is not equal to 0
#sample estimates:
#rho 
#-0.1359194 
# The correlation between rate of old descriptions and SESEduc is NS.

## BEGIN: relationship between the production of an old vs. new description and the use of definite NP
## Thw relationship is likely to be complex because the cards themselves are in CG. Thus, the use of a def NP is
## felicitous even on first mention or when speakers choose to use a new description (thus, not marking the fact
## that the card was discussed before).
## Are definite NP more likely to be old descriptions than not-def NP are?

xtabs(~ OldNewStatus + Definiteness, Data_OldNew_Mention2)
#                   Definiteness
#OldNewStatus def indef  no uncodable
#   New       146    48 131         0
#   Old       148    46 155         2
#   OldSimple  70    13  28         2

xtabs(~ OldNewStatus + DefiniteCode, Data_OldNew_Mention2)
#             DefiniteCode
#OldNewStatus    0   1
#    New       179 146
#    Old       201 148
#    OldSimple  41  70

prop.table(xtabs(~ OldNewStatus + DefiniteCode, Data_OldNew_Mention2), 2)
#                     DefiniteCode
#OldNewStatus          0          1
#   New       0.42517815 0.40109890
#   Old       0.47743468 0.40659341
#   OldSimple 0.09738717 0.19230769

## There is no evidence that the rate of old description is greater in def NP than in non-def NP...

prop.table(xtabs(~ OldNewCode + DefiniteCode, Data_OldNew_Mention2), 2)
#                     DefiniteCode
#OldNewCode            0          1
#        0      0.4251781 0.3983516
#        1      0.5748219 0.6016484

prop.table(xtabs(~ OldNewStatus + Definiteness, Data_OldNew_Mention2), 2)
#                  Definiteness
#OldNewStatus       def     indef        no uncodable
#   New       0.3983516 0.4485981 0.4171975 0.0000000
#   Old       0.4093407 0.4112150 0.4872611 0.5000000
#   OldSimple 0.1923077 0.1401869 0.0955414 0.5000000

## END: relationship between the production of an old vs. new description and the use of definite NP

ddply(Data_OldNew_Mention2[Data_OldNew_Mention2$U<4.09, ], .(Mention), summarise, OldRate=mean(OldNewCode))
# Rate of old production on the half of the pictures with LOW U VALUES
#Mention   OldRate
#      2   0.6908602

ddply(Data_OldNew_Mention2[Data_OldNew_Mention2$U>4.09, ], .(Mention), summarise, OldRate=mean(OldNewCode))
# Rate of old production on the half of the pictures with HIGH U VALUES
#Mention  OldRate
#       2 0.498801 


## Plotting the relationship between the name Uncertainty of cards and the rate with which they are being referred to using an old description:
## the more dispersed the name, the less likely the use of an old description.


png(filename = "OldDescriptRateMention2_U_Rplot.png")
ggplot(ddply(Data_OldNew_Mention2, .(U), summarise, OldRate=mean(OldNewCode)), aes(U, OldRate)) +
   geom_point() + 
   stat_smooth(method="lm", color="black") +
  labs(x="U (name uncertainty / dispersion)", 
       y="Proportion of old descriptions per trial") +
  theme(text = element_text(size=20)) + 
  ggtitle("Proportion of old descriptions\nproduced on 2nd mention")
dev.off()


DefiniteCode.labels <- list('0'="not_definite", '1'="definite")
DefiniteCode_labeller <- function(variable,value){
  return(DefiniteCode.labels[value])
}

png(filename = "OldDescriptRateMention2_U_DefiniteCode_Rplot.png")
ggplot(ddply(Data_OldNew_Mention2[is.na(Data_OldNew_Mention2$DefiniteCode)==FALSE, ], .(U, DefiniteCode), summarise, OldRate=mean(OldNewCode)), aes(U, OldRate)) +
  geom_point() + 
  stat_smooth(method="lm", color="black") +
  facet_wrap(~as.factor(DefiniteCode), labeller=DefiniteCode_labeller) +
  labs(x="U (name uncertainty / dispersion)", 
       y="Proportion of old descriptions per trial") +
  theme(text = element_text(size=20)) + 
  ggtitle("Proportion of old descriptions\nnot definite or definite\nproduced on 2nd mention")
dev.off()


ggplot(ddply(Data_OldNew_Mention2[is.na(Data_OldNew_Mention2$DefiniteCode)==FALSE, ], .(U, Definiteness), summarise, OldRate=mean(OldNewCode)), aes(U, OldRate)) +
  geom_point() + 
  stat_smooth(method="lm", color="black") +
  facet_wrap(~Definiteness) +
  labs(x="U (name uncertainty / dispersion)", 
       y="Proportion of old descriptions per trial") +
  theme(text = element_text(size=20)) + 
  ggtitle("Proportion of old descriptions\nproduced on 2nd mention")


## ==> The Uncertainty of name of a given card predicts both the use of old (vs. new) descriptions on mention 2 AND the use of def NP on mention 2:
#  THE HIGHER THE NAME UNCERTAINTY, THE LESS FREQUENT THE USE OF AN OLD DESCRIPTION AND THE MORE FREQUENT THE USE OF DEFINITE NOUN PHRASE.

Cards_U_OldRate_defNPRate <- ddply(Data_OldNew_Mention2, .(U), summarise, OldRate=mean(OldNewCode), DefNPRate=mean(DefiniteCode, na.rm=TRUE))

ggplot(Cards_U_OldRate_defNPRate, aes(OldRate, DefNPRate)) + 
  geom_point()+
  stat_smooth()

## Bizarrely, I do not see the inverse correlation that I would have predicted, with high old rate being associated with low def NP rate....
## but this may not be so bizarre because the cards are no longer 'indexed' or 'defined' by their U value and that is what predicts the relationship
## with the use of Old descriptions and the use of Definiteness. If I lose that dimension, I no longer have the explanatory power of U... just an 
## non-organized list of pictures (that would vary randomly...)

## Evaluating the individual differences in the rate of old and def NP (within participants)

IndivDiff_OldRate_defNPRate<- ddply(Data_OldNew_Mention2, .(DyadSpeakerID), summarise, OldRate=mean(OldNewCode), DefNPRate=mean(DefiniteCode, na.rm=TRUE))

ggplot(IndivDiff_OldRate_defNPRate, aes(OldRate, DefNPRate)) + 
   geom_point()+
   stat_smooth(method="lm")

 # ==> The relationship, at the individual level, is quite modest...


## Plotting the relationship between SES Educ and Old description rate

OldDescripRate_directorsOnly_SESEduc <- ddply(na.omit(Data_OldNew_Mention2[Data_OldNew_Mention2$Role=="director", ]), .(DyadSpeakerID, Age, SESEduc, Mention), summarise, OldRate=mean(OldNewCode))

png(filename = "OldDescriptRateMention2_SESEduc_Rplot.png")
ggplot(OldDescripRate_directorsOnly_SESEduc, aes(as.numeric(SESEduc), OldRate)) +
  geom_point() + 
  stat_smooth(method = 'lm', color="black") +
  labs(x="Participant's level of education", 
       y="Proportion of old descriptions\nproduced by the director") +
  theme(text = element_text(size=20)) + 
  ggtitle("Proportion of old (vs. new)\ndescriptions on 2nd mention")
dev.off()

# ==> There is no relationship between SES educ and rate of old descriptions, contrary to what was observed on rate of Def NPs. 
# ==> Age, on the other hand, is negatively related to rate of old descriptions.
  
ggplot(OldDescripRate_directorsOnly_SESEduc, aes(Age, OldRate)) +
  geom_point() + 
  stat_smooth(method = 'lm', color="black")



## If I were to put my finger on the dimension(s) that connects individual differences and rate of old description use and defNP use, 
## I would gain explanatory power.



## REGRESSION MODEL ###

## In the regression analysis, I want to predict OldNewCode (1 vs. 0)
## as a function of
## I) Description-production specific variables
##   (1) lag (the number of intervening trials between mentions)
##   (2) role match (SameRoles vs. DiffRoles) between the two mentions
##   (3) LastNotLast (which will say something about how effective old descriptions are at reaching meaning coordination)
##   (4) The number of descriptions within the trial (speaks to the same point as LastNotLast and should show that shorter trials tends to have old descriptions more than longer trials)
##   (5) The total number of descriptions produced on mention 1 for that card, to see if the more descriptions earlier, the more likely people are to believe the description is in CG
## II) Speaker-specific variables
##   (4) gender of current speaker
##   (5) age of current speaker
##   (6) SES Education of current speaker
##   (7) Extraversion of current speaker
##   (8) Agreeablenes
##   (9) Conscientiousness
##   (10) Neuroticism
##   (11) Openness
##   (12) ADHD score
##   (13) Attention symptom specifically


## Setting up EFFECTS (or SUM) coding
## if we want "deviation from grand mean coding", then use 0.5 or -0.5
# see http://talklab.psy.gla.ac.uk/tvw/catpred/


Data_OldNew_Mention2$Lag.c <- as.numeric(Data_OldNew_Mention2$Lag) -mean(as.numeric(Data_OldNew_Mention2$Lag))

Data_OldNew_Mention2$RoleMatch.Dev <- as.factor(factor(Data_OldNew_Mention2$RoleMatch))
contrasts(Data_OldNew_Mention2$RoleMatch.Dev) <- cbind("Diff"=c(1/2,-1/2))
# base group is negative, i.e,when the roles were maintained

Data_OldNew_Mention2$NbDescriptionsPerTrial.c <- as.numeric(Data_OldNew_Mention2$NbDescriptionsPerTrial) -mean(as.numeric(Data_OldNew_Mention2$NbDescriptionsPerTrial))

Data_OldNew_Mention2$PriorNbDescriptionsPerTrial.c <- as.numeric(Data_OldNew_Mention2$PriorNbDescriptionsPerTrial) -mean(as.numeric(Data_OldNew_Mention2$PriorNbDescriptionsPerTrial), na.rm=TRUE)

Data_OldNew_Mention2$LastNotLast.Dev <- as.factor(factor(Data_OldNew_Mention2$LastNotLast))
contrasts(Data_OldNew_Mention2$LastNotLast.Dev) <- cbind("last"=c(1/2,-1/2))
# base group is negative, i.e,when the descrption is not last


Data_OldNew_Mention2$Gender.Dev <- as.factor(Data_OldNew_Mention2$Gender)
contrasts(Data_OldNew_Mention2$Gender.Dev) <- cbind("female"=c(1, -1))
## base group is negative, "male", so the contrast says sth about prob of reuse when speaker is female

Data_OldNew_Mention2$Age.c <- Data_OldNew_Mention2$Age - mean(Data_OldNew_Mention2$Age)

Data_OldNew_Mention2$Extraversion <- as.numeric(Data_OldNew_Mention2$Extraversion)
Data_OldNew_Mention2$Extraversion.c <- Data_OldNew_Mention2$Extraversion - mean(Data_OldNew_Mention2$Extraversion)

Data_OldNew_Mention2$Agreeableness <- as.numeric(Data_OldNew_Mention2$Agreeableness)
Data_OldNew_Mention2$Agreeableness.c <- Data_OldNew_Mention2$Agreeableness- mean(Data_OldNew_Mention2$Agreeableness)

Data_OldNew_Mention2$Conscientiousness <- as.numeric(Data_OldNew_Mention2$Conscientiousness)
Data_OldNew_Mention2$Conscientiousness.c <- Data_OldNew_Mention2$Conscientiousness- mean(Data_OldNew_Mention2$Conscientiousness)

Data_OldNew_Mention2$Neuroticism <- as.numeric(Data_OldNew_Mention2$Neuroticism)
Data_OldNew_Mention2$Neuroticism.c <- Data_OldNew_Mention2$Neuroticism- mean(Data_OldNew_Mention2$Neuroticism)

Data_OldNew_Mention2$Openness <- as.numeric(Data_OldNew_Mention2$Openness)
Data_OldNew_Mention2$Openness.c <- Data_OldNew_Mention2$Openness- mean(Data_OldNew_Mention2$Openness)

Data_OldNew_Mention2$OverallADHD <- as.numeric(Data_OldNew_Mention2$OverallADHD)
Data_OldNew_Mention2$OverallADHD.c <- Data_OldNew_Mention2$OverallADHD- mean(Data_OldNew_Mention2$OverallADHD)

Data_OldNew_Mention2$Attention <- as.numeric(Data_OldNew_Mention2$Attention)
Data_OldNew_Mention2$Attention.c <- Data_OldNew_Mention2$Attention- mean(Data_OldNew_Mention2$Attention)

Data_OldNew_Mention2$SESEduc.c <- as.numeric(Data_OldNew_Mention2$SESEduc) - mean(na.omit(as.numeric(Data_OldNew_Mention2$SESEduc)))

#Data_OldNew_Mention2$ChoiceEval.Dev <- as.factor(Data_OldNew_Mention2$ChoiceEval)
#contrasts(Data_OldNew_Mention2$ChoiceEval.Dev) <- cbind("correct"= c(1, -1))

Data_OldNew_Mention2$DefiniteCodeFactor.Dev <- as.factor(Data_OldNew_Mention2$DefiniteCode)
contrasts(Data_OldNew_Mention2$DefiniteCodeFactor.Dev) <- cbind("def"=c(-1/2, 1/2))
# I am using 0 as baseline, with 0 being either indef or no article.

Data_OldNew_Mention2$U.c <- Data_OldNew_Mention2$U-mean(Data_OldNew_Mention2$U)


# Not best model (lastNotlast is not the best way to capture efficiency of old descriptions)
OldNewDescript.m <- glmer(OldNewCode ~ RoleMatch.Sum +
                           Lag.c +
                           LastNotLast.Sum +
                           Gender.Sum +
                           Age.c +
                           SESEduc.c +
                           Extraversion.c +
                           Agreeableness.c +
                           Conscientiousness.c +
                           Neuroticism.c +
                           Openness.c +
                           OverallADHD.c +
                           Attention.c +
                           (1 + RoleMatch.Sum + Lag.c |DyadSpeaker) + (1 | PictureCode),
                         data=Data_OldNew_Mention2, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa"))
#
#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)   
#(Intercept)          0.460793   0.176292   2.614  0.00895 **
#RoleMatch.SumDiff   -0.127682   0.141702  -0.901  0.36756   
#Lag.c               -0.037694   0.021919  -1.720  0.08549 . 
#LastNotLast.Sumlast  0.127288   0.108914   1.169  0.24253   
#Gender.Sumfemale     0.138534   0.125648   1.103  0.27022   
#Age.c               -0.040610   0.012667  -3.206  0.00135 **
#SESEduc.c           -0.021928   0.084510  -0.259  0.79527   
#Extraversion.c       0.003213   0.027206   0.118  0.90598   
#Agreeableness.c     -0.035876   0.032276  -1.112  0.26633   
#Conscientiousness.c  0.080834   0.043726   1.849  0.06451 . 
#Neuroticism.c        0.018964   0.020135   0.942  0.34628   
#Openness.c          -0.045103   0.037323  -1.208  0.22686   
#OverallADHD.c       -0.003731   0.012217  -0.305  0.76007   
#Attention.c          0.004365   0.041952   0.104  0.91714  

# The effect of LastNotLast is NS. Perhaps this is because the last one is not only old because a trial must end but it will even if a new description
# is offered. I should change it into NbDescriptPerTrial.
# The logic is having NbDescriptPerTrial is to argue that the use of old descriptions is associated with shorter trials. Thus, short trials are more likely to use old descriptions
# than longer trials.
# 
# Also, the effect of Lag may be shown stronger if I add an interaction between Age and Lag, predicting that the effect of Lag will be strong for old speakers than for young ones.

#Not complete model because it doesn't incorporate Definiteness yet.
OldNewDescript.m1 <- glmer(OldNewCode ~ RoleMatch.Sum +
                            Lag.c +
                            NbDescriptionsPerTrial.c + PriorNbDescriptionsPerTrial.c +
                            Gender.Sum +
                            Age.c +
                            Lag.c:Age.c +
                            SESEduc.c +
                            Extraversion.c +
                            Agreeableness.c +
                            Conscientiousness.c +
                            Neuroticism.c +
                            Openness.c +
                            OverallADHD.c +
                            Attention.c +
                            (1 + Lag.c |DyadSpeaker) + (1 | PictureCode),
                          data=Data_OldNew_Mention2, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)), na.action=na.exclude)
#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)    
#(Intercept)                    5.420e-01  1.553e-01   3.490 0.000483 ***
#RoleMatch.SumDiff             -1.331e-01  1.371e-01  -0.971 0.331609    
#Lag.c                         -3.856e-02  2.111e-02  -1.826 0.067776 .  
#NbDescriptionsPerTrial.c      -2.288e-01  1.049e-01  -2.181 0.029151 *  
#PriorNbDescriptionsPerTrial.c -7.012e-02  9.933e-02  -0.706 0.480258    
#Gender.Sumfemale               1.488e-01  1.245e-01   1.195 0.232166    
#Age.c                         -3.685e-02  1.275e-02  -2.890 0.003857 ** 
#SESEduc.c                     -4.717e-03  8.385e-02  -0.056 0.955136    
#Extraversion.c                -1.606e-03  2.663e-02  -0.060 0.951919    
#Agreeableness.c               -3.309e-02  3.216e-02  -1.029 0.303610    
#Conscientiousness.c            8.335e-02  4.334e-02   1.923 0.054457 .  
#Neuroticism.c                  1.378e-02  2.005e-02   0.687 0.491799    
#Openness.c                    -3.811e-02  3.709e-02  -1.028 0.304156    
#OverallADHD.c                 -2.020e-03  1.225e-02  -0.165 0.868965    
#Attention.c                    9.636e-03  4.186e-02   0.230 0.817948    
#Lag.c:Age.c                    2.693e-05  1.115e-03   0.024 0.980721    

## Important: This model was generated along with a warning message: 
#convergence code: 1
#Model failed to converge with max|grad| = 0.00173467 (tol = 0.001, component 1)
#Model is nearly unidentifiable: very large eigenvalue
#- Rescale variables?

## This is a nice model:  
#  1) nb of descriptions in the trial, with the longer the trial, the less likely the speakers are to be using old descriptions, which suggests that the use of old descriptions
##    is more effective at ending the trial than new descriptions are.
#  2) (marginal) effect of lag, with the longer the lag between mentions 1 and 2, the less likely the description is to be old.
#  3) no effect of role match (who would have thought!)
#  4) effect of age (of the speaker producing the description in question): the old the speaker, the less likely the description is to be old
#  5) (marginal) effect of conscientiousness, the more conscientious the speaker, the more likely they are to produce old descriptions.


## Slightly expanded model: Whether the trial (mention 2) ends correctly or incorrectly (but very few errors) and whether the description being
## considering is definite or not expression, and an interaction between SESEduc and DefiniteCode because the analyses on definiteness shows a 
# significant interaction between these two factors.

OldNewDescript.m2 <- glmer(OldNewCode ~ RoleMatch.Dev +
                             Lag.c + RoleMatch.Dev:Lag.c + 
                             NbDescriptionsPerTrial.c + PriorNbDescriptionsPerTrial.c + DefiniteCodeFactor.Dev + U.c + U.c:DefiniteCodeFactor.Dev + 
                             Gender.Dev +
                             Age.c +
                             SESEduc.c +
                             Extraversion.c +
                             Agreeableness.c +
                             Conscientiousness.c +
                             Neuroticism.c +
                             Openness.c +
                             OverallADHD.c +
                             Attention.c +
                             (1 + RoleMatch.Dev + Lag.c |DyadSpeakerID) + (1 | PictureCode),
                           data=Data_OldNew_Mention2, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)), na.action = na.exclude)

#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)   
#(Intercept)                    0.510663   0.115326   4.428 9.51e-06 ***
#RoleMatch.DevDiff             -0.574278   0.184058  -3.120  0.00181 ** 
#Lag.c                          0.004695   0.023408   0.201  0.84102    
#NbDescriptionsPerTrial.c      -0.208778   0.107700  -1.939  0.05256 .  
#PriorNbDescriptionsPerTrial.c -0.066625   0.100249  -0.665  0.50631    
#DefiniteCodeFactor.Devdef      0.167608   0.216669   0.774  0.43919    
#U.c                           -0.633811   0.113096  -5.604 2.09e-08 ***
#Gender.Devfemale               0.130771   0.132662   0.986  0.32426    
#Age.c                         -0.035702   0.013563  -2.632  0.00848 ** 
#SESEduc.c                     -0.025192   0.091672  -0.275  0.78346    
#Extraversion.c                 0.017631   0.028526   0.618  0.53653    
#Agreeableness.c               -0.028869   0.036019  -0.801  0.42285    
#Conscientiousness.c            0.077292   0.046692   1.655  0.09785 .  
#Neuroticism.c                  0.014094   0.021404   0.658  0.51025    
#Openness.c                    -0.030760   0.039217  -0.784  0.43283    
#OverallADHD.c                 -0.001452   0.013353  -0.109  0.91342    
#Attention.c                    0.010215   0.045221   0.226  0.82129    
#RoleMatch.DevDiff:Lag.c       -0.087082   0.044851  -1.942  0.05219 .  
#DefiniteCodeFactor.Devdef:U.c  0.078017   0.205051   0.380  0.70359 


## IMPORTANT: The problem of sampling may still be present here: The more descriptions people produce, the more likely they are
## to produce descriptions of variety of kinds (New, Old, OldSimple), and the model doesn't take this into account. 

## MODEL THAT LOOKS AT THE FIRST DESCRIPTIONS ONLY, as a way to go arount the sample problem.
OldNewDescript.m3 <- glmer(OldNewCode ~  RoleMatch.Dev +
                                         Lag.c + RoleMatch.Dev:Lag.c + 
                                         NbDescriptionsPerTrial.c + PriorNbDescriptionsPerTrial.c + DefiniteCodeFactor.Dev + U.c + U.c:DefiniteCodeFactor.Dev + 
                                         Gender.Dev +
                                         Age.c +
                                         SESEduc.c +
                                         Extraversion.c +
                                         Agreeableness.c +
                                         Conscientiousness.c +
                                         Neuroticism.c +
                                         Openness.c +
                                         OverallADHD.c +
                                         Attention.c +
                                         (1 + RoleMatch.Dev + Lag.c |DyadSpeaker) + (1 | PictureCode),
                           data=Data_OldNew_Mention2[Data_OldNew_Mention2$DescripRank==1, ], family="binomial"(link="logit"), 
                           control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)), na.action = na.exclude)
#Fixed effects:
#  Estimate Std. Error z value Pr(>|z|)   
#(Intercept)                    0.5389127  0.1313270   4.104 4.07e-05 ***
#RoleMatch.DevDiff             -0.5786563  0.2049593  -2.823  0.00475 ** 
#Lag.c                         -0.0082423  0.0253109  -0.326  0.74470    
#NbDescriptionsPerTrial.c      -0.2312423  0.1630710  -1.418  0.15618    
#PriorNbDescriptionsPerTrial.c -0.1018970  0.1186351  -0.859  0.39039    
#DefiniteCodeFactor.Devdef      0.2142442  0.2410356   0.889  0.37408    
#U.c                           -0.7088518  0.1246118  -5.688 1.28e-08 ***
#Gender.Devfemale               0.1596711  0.1399332   1.141  0.25385    
#Age.c                         -0.0420855  0.0151590  -2.776  0.00550 ** 
#SESEduc.c                     -0.0587934  0.0997092  -0.590  0.55543    
#Extraversion.c                 0.0248079  0.0306121   0.810  0.41771    
#Agreeableness.c               -0.0056551  0.0387371  -0.146  0.88393    
#Conscientiousness.c            0.0492811  0.0510148   0.966  0.33404    
#Neuroticism.c                  0.0235690  0.0239419   0.984  0.32491    
#Openness.c                    -0.0295159  0.0426891  -0.691  0.48930    
#OverallADHD.c                 -0.0005632  0.0140406  -0.040  0.96800    
#Attention.c                   -0.0006239  0.0484057  -0.013  0.98972    
#RoleMatch.DevDiff:Lag.c       -0.0533428  0.0488090  -1.093  0.27444    
#DefiniteCodeFactor.Devdef:U.c  0.1381181  0.2305340   0.599  0.54909    

## The (lag and) age effect(s) remain the same when focusing on the first descriptions given, an analysis that doesn't have the bias of rate of production
## that the full analysis had. Surprisingly, the effect of NbdescriptionsPerTrial is no longer significant but it is in the right direction so perhaps
## I don't need to worry about it.


#############################################################################
## Comparing rate of old (vs. new) descriptions on first description       ##
## on mention 2 when that description is also definite, vs. indefinite     ##
## for a subset of the cases, i.e., those where the last description       ## 
## on mention 1 was indefinite or bare                                     ##
#############################################################################

## Consider Clark's claim that for why people use indefinite NP on mention 1 and why
## they use definite NP on mention 2. Theoretically, because the sets of cards are known to be shared between participants, people could use definite NP from 
## the start, as they (allegedly) did with simple objects. The claim is that the preference for an indefinite NP on first mention reflects the need to agree on a 
## conceptualization conveyed by the noun phrase, i.e., an attribute use of an indefinite NP. Once this conceptualization has been accepted and proved 
## successful in coordinating meaning and understanding, participants can use the (referential) definite NP. This implies that that definite NP 
## should make use of the NP accepted on mention 1. 
## In order to test that particular claim, I want to compare the rate of old descriptions on mention 2 (the only description or the first one) that were definite
## AND the only or last description of mention 1, indefinite or bare, to the rate of old descriptions on mention 2 (only or first) that were not definite AND
## the only or last description of mention 1, indefinite or bare. The reasoning for this comparison is as follows: If, as claimed by Clark, people go from indefinite
## or bare) to definite NP because the contents of the NP has been established as CG for this referent, people should be more likely to use an old description in
## the def NP than they are when they continue to use a non-definite NP.


nrow(subset(DataC_PicUncertainty, Mention==1 & LastNotLast=="last"))
nrow(subset(DataC_PicUncertainty, Mention==2 & DescripRank==1))

DataC_PicUncertainty_LastFirst<- rbind(subset(DataC_PicUncertainty, Mention==1 & LastNotLast=="last"), subset(DataC_PicUncertainty, Mention==2 & DescripRank==1))


OldNewDescription_LastFirst <- ddply(DataC_PicUncertainty_LastFirst, .(Dyad, PictureCode), function(x){
  for(i in 1:nrow(x)){
    z <- x[x$Mention==1 & x$LastNotLast=="last", ]
    if(nrow(z)>0){
     if((x[i, c("Mention")]==2) & (x[i, c("DescripRank")]==1)){
      setmention2 <- str_split(x[i, c("Description")], "_", simplify=TRUE)
      for(j in 1:nrow(z)){
        setmention1 <- str_split(z[j, c("Description")], "_", simplify=TRUE)
        Definiteness1 <- z[j, c("Definiteness")]
        compIdentity = setequal(setmention1, setmention2)
        compSimpli = setdiff(setmention2, setmention1)
        if(compIdentity=="TRUE"){
          x[i, c("OldNewStatus")] <- "Old"
          x[i, c("Definiteness1")] <- Definiteness1
          break
        }
        else {
          if(length(compSimpli)==0){
            x[i, c("OldNewStatus")]  <- "OldSimple"
            x[i, c("Definiteness1")] <- Definiteness1
            break
          }
          else{
            if(j==nrow(z)){
              x[i, c("OldNewStatus")] <- "New"
              x[i, c("Definiteness1")] <- Definiteness1
            }
          }
        }
      }
    }
#    else{
#      x[i, c("OldNewStatus")] <- "NotApplicable"
#      x[i, c("Definiteness1")] <- "NotApplicable"
#     }
    }
  }
  return(x)
})

nrow(OldNewDescription_LastFirst)

## getting rid of the lines for Mention 1
OldNewDescription_LastFirstMention2_TEMP <- OldNewDescription_LastFirst[OldNewDescription_LastFirst$Mention==2, ]

#Assessing how many trials to remove because 1) the mention 1 of those trials were missing (4) or 2) the definiteness of description on mention 1 was uncodable (6),
# or 3) the definiteness of description on mention 2 was uncodable (3).

table(OldNewDescription_LastFirstMention2_TEMP$OldNewStatus, useNA="always")
xtabs(~Definiteness1, OldNewDescription_LastFirstMention2_TEMP)
xtabs(~Definiteness, OldNewDescription_LastFirstMention2_TEMP)

# deleting those (4+6+3) 13 trials
OldNewDescription_LastFirstMention2 <-subset(OldNewDescription_LastFirstMention2_TEMP, is.na(OldNewStatus)==FALSE & Definiteness!="uncodable" & Definiteness1!="uncodable")

OldNewDescription_LastFirstMention2$OldNewCode <- ifelse(grepl("Old", OldNewDescription_LastFirstMention2$OldNewStatus), 1, 0)
OldNewDescription_LastFirstMention2$DefiniteCode <- ifelse(OldNewDescription_LastFirstMention2$Definiteness=="def", 1, 0)


nrow(OldNewDescription_LastFirstMention2)
# 627, which is (20*32) 640 - 13

# Rate of old descriptions (with this slightly more restrictive definition of comparison the first description of mention 2 solely to the last one on mention 1)

xtabs(~OldNewStatus, OldNewDescription_LastFirstMention2)
#=> 'old' rate is 60%, 'new' is 40%

# assessing whether the rate of old varies across the type of NP used on Mention 2
xtabs(~OldNewStatus + Definiteness, OldNewDescription_LastFirstMention2)
prop.table(xtabs(~OldNewStatus + Definiteness, OldNewDescription_LastFirstMention2), 2)
#=> rate of new (easier to see) is only very slightly lower when the NP is def (39%) than when it is indef (43%) or bare (39%)

# Given that I want to restrict the analysis to mention 2 trials for which mention 1 was an indefinite or bare NP, I need to know how many
# trials are going to be included:
xtabs(~Definiteness1, OldNewDescription_LastFirstMention2)
# out of 627, I am keeping (157+251) 408 trials, which is 65%

## Effect of Definiteness status (definite or not definite) of first description of mention 2 on prob of being old (given that the last description of mention 1 
## was not already definite

xtabs(~OldNewStatus + Definiteness, OldNewDescription_LastFirstMention2[OldNewDescription_LastFirstMention2$Definiteness1=="indef" | OldNewDescription_LastFirstMention2$Definiteness1=="no", ])

xtabs(~OldNewCode + DefiniteCode, OldNewDescription_LastFirstMention2[OldNewDescription_LastFirstMention2$Definiteness1=="indef" | OldNewDescription_LastFirstMention2$Definiteness1=="no", ])
prop.table(xtabs(~OldNewCode + DefiniteCode, OldNewDescription_LastFirstMention2[OldNewDescription_LastFirstMention2$Definiteness1=="indef" | 
                                                                                   OldNewDescription_LastFirstMention2$Definiteness1=="no", ]), 2)

#                  DefiniteCode
#OldNewCode         0         1
#         0 0.4119850 0.4255319
#         1 0.5880150 0.5744681

## => Amazingly enough, there is no evidence that when people move from a non-definite expression on mention 1 to a definite expression on mention 2, it is
## because they feel they can use the def NP now that the NP has been grounded BECAUSE the rate of old NP/ description is NOT greater then than
## it is when people continued to use a non-def NP on mention 2 (57% of definite NP on mention 2 were old, and 59% of non-definite NP on mention 2 were old). 


## IMPORTANT: The status of bare NP is ambiguous. Perhaps the analysis is not as compelling as it should be because I included the bare NP with
## the indefinite NP in definiteness 1 in the analysis. Let's look at the % of oldnew when I limit the analysis to indef NP in mention 1 (only 157 cases, though):

xtabs(~OldNewCode + Definiteness, OldNewDescription_LastFirstMention2[OldNewDescription_LastFirstMention2$Definiteness1=="indef", ])
prop.table(xtabs(~OldNewCode + Definiteness, OldNewDescription_LastFirstMention2[OldNewDescription_LastFirstMention2$Definiteness1=="indef", ]), 2)

#                 Definiteness
# OldNewCode       def     indef        no uncodable
#          0 0.4166667 0.3400000 0.4857143          
#          1 0.5833333 0.6600000 0.5142857

## => On specifically those trials where participants used an indefinite NP at the end of mention 1, the probability of producing 
## an old NP first on mention 2 was not different whether that NP was definite (58%) or indefinite (66%) (in fact, numerically, 66 > 58, which goes
## against Clark's claim).

## In order to run the proper regression, I should merge the participant demographics data to the data file here.

OldNewDescription_LastFirstMention2_C <- merge(OldNewDescription_LastFirstMention2, ParticipantsDemograph[, c("ID", "Age", "Gender", "Extraversion", "Agreeableness","Conscientiousness", "Neuroticism", "Openness", "OverallADHD", "Attention", "SESEduc")], 
      by.x=c("DyadSpeakerID"), by.y=c("ID"), all.x=TRUE, all.y=FALSE)


## PREPARING FOR REGRESSION ANALYSIS

OldNewDescription_LastFirstMention2_C$Lag.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Lag) -mean(as.numeric(OldNewDescription_LastFirstMention2_C$Lag))

OldNewDescription_LastFirstMention2_C$RoleMatch.Sum <- as.factor(factor(OldNewDescription_LastFirstMention2_C$RoleMatch))
contrasts(OldNewDescription_LastFirstMention2_C$RoleMatch.Sum) <- cbind("Diff"=c(1,-1))
# base group is negative, i.e,when the roles were maintained

OldNewDescription_LastFirstMention2_C$NbDescriptionsPerTrial.c <- as.numeric(OldNewDescription_LastFirstMention2_C$NbDescriptionsPerTrial) - 
                    mean(as.numeric(OldNewDescription_LastFirstMention2_C$NbDescriptionsPerTrial))

OldNewDescription_LastFirstMention2_C$Gender.Sum <- as.factor(OldNewDescription_LastFirstMention2_C$Gender)
contrasts(OldNewDescription_LastFirstMention2_C$Gender.Sum) <- cbind("female"=c(1, -1))
## base group is negative, "male", so the contrast says sth about prob of reuse when speaker is female

OldNewDescription_LastFirstMention2_C$Age.c <- OldNewDescription_LastFirstMention2_C$Age - mean(OldNewDescription_LastFirstMention2_C$Age)

OldNewDescription_LastFirstMention2_C$Extraversion.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Extraversion) - mean(as.numeric(OldNewDescription_LastFirstMention2_C$Extraversion))

OldNewDescription_LastFirstMention2_C$Agreeableness.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Agreeableness)- mean(as.numeric(OldNewDescription_LastFirstMention2_C$Agreeableness))

OldNewDescription_LastFirstMention2_C$Conscientiousness.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Conscientiousness)- mean(as.numeric(OldNewDescription_LastFirstMention2_C$Conscientiousness))

OldNewDescription_LastFirstMention2_C$Neuroticism.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Neuroticism) - mean(as.numeric(OldNewDescription_LastFirstMention2_C$Neuroticism))

OldNewDescription_LastFirstMention2_C$Openness.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Openness) - mean(as.numeric(OldNewDescription_LastFirstMention2_C$Openness))

OldNewDescription_LastFirstMention2_C$OverallADHD.c <- as.numeric(OldNewDescription_LastFirstMention2_C$OverallADHD) - mean(as.numeric(OldNewDescription_LastFirstMention2_C$OverallADHD))

OldNewDescription_LastFirstMention2_C$Attention.c <- as.numeric(OldNewDescription_LastFirstMention2_C$Attention) - mean(as.numeric(OldNewDescription_LastFirstMention2_C$Attention))

OldNewDescription_LastFirstMention2_C$SESEduc.c <- as.numeric(OldNewDescription_LastFirstMention2_C$SESEduc) - mean(na.omit(as.numeric(OldNewDescription_LastFirstMention2_C$SESEduc)))

OldNewDescription_LastFirstMention2_C$DefiniteCode.Sum <- as.factor(OldNewDescription_LastFirstMention2_C$DefiniteCode)
contrasts(OldNewDescription_LastFirstMention2_C$DefiniteCode.Sum) <- cbind("def"=c(-1, 1))
# I am using 0 as baseline, with 0 being either indef or no article.

OldNewDescription_LastFirstMention2_C$U.c <- OldNewDescription_LastFirstMention2_C$U-mean(OldNewDescription_LastFirstMention2_C$U)



OldNewLastFirst.m0 <- glmer(OldNewCode ~ RoleMatch.Sum +
                             Lag.c + RoleMatch.Sum:Lag.c + 
                             NbDescriptionsPerTrial.c + DefiniteCode.Sum + U.c +
                             Gender.Sum +
                             Age.c +
                             Lag.c:Age.c +
                             SESEduc.c +
                             Extraversion.c +
                             Agreeableness.c +
                             Conscientiousness.c +
                             Neuroticism.c +
                             Openness.c +
                             OverallADHD.c +
                             Attention.c +
                             RoleMatch.Sum:Age.c +
                             SESEduc.c:DefiniteCode.Sum + 
                             (1 + RoleMatch.Sum |DyadSpeakerID) + (1 | PictureCode),
                           data=OldNewDescription_LastFirstMention2_C[OldNewDescription_LastFirstMention2_C$Definiteness1=="indef" | 
                                                                        OldNewDescription_LastFirstMention2_C$Definiteness1=="no", ],
                           family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)), na.action = na.exclude)

#Fixed effects:
#Estimate Std. Error z value Pr(>|z|)    
#(Intercept)                    0.563058   0.170440   3.304 0.000955 ***
#RoleMatch.SumDiff             -0.246586   0.124529  -1.980 0.047686 *  
#Lag.c                         -0.009042   0.031638  -0.286 0.775040    
#NbDescriptionsPerTrial.c      -0.155342   0.188567  -0.824 0.410051    
#DefiniteCode.Sumdef            0.170167   0.162257   1.049 0.294293    
#U.c                           -0.741309   0.152642  -4.857 1.19e-06 ***
#Gender.Sumfemale              -0.010445   0.158614  -0.066 0.947498    
#Age.c                         -0.049585   0.018525  -2.677 0.007435 ** 
#SESEduc.c                     -0.125741   0.121256  -1.037 0.299743    
#Extraversion.c                 0.011111   0.038063   0.292 0.770349    
#Agreeableness.c                0.008956   0.042139   0.213 0.831682    
#Conscientiousness.c            0.038054   0.059197   0.643 0.520331    
#Neuroticism.c                  0.032334   0.024652   1.312 0.189652    
#Openness.c                    -0.011378   0.054454  -0.209 0.834483    
#OverallADHD.c                 -0.012341   0.017128  -0.720 0.471220    
#Attention.c                    0.028399   0.056951   0.499 0.618019    
#RoleMatch.SumDiff:Lag.c       -0.026110   0.031238  -0.836 0.403244    
#Lag.c:Age.c                    0.001195   0.001856   0.644 0.519524    
#RoleMatch.SumDiff:Age.c       -0.002263   0.011998  -0.189 0.850397    
#DefiniteCode.Sumdef:SESEduc.c -0.000987   0.100084  -0.010 0.992132    

# CONCLUSION: There is no effect of definiteness of the NP used first on mention 2 on the probability of producing an old description, and no interaction 
# between SES Educ and definiteness (SESEduc is a variable that affects the production of definite NP overall). Thus, there is no evidence supporting
# Clark's claim regarding conceptual pacts and definiteness.

#############################################################################
## END: Comparing rate of old (vs. new) descriptions on first description  ##
## on mention 2 when that description is also definite, vs. indefinite     ##
## for a subset of the cases, i.e., those where the last description       ## 
## on mention 1 was indefinite or bare                                     ##
#############################################################################


###################################################################
## Comparing the rate of definite (vs. non-definite) NP on       ##
## on mention 2 as a function of role match and lag              ##
###################################################################

table(Data_OldNew_Mention2$DefiniteCode, useNA="always")
#    0    1 <NA> 
#  421  364    4 
mean(na.omit(Data_OldNew_Mention2$DefiniteCode))
#0.4636943

ddply(Data_OldNew_Mention2, .(RoleMatch), summarise, DefiniteNP=mean(na.omit(DefiniteCode)))
#RoleMatch DefiniteNP
# different  0.4430380
#      same  0.4846154

#The rate of def NP is slightly greater when the roles were maintained between mentions 1 and 2 than when they were switched.

ddply(Data_OldNew_Mention2, .(Lag), summarise, DefiniteNP=mean(na.omit(DefiniteCode)))
#Lag DefiniteNP
#    2  0.4107143
#    4  0.3541667
#    8  0.4791667
#   10  0.4166667
#   12  0.3939394
#   14  0.4257426
#   16  0.5833333
#   18  0.5492958
#   22  0.5368421
#   24  0.3829787
#   26  0.4528302


xtabs(~Lag + DefiniteCode, Data_OldNew_Mention2)
#DefiniteCode
#Lag  0  1
# 2  33 23
# 4  31 17
# 8  25 23
#10  28 20
#12  60 39
#14  58 43
#16  20 28
#18  64 78
#22  44 51
#24  29 18
#26  29 24

## There seems to be a slight increase of use of def NP as the lag increases. This is rather odd...


ddply(Data_OldNew_Mention2, .(Lag), summarise, DefiniteNP=mean(na.omit(DefiniteCode)), OldNew=mean(OldNewCode))
#   Lag DefiniteNP    OldNew
#    2  0.4107143 0.6250000
#    4  0.3541667 0.7083333
#    8  0.4791667 0.7083333
#   10  0.4166667 0.6041667
#   12  0.3939394 0.5454545
#   14  0.4257426 0.5742574
#   16  0.5833333 0.6000000
#   18  0.5492958 0.6853147
#   22  0.5368421 0.4842105
#   24  0.3829787 0.5208333
#   26  0.4528302 0.4150943

# It looks like as the definite NP increases with lag, the probability of old descriptions decreases over time. 

ddply(Data_OldNew_Mention2, .(Lag, RoleMatch), summarise, DefiniteNP=mean(na.omit(DefiniteCode)))
#  Lag RoleMatch DefiniteNP
#    2 different  0.4107143
#    4 different  0.3541667
#    8 different  0.4791667
#   10      same  0.4166667
#   12 different  0.3958333
#   12      same  0.3921569
#   14      same  0.4257426
#   16      same  0.5833333
#   18      same  0.5492958
#   22 different  0.5368421
#   24 different  0.3829787
#   26 different  0.4528302

## I notice that same and different are not distributed equally across lag. Instead, "different" tends to be associated with short or long lag, while "same"
## is associated with medium lags (between 10 and 20). 

## Plotting the effect of U, and see if it may be confounded with lag and/or RoleMatch

ddply(Data_OldNew_Mention2, .(U), summarise, DefiniteNP=mean(na.omit(DefiniteCode)))
ggplot(ddply(Data_OldNew_Mention2, .(U), summarise, DefiniteNP=mean(na.omit(DefiniteCode))), aes(U, DefiniteNP)) +
   geom_line()
## => the line is so jittery that it is a bit difficult to trust the effect of U. On the other hand, each U value corresponds, roughtly to one item and there
## aren't very much data points in each point....

ddply(Data_OldNew_Mention2, .(RoleMatch, U), summarise, DefiniteNP=mean(na.omit(DefiniteCode)))
ggplot(ddply(Data_OldNew_Mention2, .(RoleMatch, U), summarise, DefiniteNP=mean(na.omit(DefiniteCode))), aes(U, DefiniteNP)) +
  geom_line() +
  facet_wrap(~RoleMatch)
##==> Nothing obvious emerges from this.

prop.table(xtabs(~OldNewCode + DefiniteCode, data=Data_OldNew_Mention2), 1)
##       DefiniteCode
#OldNewCode         0         1
#         0 0.5524691 0.4475309
#         1 0.5249458 0.4750542

## 47% of old noun phrases produced on mention 2 are definite, while 45% of new noun phrases are. The difference, is very small. 
## This suggests that when people produce an old NP, they are not much more likely to mark its referent as familiar (def NP) than
## they are to not mark its referent as familiar.

Data_OldNew_Mention2$OldNewCodeFactor.Dev <- as.factor(factor(Data_OldNew_Mention2$OldNewCode))
contrasts(Data_OldNew_Mention2$OldNewCodeFactor.Dev) <- cbind("Old"=c(-1/2, 1/2))


DefNP_Mention2.m0 <- glmer(DefiniteCode ~ RoleMatch.Dev +
                             Lag.c + RoleMatch.Dev:Lag.c + 
                             NbDescriptionsPerTrial.c + PriorNbDescriptionsPerTrial.c + OldNewCodeFactor.Dev + U.c +
                             Gender.Dev +
                             Age.c +
                             Lag.c:Age.c +
                             SESEduc.c +
                             Extraversion.c +
                             Agreeableness.c +
                             Conscientiousness.c +
                             Neuroticism.c +
                             Openness.c +
                             OverallADHD.c +
                             Attention.c +
                             RoleMatch.Dev:Age.c +
                             SESEduc.c:RoleMatch.Dev + 
                             (1 + RoleMatch.Dev |DyadSpeakerID) + (1 | PictureCode),
                           data=Data_OldNew_Mention2, family="binomial"(link="logit"), control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)), na.action = na.exclude)

#Estimate Std. Error z value Pr(>|z|)    
#(Intercept)                   -0.402014   0.380926  -1.055 0.291261    
#RoleMatch.DevDiff             -0.094389   0.233341  -0.405 0.685838    
#Lag.c                          0.072495   0.029160   2.486 0.012915 *  
#NbDescriptionsPerTrial.c       0.005317   0.122613   0.043 0.965411    
#PriorNbDescriptionsPerTrial.c -0.124661   0.118195  -1.055 0.291562    
#OldNewCodeFactor.DevOld        0.199447   0.226694   0.880 0.378965    
#U.c                            0.291352   0.129682   2.247 0.024662 *  
#Gender.Devfemale              -0.055944   0.402126  -0.139 0.889355    
#Age.c                         -0.049520   0.044001  -1.125 0.260410    
#SESEduc.c                      0.983500   0.282925   3.476 0.000509 ***
#Extraversion.c                 0.004899   0.083603   0.059 0.953271    
#Agreeableness.c               -0.140353   0.105602  -1.329 0.183823    
#Conscientiousness.c            0.383387   0.146488   2.617 0.008865 ** 
#Neuroticism.c                  0.022758   0.064517   0.353 0.724284    
#Openness.c                    -0.152623   0.121869  -1.252 0.210441    
#OverallADHD.c                  0.030670   0.038075   0.806 0.420516    
#Attention.c                    0.020126   0.128777   0.156 0.875811    
#RoleMatch.DevDiff:Lag.c       -0.140375   0.058229  -2.411 0.015920 *  
#Lag.c:Age.c                   -0.001172   0.001218  -0.963 0.335693    
#RoleMatch.DevDiff:Age.c       -0.011615   0.018736  -0.620 0.535309    
#RoleMatch.DevDiff:SESEduc.c   -0.105238   0.157725  -0.667 0.504629   


# => The status of the roles did not influence the rate with which people produced definite NP. Lag has a strange effect 
# but I think it is largely because lag and role match status is not well balanced across lag: the short (<10) lags are with switched roles and
# they tend to be lower than the greater lag (those between 10 and 20), which are largely "same". The values are then lower for the long lags but all of them 
## are 'different'.
## Looking carefully at the data, here is a plausible interpretation: There is no effect of lag on the production of def NP and only an effect of Role Match, 
## such that the rate of def NP is greater when the roles were maintained than when they were switched. This explains why the effect of lag is positive: 
## On short (<10) and long (>22) lags, the roles were switched and the production of def NP is lower than for intermediate lags. 
## In addition to the descriptive data, the significant interaction between RoleMatch and lag supports this view. Indeed, the interaction indicates
## that the effect of lag is *smaller* when roles were switched than when they were maintained. This is important because the best test of lag 
## was done with the roles swtiched (because it was tested for short and long lags). When tested this way, the effect of lag is small (in fact, one could argue 
## it is not there at all). 
## ==> The role match status is the design factor that affects the production of def NP. (But then, why isn't the factor significant??)
## The uncertainty for the name of the card affects the rate with which people produced a def NP: The larger the uncertainty, the more frequent the use of def NP. 
## THAT IS POTENTIALLY SUPER INTERESTING: Perhaps people feel compelled to use the def NP precisely because they are a bit unsure of how successful the NP will
## be in being successful at identifying the referent.

## In addition, the level of education predicts def NP production, with more educated people producing more def NP on mention 2 than less educated people.

OldNewCode.labels <- list('0'="new", '1'="old")
OldNew_labeller <- function(variable,value){
  return(OldNewCode.labels[value])
}

png(filename = "DefNPRateMention2_U_OldNewCode_Rplot.png")
ggplot(ddply(Data_OldNew_Mention2[is.na(Data_OldNew_Mention2$DefiniteCode)==FALSE, ], .(U, OldNewCode), summarise, DefNPRate=mean(DefiniteCode)), 
     aes(U, DefNPRate)) +
     geom_point() + 
     stat_smooth(method="lm", color="black") +
     facet_wrap(~ as.factor(OldNewCode), labeller=OldNew_labeller) +
     labs(x="U (name uncertainty / dispersion)", 
          y="Proportion of definite NPs per trial") +
     theme(text = element_text(size=20)) + 
     ggtitle("Proportion of definite Noun Phrases\nnew or old,\nproduced on 2nd mention")
dev.off()




